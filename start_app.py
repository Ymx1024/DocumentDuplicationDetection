#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯åŠ¨è„šæœ¬ - ç¡®ä¿å¤šæ ¸ä¼˜åŒ–ç¯å¢ƒå˜é‡åœ¨åº”ç”¨å¯åŠ¨å‰è®¾ç½®
"""

import os
import multiprocessing
import sys

# è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸ºspawnï¼Œè§£å†³CUDA forké—®é¢˜
multiprocessing.set_start_method('spawn', force=True)

def setup_multicore_environment():
    """è®¾ç½®å¤šæ ¸ä¼˜åŒ–ç¯å¢ƒå˜é‡"""
    cpu_cores = multiprocessing.cpu_count()
    worker_threads = min(cpu_cores, 16)
    
    # è®¾ç½®æ‰€æœ‰ç›¸å…³çš„çº¿ç¨‹æ•°ç¯å¢ƒå˜é‡
    env_vars = {
        'OMP_NUM_THREADS': str(worker_threads),
        'MKL_NUM_THREADS': str(worker_threads),
        'NUMEXPR_NUM_THREADS': str(worker_threads),
        'OPENBLAS_NUM_THREADS': str(worker_threads),
        'VECLIB_MAXIMUM_THREADS': str(worker_threads),
        'NUMBA_NUM_THREADS': str(worker_threads),
        'BLIS_NUM_THREADS': str(worker_threads),
        'MKL_DYNAMIC': 'FALSE',  # ç¦ç”¨åŠ¨æ€çº¿ç¨‹æ•°
        'OMP_DYNAMIC': 'FALSE',  # ç¦ç”¨åŠ¨æ€çº¿ç¨‹æ•°
        'TOKENIZERS_PARALLELISM': 'false',  # ç¦ç”¨tokenizerså¹¶è¡Œï¼Œé¿å…forkè­¦å‘Š
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")
    
    print(f"æ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•°: {cpu_cores}")
    print(f"ä½¿ç”¨å·¥ä½œçº¿ç¨‹æ•°: {worker_threads}")
    print("å¤šæ ¸ç¯å¢ƒé…ç½®å®Œæˆï¼")

if __name__ == "__main__":
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—¥å¿—æŠ‘åˆ¶ç¯å¢ƒå˜é‡
    os.environ.pop('SUPPRESS_INIT_LOGS', None)
    
    # è®¾ç½®å¤šæ ¸ç¯å¢ƒ
    setup_multicore_environment()
    
    # ç®€åŒ–å¯¼å…¥åº”ç”¨ - æ‰€æœ‰å¤æ‚é€»è¾‘éƒ½å·²ç§»åˆ° utils.py ä¸­
    print("ğŸ“¦ å¼€å§‹å¯¼å…¥åº”ç”¨æ¨¡å—...")
    try:
        from app import app
        print("âœ… åº”ç”¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åº”ç”¨å¯¼å…¥å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä¾èµ–æ˜¯å¦å®Œæ•´å®‰è£…")
        sys.exit(1)
    
    # æ˜¾ç¤ºè®¡ç®—è®¾å¤‡ä¿¡æ¯å’Œå¯åŠ¨ç±»å‹
    try:
        import torch
        from utils import get_model_info, device, _model, _use_tfidf_fallback
        
        print("\n" + "ğŸš€" + "="*58 + "ğŸš€")
        print("ğŸ“± æ–‡æ¡£é‡å¤æ£€æµ‹ç³»ç»Ÿå¯åŠ¨")
        print("="*60)
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = get_model_info()
        
        # æ˜¾ç¤ºå¯åŠ¨æ¨¡å¼ä¿¡æ¯
        print("ğŸ”§ å¯åŠ¨æ¨¡å¼ä¿¡æ¯:")
        startup_mode_names = {
            'normal': 'æ ‡å‡†å¯åŠ¨',
            'fast_start': 'å¿«é€Ÿå¯åŠ¨æ¨¡å¼',
        }
        print(f"   â””â”€ å¯åŠ¨ç±»å‹: {startup_mode_names.get(model_info['startup_mode'], model_info['startup_mode'])}")
        
        # æ˜¾ç¤ºæ¨¡å‹æ¥æºä¿¡æ¯
        model_source_names = {
            'online_download': 'ğŸŒ è”ç½‘ä¸‹è½½æ¨¡å‹',
            'local_cache': 'ğŸ’¾ æœ¬åœ°ç¼“å­˜æ¨¡å‹',
            'local_path': 'ğŸ“ æŒ‡å®šè·¯å¾„æ¨¡å‹', 
            'tfidf_fallback': 'ğŸ“Š TF-IDFåå¤‡æ¨¡å‹'
        }
        print(f"   â””â”€ æ¨¡å‹æ¥æº: {model_source_names.get(model_info['model_source'], model_info['model_source'])}")
        
        if model_info['load_time_seconds'] > 0:
            print(f"   â””â”€ åŠ è½½ç”¨æ—¶: {model_info['load_time_seconds']}ç§’")
        
        print()
        
        # æ£€æŸ¥å®é™…ä½¿ç”¨çš„è®¡ç®—æ¨¡å¼
        actual_device_mode = "CPU"
        actual_model_type = "TF-IDF"
        
        if _model is not None:
            model_type = type(_model).__name__
            if 'SentenceTransformer' in model_type:
                actual_model_type = "SentenceTransformer"
                # æ£€æŸ¥æ¨¡å‹å®é™…è¿è¡Œè®¾å¤‡
                model_device = getattr(_model, 'device', 'cpu')
                if str(model_device).startswith('cuda'):
                    actual_device_mode = "GPU"
            else:
                actual_model_type = "TF-IDF"
                actual_device_mode = "CPU"  # TF-IDFæ€»æ˜¯åœ¨CPUä¸Šè¿è¡Œ
        
        # æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µæ˜¾ç¤ºä¿¡æ¯
        print("ğŸ’» è®¡ç®—ç¯å¢ƒä¿¡æ¯:")
        if actual_device_mode == "GPU":
            print("   â””â”€ ğŸ® GPUåŠ é€Ÿæ¨¡å¼ - é«˜æ€§èƒ½è®¡ç®—")
            print(f"      â”œâ”€ GPU: {torch.cuda.get_device_name(0)}")
            print(f"      â””â”€ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("   â””â”€ ğŸ–¥ï¸  CPUè®¡ç®—æ¨¡å¼ - å¤šçº¿ç¨‹å¤„ç†")
            print(f"      â”œâ”€ å¤„ç†å™¨: {multiprocessing.cpu_count()}æ ¸å¿ƒ")
            if torch.cuda.is_available():
                print(f"      â””â”€ æ³¨æ„: GPUå¯ç”¨ä½†æœªä½¿ç”¨ (å½“å‰æ¨¡å‹: {actual_model_type})")
        
        print()
        print("ğŸ§  AIæ¨¡å‹ä¿¡æ¯:")
        if actual_model_type == "SentenceTransformer":
            print("   â””â”€ SentenceTransformer (é«˜ç²¾åº¦è¯­ä¹‰ç†è§£)")
            if model_info['model_source'] == 'online_download':
                print("      â”œâ”€ é¦–æ¬¡è”ç½‘ä¸‹è½½å®Œæˆï¼Œåç»­å¯åŠ¨å°†ä½¿ç”¨ç¼“å­˜")
            elif model_info['model_source'] == 'local_cache':
                print("      â”œâ”€ ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
            elif model_info['model_source'] == 'local_path':
                print("      â”œâ”€ ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„æ¨¡å‹")
            print("      â””â”€ æä¾›æœ€ä½³çš„æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—ç²¾åº¦")
        else:
            print("   â””â”€ TF-IDFæ¨¡å‹ (æ ‡å‡†ç²¾åº¦æ–‡æœ¬åˆ†æ)")
            if model_info['startup_mode'] == 'fast_start':
                print("      â”œâ”€ å¿«é€Ÿå¯åŠ¨æ¨¡å¼è‡ªåŠ¨é€‰æ‹©")
            else:
                print("      â”œâ”€ è‡ªåŠ¨å›é€€åˆ°TF-IDFæ–¹æ¡ˆ")
            print("      â””â”€ æ— éœ€ä¸‹è½½ï¼Œå¯åŠ¨å¿«é€Ÿï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯")
        
        print("ğŸŒ WebæœåŠ¡: http://0.0.0.0:5000")
        print("ğŸš€" + "="*58 + "ğŸš€")
        
    except Exception as e:
        print(f"è®¾å¤‡ä¿¡æ¯æ˜¾ç¤ºå¤±è´¥: {e}")
    
    print("\n" + "ğŸŒ" + "="*58 + "ğŸŒ")
    print("ğŸš€ å¯åŠ¨Flask WebæœåŠ¡...")
    print("ğŸ“¡ æœåŠ¡å°†åœ¨ http://0.0.0.0:5000 ä¸Šè¿è¡Œ")
    print("â±ï¸  å¯åŠ¨è¿‡ç¨‹ä¸­è¯·ç¨å€™...")
    print("ğŸŒ" + "="*58 + "ğŸŒ")
    
    app.run(host='0.0.0.0', port=5000, debug=True)
