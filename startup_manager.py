"""
æ™ºèƒ½å¯åŠ¨ç®¡ç†å™¨
æŒ‰ç…§ ç¡¬ä»¶ â†’ æœ¬åœ°ç¼“å­˜ â†’ ç½‘ç»œ çš„é¡ºåºè¿›è¡Œæ£€æµ‹å’Œå†³ç­–
"""

import os
import torch
import multiprocessing
import time
from pathlib import Path
from multiprocessing import Process, Queue


def _model_loading_worker(q, model_name_to_load, device):
    """åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œçš„åŠ è½½å‡½æ•°ï¼ˆé¡¶å±‚å‡½æ•°ï¼Œå¯ä»¥è¢«pickleï¼‰"""
    try:
        # åœ¨å­è¿›ç¨‹ä¸­ï¼Œæ—¥å¿—æ˜¯ä¸å¯è§çš„ï¼Œä½†åŠ è½½ä»åœ¨è¿›è¡Œ
        from sentence_transformers import SentenceTransformer
        import torch
        import os
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–åŠ è½½
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['OMP_NUM_THREADS'] = '1'
        os.environ['MKL_NUM_THREADS'] = '1'
        os.environ['NUMEXPR_NUM_THREADS'] = '1'
        
        # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…å¤šè¿›ç¨‹ä¸­çš„CUDAé—®é¢˜
        device = 'cpu'
        
        # ä¼˜åŒ–åŠ è½½å‚æ•°
        load_kwargs = {
            'device': device,
            'trust_remote_code': True,  # å…è®¸è¿œç¨‹ä»£ç 
        }
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œæ·»åŠ é¢å¤–ä¼˜åŒ–
        if os.path.exists(model_name_to_load):
            load_kwargs['use_auth_token'] = False  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦è®¤è¯
        
        # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
        model = SentenceTransformer(model_name_to_load, **load_kwargs)
        
        # è¿”å›æ¨¡å‹å’Œå®é™…ä½¿ç”¨çš„è®¾å¤‡
        q.put((model, device))
        
    except Exception as e:
        # å°†å¼‚å¸¸ä¿¡æ¯æ”¾å…¥é˜Ÿåˆ—ï¼Œä»¥ä¾¿ä¸»è¿›ç¨‹å¯ä»¥æ•è·
        q.put(e)


def load_model_with_timeout(model_name, timeout=30):
    """
    åœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹ï¼Œå¹¶è®¾ç½®è¶…æ—¶ä¿æŠ¤ã€‚
    è¿™æ˜¯è§£å†³æ¨¡å‹åŠ è½½å¡æ­»çš„å…³é”®ã€‚
    """
    # è·å–è®¾å¤‡ä¿¡æ¯ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªæ£€æµ‹CUDA
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    q = Queue()
    p = Process(target=_model_loading_worker, args=(q, model_name, device))
    p.daemon = True
    
    print(f"ğŸ”„ å¼€å§‹åŠ è½½SentenceTransformeræ¨¡å‹: '{model_name}' (è¶…æ—¶: {timeout}ç§’)...")
    start_time = time.time()
    
    p.start()
    p.join(timeout)
    
    elapsed_time = time.time() - start_time

    if p.is_alive():
        print(f"âŒ æ¨¡å‹åŠ è½½è¶…æ—¶ï¼(è¶…è¿‡ {timeout} ç§’)")
        p.terminate()  # å¼ºåˆ¶ç»ˆæ­¢å­è¿›ç¨‹
        p.join()
        return None, "timeout", elapsed_time

    if not q.empty():
        result = q.get()
        if isinstance(result, Exception):
            print(f"âŒ æ¨¡å‹åŠ è½½æ—¶å‘ç”Ÿé”™è¯¯: {result}")
            return None, str(result), elapsed_time
        elif isinstance(result, tuple) and len(result) == 2:
            # æ–°æ ¼å¼ï¼š(model, actual_device)
            model, actual_device = result
            
            # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„è®¾å¤‡
            if actual_device.startswith('cuda'):
                device_info = f"GPU ({actual_device})"
            else:
                device_info = "CPU"
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! (è€—æ—¶: {elapsed_time:.2f}ç§’, è¿è¡Œäº: {device_info})")
            return model, "success", elapsed_time
        else:
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆåªè¿”å›modelï¼‰
            model = result
            model_device = "CPU"
            if torch.cuda.is_available():
                try:
                    # å°è¯•å°†ä¸€ä¸ªæµ‹è¯•å¼ é‡æ”¾åˆ°æ¨¡å‹è®¾å¤‡ä¸Šï¼ŒéªŒè¯å¯ç”¨æ€§
                    test_tensor = torch.tensor([1]).to(model.device)
                    model_device = f"GPU ({model.device})"
                except Exception:
                    model_device = "CPU (GPUéªŒè¯å¤±è´¥)"
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! (è€—æ—¶: {elapsed_time:.2f}ç§’, è¿è¡Œäº: {model_device})")
            return model, "success", elapsed_time
    
    return None, "unknown_error", elapsed_time


def _cpu_model_worker(q, model_name_to_load):
    """CPUæ¨¡å¼æ¨¡å‹åŠ è½½å·¥ä½œè¿›ç¨‹"""
    try:
        from sentence_transformers import SentenceTransformer
        import os
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–CPUåŠ è½½
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['OMP_NUM_THREADS'] = '1'
        os.environ['MKL_NUM_THREADS'] = '1'
        
        # ä¼˜åŒ–åŠ è½½å‚æ•°
        load_kwargs = {
            'device': 'cpu',
            'trust_remote_code': True,
        }
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œæ·»åŠ é¢å¤–ä¼˜åŒ–
        if os.path.exists(model_name_to_load):
            load_kwargs['use_auth_token'] = False
        
        # å¼ºåˆ¶ä½¿ç”¨CPU
        model = SentenceTransformer(model_name_to_load, **load_kwargs)
        q.put((model, 'cpu'))
    except Exception as e:
        q.put(e)


def _load_model_cpu_only(model_name, timeout=30):
    """å¼ºåˆ¶åœ¨CPUä¸ŠåŠ è½½æ¨¡å‹"""
    q = Queue()
    p = Process(target=_cpu_model_worker, args=(q, model_name))
    p.daemon = True
    
    print(f"ğŸ”„ å¼ºåˆ¶CPUæ¨¡å¼åŠ è½½: '{model_name}' (è¶…æ—¶: {timeout}ç§’)...")
    start_time = time.time()
    
    p.start()
    p.join(timeout)
    
    elapsed_time = time.time() - start_time

    if p.is_alive():
        print(f"âŒ CPUæ¨¡å¼åŠ è½½ä¹Ÿè¶…æ—¶ï¼(è¶…è¿‡ {timeout} ç§’)")
        p.terminate()
        p.join()
        return None, "timeout", elapsed_time

    if not q.empty():
        result = q.get()
        if isinstance(result, Exception):
            print(f"âŒ CPUæ¨¡å¼åŠ è½½æ—¶å‘ç”Ÿé”™è¯¯: {result}")
            return None, str(result), elapsed_time
        elif isinstance(result, tuple):
            model, device = result
            print(f"âœ… CPUæ¨¡å¼åŠ è½½æˆåŠŸ! (è€—æ—¶: {elapsed_time:.2f}ç§’)")
            return model, "success", elapsed_time
    
    return None, "unknown_error", elapsed_time


def _load_model_simple(model_name, device='cpu', timeout=20):
    """ç®€åŒ–çš„æ¨¡å‹åŠ è½½æ–¹æ³•ï¼Œå‡å°‘è¶…æ—¶æ—¶é—´"""
    try:
        from sentence_transformers import SentenceTransformer
        import os
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–åŠ è½½
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['OMP_NUM_THREADS'] = '1'
        os.environ['MKL_NUM_THREADS'] = '1'
        
        print(f"ğŸ”„ ç®€åŒ–æ¨¡å¼åŠ è½½: '{model_name}' (è®¾å¤‡: {device}, è¶…æ—¶: {timeout}ç§’)...")
        start_time = time.time()
        
        # ç›´æ¥åŠ è½½ï¼Œä¸ä½¿ç”¨å¤šè¿›ç¨‹
        model = SentenceTransformer(model_name, device=device)
        
        elapsed_time = time.time() - start_time
        print(f"âœ… ç®€åŒ–æ¨¡å¼åŠ è½½æˆåŠŸ! (è€—æ—¶: {elapsed_time:.2f}ç§’)")
        return model, "success", elapsed_time
        
    except Exception as e:
        elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
        print(f"âŒ ç®€åŒ–æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
        return None, str(e), elapsed_time


def detect_hardware_capabilities():
    """
    æ£€æµ‹ç¡¬ä»¶èƒ½åŠ›
    è¿”å›: dict with 'device', 'gpu_available', 'gpu_info'
    """
    result = {
        'device': 'cpu',
        'gpu_available': False,
        'gpu_info': None
    }
    
    print("ğŸ” æ­¥éª¤1: ç¡¬ä»¶èƒ½åŠ›æ£€æµ‹")
    
    # æ£€æµ‹CUDA GPU
    if torch.cuda.is_available():
        try:
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            result.update({
                'device': 'cuda:0',
                'gpu_available': True,
                'gpu_info': {
                    'name': gpu_name,
                    'memory_gb': round(gpu_memory, 1),
                    'count': gpu_count
                }
            })
            
            print(f"  âœ… GPUå¯ç”¨: {gpu_name} ({gpu_memory:.1f}GB)")
            print(f"  ğŸ¯ æ¨èè®¾å¤‡: GPU (cuda:0)")
            
        except Exception as e:
            print(f"  âš ï¸ GPUæ£€æµ‹å¼‚å¸¸: {e}")
            print(f"  ğŸ”„ å›é€€åˆ°CPUæ¨¡å¼")
    else:
        print(f"  âŒ GPUä¸å¯ç”¨")
        print(f"  ğŸ–¥ï¸ ä½¿ç”¨CPUæ¨¡å¼")
    
    # CPUä¿¡æ¯
    cpu_cores = multiprocessing.cpu_count()
    print(f"  ğŸ’» CPUæ ¸å¿ƒæ•°: {cpu_cores}")
    
    return result


def search_local_model_cache(model_keywords=None):
    """
    å…¨ç›®å½•æœç´¢æ¨¡å‹ç¼“å­˜ï¼Œæ”¯æŒå¤šæ¨¡å‹æ£€æµ‹
    è¿”å›: dict with 'found', 'models' (ä¼˜å…ˆçº§åˆ—è¡¨), 'best_model'
    """
    if model_keywords is None:
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šé«˜ç²¾åº¦æ¨¡å‹ä¼˜å…ˆ
        model_keywords = [
            'all-mpnet-base-v2',           # æœ€é«˜ç²¾åº¦
            'all-MiniLM-L6-v2', 
            'paraphrase-MiniLM-L6-v2',
            'paraphrase-multilingual-MiniLM-L12-v2',  # å¤šè¯­è¨€æ”¯æŒ
            'sentence-transformers'        # é€šç”¨åŒ¹é…
        ]
    
    print("ğŸ” æ­¥éª¤2: æœ¬åœ°æ¨¡å‹ç¼“å­˜æœç´¢")
    
    # è·å–å½“å‰ç”¨æˆ·çš„ä¸»ç›®å½•
    home_dir = Path.home()
    
    # æœç´¢è·¯å¾„åˆ—è¡¨
    search_paths = [
        # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        os.environ.get('ST_MODEL_PATH', 'all-mpnet-base-v2'),
        
        # æ ‡å‡†Hugging Faceç¼“å­˜è·¯å¾„
        home_dir / '.cache' / 'huggingface' / 'hub',
        home_dir / '.cache' / 'huggingface' / 'transformers',
        
        # å¯èƒ½çš„ç³»ç»Ÿè·¯å¾„
        Path('/data/doc_similarity_env/cache/huggingface/hub'),
        Path('/root/.cache/huggingface/hub'),
        Path('/home/user/.cache/huggingface/hub'),
        
        # é¡¹ç›®æœ¬åœ°ç¼“å­˜
        Path.cwd() / '.cache' / 'huggingface' / 'hub',
        Path.cwd() / 'models',
        
        # Windowså¸¸è§è·¯å¾„
        home_dir / 'AppData' / 'Local' / 'huggingface' / 'hub' if os.name == 'nt' else None,
    ]
    
    # ç§»é™¤Noneå€¼å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„
    search_paths = [str(p) for p in search_paths if p is not None]
    
    # æ£€æŸ¥ç”¨æˆ·æŒ‡å®šè·¯å¾„æ˜¯å¦æ˜¯ç›®å½•
    user_specified = os.environ.get('ST_MODEL_PATH', 'all-mpnet-base-v2')
    if os.path.isdir(user_specified):
        search_paths.insert(0, user_specified)
    
    print(f"  ğŸ“ æœç´¢ {len(search_paths)} ä¸ªå¯èƒ½çš„è·¯å¾„...")
    
    # å­˜å‚¨æ‰¾åˆ°çš„æ‰€æœ‰æ¨¡å‹
    found_models = []
    
    for i, search_path in enumerate(search_paths, 1):
        print(f"  ğŸ“‚ [{i}/{len(search_paths)}] æ£€æŸ¥: {search_path}")
        
        if not os.path.exists(search_path):
            print(f"     âŒ è·¯å¾„ä¸å­˜åœ¨")
            continue
            
        try:
            # é€’å½’æœç´¢åŒ…å«å…³é”®è¯çš„ç›®å½•
            for root, dirs, files in os.walk(search_path):
                for keyword in model_keywords:
                    # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨è·¯å¾„ä¸­
                    keyword_variants = [
                        keyword,
                        keyword.replace('/', '--'),  # Hugging Faceå‘½åè§„èŒƒ
                        keyword.replace('-', '_')     # å¯èƒ½çš„ä¸‹åˆ’çº¿å˜ä½“
                    ]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å˜ä½“åœ¨è·¯å¾„ä¸­
                    if any(variant in root for variant in keyword_variants):
                        # è·³è¿‡å­ç›®å½•ï¼ˆå¦‚ 1_Pooling, 2_Dense ç­‰ï¼‰
                        if any(subdir in root for subdir in ['/1_Pooling', '/2_Dense', '/3_Dense', '/0_Transformer']):
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                        model_files = ['config.json', 'pytorch_model.bin', 'model.safetensors']
                        found_files = [f for f in model_files if os.path.isfile(os.path.join(root, f))]
                        
                        # ç¡®ä¿æ˜¯å®Œæ•´çš„æ¨¡å‹ç›®å½•ï¼ˆå¿…é¡»åŒ…å«config.jsonå’Œè‡³å°‘ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼‰
                        if 'config.json' in found_files and len(found_files) >= 2:
                            # é¿å…é‡å¤æ·»åŠ ç›¸åŒæ¨¡å‹
                            if not any(model['path'] == root for model in found_models):
                                model_info = {
                                    'path': root,
                                    'model_type': keyword,
                                    'files': found_files,
                                    'priority': model_keywords.index(keyword)  # ä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šä¼˜å…ˆï¼‰
                                }
                                found_models.append(model_info)
                                
                                print(f"     âœ… å‘ç°æ¨¡å‹: {keyword}")
                                print(f"     ğŸ“ è·¯å¾„: {root}")
                                print(f"     ğŸ“„ æ–‡ä»¶: {', '.join(found_files)}")
                                
                                # å¦‚æœæ‰¾åˆ°çš„æ˜¯å®Œæ•´è·¯å¾„æ¨¡å‹ï¼Œå°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¾¿ä¸‹æ¬¡å¿«é€Ÿæ‰¾åˆ°
                                if keyword != 'sentence-transformers':
                                    try:
                                        # ä¸ºä¸‹æ¬¡å¯åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡æç¤º
                                        print(f"     ğŸ’¡ æç¤º: å¯è®¾ç½®ç¯å¢ƒå˜é‡ ST_MODEL_PATH={root} ä»¥å¿«é€Ÿå¯åŠ¨")
                                    except Exception:
                                        pass
            
            if not found_models:
                print(f"     âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            
        except PermissionError:
            print(f"     âš ï¸ æƒé™ä¸è¶³ï¼Œè·³è¿‡")
        except Exception as e:
            print(f"     âŒ æœç´¢å¤±è´¥: {e}")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºæ‰¾åˆ°çš„æ¨¡å‹
    found_models.sort(key=lambda x: x['priority'])
    
    if found_models:
        print(f"  ğŸ‰ å…±æ‰¾åˆ° {len(found_models)} ä¸ªæ¨¡å‹:")
        for i, model in enumerate(found_models, 1):
            print(f"    [{i}] {model['model_type']} - {model['path']}")
        
        return {
            'found': True,
            'models': found_models,
            'best_model': found_models[0],  # æœ€é«˜ä¼˜å…ˆçº§çš„æ¨¡å‹
            'count': len(found_models)
        }
    else:
        print("  ğŸ’” æœªæ‰¾åˆ°ä»»ä½•æœ¬åœ°æ¨¡å‹ç¼“å­˜")
        return {
            'found': False, 
            'models': [], 
            'best_model': None,
            'count': 0
        }


def validate_model_integrity(model_path):
    """
    éªŒè¯æ¨¡å‹å®Œæ•´æ€§
    è¿”å›: dict with 'valid', 'issues', 'repairable'
    """
    print(f"ğŸ” éªŒè¯æ¨¡å‹å®Œæ•´æ€§: {model_path}")
    
    issues = []
    repairable = True
    
    try:
        # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ['config.json']
        model_files = ['pytorch_model.bin', 'model.safetensors']
        
        for file in required_files:
            file_path = os.path.join(model_path, file)
            if not os.path.exists(file_path):
                issues.append(f"ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {file}")
                repairable = False
        
        # æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶
        has_model_file = False
        for file in model_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                has_model_file = True
                break
        
        if not has_model_file:
            issues.append("ç¼ºå°‘æ¨¡å‹æƒé‡æ–‡ä»¶")
            repairable = False
        
        # æ£€æŸ¥config.jsonå†…å®¹
        config_path = os.path.join(model_path, 'config.json')
        if os.path.exists(config_path):
            try:
                import json
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                if 'model_type' not in config:
                    issues.append("config.jsonç¼ºå°‘model_typeå­—æ®µ")
                    repairable = True  # å¯ä»¥ä¿®å¤
                
                if 'architectures' not in config:
                    issues.append("config.jsonç¼ºå°‘architectureså­—æ®µ")
                    repairable = True  # å¯ä»¥ä¿®å¤
                    
            except Exception as e:
                issues.append(f"config.jsonè§£æå¤±è´¥: {e}")
                repairable = False
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé˜²æ­¢ç©ºæ–‡ä»¶ï¼‰
        for file in ['config.json'] + model_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                if os.path.getsize(file_path) == 0:
                    issues.append(f"æ–‡ä»¶ä¸ºç©º: {file}")
                    repairable = False
        
        if not issues:
            print("  âœ… æ¨¡å‹å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            return {'valid': True, 'issues': [], 'repairable': True}
        else:
            print(f"  âš ï¸ å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues:
                print(f"    - {issue}")
            return {'valid': False, 'issues': issues, 'repairable': repairable}
            
    except Exception as e:
        print(f"  âŒ æ¨¡å‹å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
        return {'valid': False, 'issues': [f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}"], 'repairable': False}


def test_model_availability(model_path, device='cpu', quick_test=True):
    """
    æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
    è¿”å›: dict with 'available', 'error'
    """
    print(f"ğŸ” æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§: {model_path}")
    
    try:
        from sentence_transformers import SentenceTransformer
        import os
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–æµ‹è¯•
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['OMP_NUM_THREADS'] = '1'
        
        # å¿«é€Ÿæµ‹è¯•åŠ è½½ï¼ˆä¸åœ¨GPUä¸Šæµ‹è¯•ï¼Œé¿å…é•¿æ—¶é—´å ç”¨ï¼‰
        print("  â³ å¿«é€Ÿæµ‹è¯•åŠ è½½...")
        
        # ä¼˜åŒ–åŠ è½½å‚æ•°
        load_kwargs = {
            'device': 'cpu',
            'trust_remote_code': True,
        }
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œæ·»åŠ é¢å¤–ä¼˜åŒ–
        if os.path.exists(model_path):
            load_kwargs['use_auth_token'] = False
        
        model = SentenceTransformer(model_path, **load_kwargs)
        
        if quick_test:
            # å¿«é€Ÿæµ‹è¯•ï¼šåªæ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸åˆå§‹åŒ–
            print("  â³ å¿«é€ŸéªŒè¯æ¨¡å‹ç»“æ„...")
            if hasattr(model, 'encode') and hasattr(model, '_modules'):
                print("  âœ… æ¨¡å‹ç»“æ„éªŒè¯é€šè¿‡")
                del model  # é‡Šæ”¾å†…å­˜
                return {'available': True, 'error': None}
            else:
                print("  âŒ æ¨¡å‹ç»“æ„éªŒè¯å¤±è´¥")
                return {'available': False, 'error': 'æ¨¡å‹ç»“æ„ä¸å®Œæ•´'}
        else:
            # å®Œæ•´æµ‹è¯•ï¼šæµ‹è¯•ç¼–ç åŠŸèƒ½
            print("  â³ æµ‹è¯•ç¼–ç åŠŸèƒ½...")
            test_text = "This is a test sentence."
            embedding = model.encode(test_text, convert_to_tensor=True)
            
            if embedding is not None and len(embedding) > 0:
                print("  âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡")
                del model  # é‡Šæ”¾å†…å­˜
                return {'available': True, 'error': None}
            else:
                print("  âŒ æ¨¡å‹ç¼–ç æµ‹è¯•å¤±è´¥")
                return {'available': False, 'error': 'ç¼–ç ç»“æœæ— æ•ˆ'}
            
    except Exception as e:
        print(f"  âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return {'available': False, 'error': str(e)}


def check_network_connectivity():
    """
    æ£€æµ‹ç½‘ç»œè¿é€šæ€§
    è¿”å›: dict with 'huggingface_available', 'general_internet'
    """
    print("ğŸ” æ­¥éª¤3: ç½‘ç»œè¿é€šæ€§æ£€æµ‹")
    
    def test_url(url, timeout=3):
        try:
            import urllib.request
            urllib.request.urlopen(url, timeout=timeout)
            return True
        except Exception:
            return False
    
    # æµ‹è¯•Hugging Faceè¿é€šæ€§
    hf_available = test_url("https://huggingface.co", timeout=5)
    if hf_available:
        print("  âœ… Hugging Face å¯è®¿é—®")
    else:
        print("  âŒ Hugging Face ä¸å¯è®¿é—®")
    
    # æµ‹è¯•ä¸€èˆ¬ç½‘ç»œè¿é€šæ€§
    general_internet = test_url("https://www.google.com", timeout=3) or test_url("https://www.baidu.com", timeout=3)
    if general_internet:
        print("  âœ… äº’è”ç½‘è¿æ¥æ­£å¸¸")
    else:
        print("  âŒ äº’è”ç½‘è¿æ¥å¼‚å¸¸")
    
    return {
        'huggingface_available': hf_available,
        'general_internet': general_internet
    }


def repair_model(model_path, model_type):
    """
    ä¿®å¤æŸåçš„æ¨¡å‹
    è¿”å›: dict with 'success', 'new_path', 'error'
    """
    print(f"ğŸ”§ å¼€å§‹ä¿®å¤æ¨¡å‹: {model_type}")
    print(f"ğŸ“ åŸè·¯å¾„: {model_path}")
    
    try:
        # è·å–æ¨¡å‹åç§°æ˜ å°„
        model_name_mapping = {
            'all-mpnet-base-v2': 'sentence-transformers/all-mpnet-base-v2',
            'all-MiniLM-L6-v2': 'sentence-transformers/all-MiniLM-L6-v2',
            'paraphrase-MiniLM-L6-v2': 'sentence-transformers/paraphrase-MiniLM-L6-v2',
            'paraphrase-multilingual-MiniLM-L12-v2': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        }
        
        if model_type not in model_name_mapping:
            return {'success': False, 'new_path': None, 'error': f'æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}'}
        
        model_name = model_name_mapping[model_type]
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_path = model_path + '.backup'
        if os.path.exists(model_path):
            print(f"ğŸ“¦ åˆ›å»ºå¤‡ä»½: {backup_path}")
            import shutil
            shutil.move(model_path, backup_path)
        
        # é‡æ–°ä¸‹è½½æ¨¡å‹
        print(f"â¬‡ï¸ é‡æ–°ä¸‹è½½æ¨¡å‹: {model_name}")
        from sentence_transformers import SentenceTransformer
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–ä¸‹è½½
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['OMP_NUM_THREADS'] = '1'
        
        # ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
        model = SentenceTransformer(model_name, cache_folder=os.path.dirname(model_path))
        
        # è·å–å®é™…ä¸‹è½½è·¯å¾„
        new_path = model._modules['0'].auto_model.config.name_or_path
        if not new_path or not os.path.exists(new_path):
            # å¦‚æœæ— æ³•è·å–è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            new_path = model_path
        
        print(f"âœ… æ¨¡å‹ä¿®å¤æˆåŠŸ!")
        print(f"ğŸ“ æ–°è·¯å¾„: {new_path}")
        
        # æ¸…ç†å¤‡ä»½ï¼ˆå¦‚æœæ–°æ¨¡å‹å·¥ä½œæ­£å¸¸ï¼‰
        if os.path.exists(backup_path):
            print(f"ğŸ—‘ï¸ æ¸…ç†å¤‡ä»½ç›®å½•: {backup_path}")
            shutil.rmtree(backup_path)
        
        return {'success': True, 'new_path': new_path, 'error': None}
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿®å¤å¤±è´¥: {e}")
        
        # æ¢å¤å¤‡ä»½
        if os.path.exists(backup_path):
            print(f"ğŸ”„ æ¢å¤å¤‡ä»½: {backup_path}")
            import shutil
            if os.path.exists(model_path):
                shutil.rmtree(model_path)
            shutil.move(backup_path, model_path)
        
        return {'success': False, 'new_path': None, 'error': str(e)}


def auto_repair_models(local_models, network_status):
    """
    è‡ªåŠ¨ä¿®å¤æœ‰é—®é¢˜çš„æ¨¡å‹
    è¿”å›: dict with 'repaired_models', 'failed_models'
    """
    if not network_status['huggingface_available']:
        print("âš ï¸ Hugging Faceä¸å¯è®¿é—®ï¼Œè·³è¿‡æ¨¡å‹ä¿®å¤")
        return {'repaired_models': [], 'failed_models': []}
    
    print("ğŸ”§ å¼€å§‹è‡ªåŠ¨æ¨¡å‹ä¿®å¤æ£€æŸ¥")
    repaired_models = []
    failed_models = []
    
    for model in local_models['models']:
        model_path = model['path']
        model_type = model['model_type']
        
        print(f"\nğŸ” æ£€æŸ¥æ¨¡å‹: {model_type}")
        
        # éªŒè¯æ¨¡å‹å®Œæ•´æ€§
        integrity_result = validate_model_integrity(model_path)
        
        if integrity_result['valid']:
            print(f"âœ… æ¨¡å‹ {model_type} å®Œæ•´æ€§æ­£å¸¸")
            continue
        
        if not integrity_result['repairable']:
            print(f"âŒ æ¨¡å‹ {model_type} æ— æ³•ä¿®å¤")
            failed_models.append({
                'model_type': model_type,
                'path': model_path,
                'issues': integrity_result['issues']
            })
            continue
        
        # å°è¯•ä¿®å¤æ¨¡å‹
        print(f"ğŸ”§ å°è¯•ä¿®å¤æ¨¡å‹: {model_type}")
        repair_result = repair_model(model_path, model_type)
        
        if repair_result['success']:
            print(f"âœ… æ¨¡å‹ {model_type} ä¿®å¤æˆåŠŸ")
            repaired_models.append({
                'model_type': model_type,
                'old_path': model_path,
                'new_path': repair_result['new_path']
            })
        else:
            print(f"âŒ æ¨¡å‹ {model_type} ä¿®å¤å¤±è´¥: {repair_result['error']}")
            failed_models.append({
                'model_type': model_type,
                'path': model_path,
                'issues': integrity_result['issues'],
                'repair_error': repair_result['error']
            })
    
    print(f"\nğŸ“Š ä¿®å¤ç»“æœ:")
    print(f"  âœ… æˆåŠŸä¿®å¤: {len(repaired_models)} ä¸ªæ¨¡å‹")
    print(f"  âŒ ä¿®å¤å¤±è´¥: {len(failed_models)} ä¸ªæ¨¡å‹")
    
    return {'repaired_models': repaired_models, 'failed_models': failed_models}


def intelligent_startup_strategy():
    """
    æ™ºèƒ½å¯åŠ¨ç­–ç•¥å†³ç­–å™¨
    æŒ‰ç…§ ç¡¬ä»¶ â†’ æœ¬åœ°ç¼“å­˜ â†’ ç½‘ç»œ çš„é¡ºåºè¿›è¡Œæ£€æµ‹å’Œå†³ç­–
    è¿”å›: dict with 'strategy', 'config', 'reason'
    """
    print("\nğŸ¤– æ™ºèƒ½å¯åŠ¨ç­–ç•¥å†³ç­–å™¨")
    print("="*60)
    
    # æ£€æŸ¥ç”¨æˆ·å¼ºåˆ¶è®¾ç½®
    if os.getenv('FORCE_TFIDF', '0') == '1':
        return {
            'strategy': 'force_tfidf',
            'config': {'device': 'cpu', 'model_type': 'tfidf'},
            'reason': 'ç”¨æˆ·å¼ºåˆ¶æŒ‡å®šTF-IDFæ¨¡å¼'
        }
    
    if os.getenv('FAST_START', '0') == '1':
        return {
            'strategy': 'fast_start', 
            'config': {'device': 'cpu', 'model_type': 'tfidf'},
            'reason': 'ç”¨æˆ·æŒ‡å®šå¿«é€Ÿå¯åŠ¨æ¨¡å¼'
        }
    
    # æ­¥éª¤1: ç¡¬ä»¶æ£€æµ‹
    hardware = detect_hardware_capabilities()
    
    # æ­¥éª¤2: æœ¬åœ°æ¨¡å‹æœç´¢
    local_model = search_local_model_cache()
    
    # æ­¥éª¤3: ç½‘ç»œæ£€æµ‹
    network = check_network_connectivity()
    
    # æ­¥éª¤4: æ¨¡å‹ä¿®å¤æ£€æŸ¥ï¼ˆå¦‚æœç½‘ç»œå¯ç”¨ä¸”æœ¬åœ°æ¨¡å‹æœ‰é—®é¢˜ï¼‰
    if network['huggingface_available'] and local_model['found']:
        print("\nğŸ”§ æ­¥éª¤4: æ¨¡å‹ä¿®å¤æ£€æŸ¥")
        repair_result = auto_repair_models(local_model, network)
        
        # å¦‚æœæœ‰æ¨¡å‹è¢«ä¿®å¤ï¼Œé‡æ–°æœç´¢æœ¬åœ°æ¨¡å‹
        if repair_result['repaired_models']:
            print("ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹ä¿®å¤ï¼Œé‡æ–°æœç´¢æœ¬åœ°æ¨¡å‹...")
            local_model = search_local_model_cache()
    
    print("\nğŸ¯ ç­–ç•¥å†³ç­–åˆ†æ:")
    
    # å†³ç­–é€»è¾‘ - å¤šæ¨¡å‹æ”¯æŒ
    if local_model['found'] and local_model['count'] > 0:
        print(f"  ğŸ“‹ å‘ç° {local_model['count']} ä¸ªæœ¬åœ°æ¨¡å‹ï¼ŒæŒ‰ä¼˜å…ˆçº§æµ‹è¯•...")
        
        # æŒ‰ä¼˜å…ˆçº§æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for i, model in enumerate(local_model['models']):
            print(f"  ğŸ” æµ‹è¯•æ¨¡å‹ {i+1}/{local_model['count']}: {model['model_type']}")
            model_test = test_model_availability(model['path'])
            
            if model_test['available']:
                # æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ï¼Œé€‰æ‹©æœ€ä½³è®¾å¤‡
                device = hardware['device']
                
                print(f"  âœ… æ¨¡å‹ {model['model_type']} æµ‹è¯•é€šè¿‡ï¼Œå°†ä½¿ç”¨æ­¤æ¨¡å‹")
                
                return {
                    'strategy': 'local_model',
                    'config': {
                        'device': device,
                        'model_path': model['path'],
                        'model_type': model['model_type'],
                        'gpu_available': hardware['gpu_available'],
                        'model_priority': i + 1,  # åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®
                        'total_models': local_model['count']
                    },
                    'reason': f"æœ¬åœ°æ¨¡å‹ {model['model_type']} å¯ç”¨ + {device.upper()}è®¡ç®—"
                }
            else:
                print(f"  âŒ æ¨¡å‹ {model['model_type']} æµ‹è¯•å¤±è´¥: {model_test['error']}")
                continue
        
        print(f"  âš ï¸ æ‰€æœ‰ {local_model['count']} ä¸ªæœ¬åœ°æ¨¡å‹éƒ½ä¸å¯ç”¨")
        print(f"  ğŸ”„ ç»§ç»­æ£€æŸ¥ç½‘ç»œä¸‹è½½é€‰é¡¹...")
    
    # æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨æˆ–ä¸å­˜åœ¨ï¼Œæ£€æŸ¥ç½‘ç»œä¸‹è½½
    if network['huggingface_available']:
        device = hardware['device']
        model_name = os.environ.get('ST_MODEL_PATH', 'all-mpnet-base-v2')
        
        return {
            'strategy': 'download_model',
            'config': {
                'device': device,
                'model_name': model_name,
                'gpu_available': hardware['gpu_available']
            },
            'reason': f"è”ç½‘ä¸‹è½½æ¨¡å‹ + {device.upper()}è®¡ç®—"
        }
    
    # ç½‘ç»œä¸å¯ç”¨ï¼Œæ£€æŸ¥TF-IDFå¯ç”¨æ€§
    print("  âš ï¸ ç½‘ç»œä¸å¯ç”¨ï¼Œæ£€æŸ¥TF-IDFåå¤‡æ–¹æ¡ˆ...")
    
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        return {
            'strategy': 'tfidf_fallback',
            'config': {
                'device': 'cpu',
                'model_type': 'tfidf'
            },
            'reason': 'TF-IDFåå¤‡æ–¹æ¡ˆ (æ— ç½‘ç»œè¿æ¥)'
        }
    except ImportError:
        print("  âŒ TF-IDFä¾èµ–ä¸å¯ç”¨")
        
        # æœ€åçš„CPU fallback
        return {
            'strategy': 'minimal_cpu',
            'config': {
                'device': 'cpu',
                'model_type': 'basic'
            },
            'reason': 'CPUåŸºç¡€æ¨¡å¼ (æœ€å°å¯åŠ¨éœ€æ±‚)'
        }


def execute_startup_strategy(strategy_result):
    """
    æ‰§è¡Œå¯åŠ¨ç­–ç•¥
    è¿”å›: dict with 'success', 'model', 'actual_config'
    """
    strategy = strategy_result['strategy']
    config = strategy_result['config']
    
    print(f"\nğŸš€ æ‰§è¡Œå¯åŠ¨ç­–ç•¥: {strategy}")
    print(f"ğŸ“‹ é…ç½®: {config}")
    print(f"ğŸ’¡ åŸå› : {strategy_result['reason']}")
    print("-" * 40)
    
    try:
        if strategy == 'local_model':
            return _execute_local_model_strategy(config)
        elif strategy == 'download_model':
            return _execute_download_strategy(config)
        elif strategy == 'tfidf_fallback':
            return _execute_tfidf_strategy(config)
        elif strategy == 'force_tfidf' or strategy == 'fast_start':
            return _execute_tfidf_strategy(config)
        elif strategy == 'minimal_cpu':
            return _execute_minimal_strategy(config)
        else:
            raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")
            
    except Exception as e:
        print(f"âŒ ç­–ç•¥æ‰§è¡Œå¤±è´¥: {e}")
        # æœ€åçš„åå¤‡æ–¹æ¡ˆ
        print("ğŸ”„ æ‰§è¡Œæœ€å°åŒ–åå¤‡ç­–ç•¥...")
        return _execute_minimal_strategy({'device': 'cpu', 'model_type': 'basic'})


def _execute_local_model_strategy(config):
    """æ‰§è¡Œæœ¬åœ°æ¨¡å‹ç­–ç•¥ï¼Œæ”¯æŒå¤šæ¨¡å‹å›é€€"""
    model_path = config['model_path']
    device = config['device']
    model_type = config.get('model_type', 'unknown')
    total_models = config.get('total_models', 1)
    model_priority = config.get('model_priority', 1)
    
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
    print(f"ğŸ¯ ç›®æ ‡è®¾å¤‡: {device}")
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: {model_type} (ä¼˜å…ˆçº§ {model_priority}/{total_models})")
    
    # ä¼˜å…ˆçº§1: ç®€åŒ–åŠ è½½ï¼ˆæœ€å¿«æœ€ç¨³å®šï¼‰
    print("ğŸ”„ å°è¯•ç®€åŒ–åŠ è½½...")
    model, status, load_time = _load_model_simple(model_path, device='cpu', timeout=20)
    
    if status == "success" and model is not None:
        return {
            'success': True,
            'model': model,
            'actual_config': {
                'strategy': 'local_model_simple',
                'device': 'cpu',
                'model_path': model_path,
                'model_type': model_type,
                'load_time': load_time,
                'model_priority': model_priority
            }
        }
    
    # ä¼˜å…ˆçº§2: å¤šè¿›ç¨‹GPUåŠ è½½
    if device.startswith('cuda'):
        print("ğŸ”„ ç®€åŒ–åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤šè¿›ç¨‹GPUåŠ è½½...")
        model, status, load_time = load_model_with_timeout(model_path, timeout=30)
        
        if status == "success" and model is not None:
            return {
                'success': True,
                'model': model,
                'actual_config': {
                    'strategy': 'local_model_gpu',
                    'device': device,
                    'model_path': model_path,
                    'model_type': model_type,
                    'load_time': load_time,
                    'model_priority': model_priority
                }
            }
    
    # ä¼˜å…ˆçº§3: å¤šè¿›ç¨‹CPUåŠ è½½
    print("ğŸ”„ GPUåŠ è½½å¤±è´¥ï¼Œå°è¯•å¤šè¿›ç¨‹CPUåŠ è½½...")
    model, status, load_time = _load_model_cpu_only(model_path, timeout=30)
    
    if status == "success" and model is not None:
        return {
            'success': True,
            'model': model,
            'actual_config': {
                'strategy': 'local_model_cpu',
                'device': 'cpu',
                'model_path': model_path,
                'model_type': model_type,
                'load_time': load_time,
                'model_priority': model_priority
            }
        }
    
    # å¦‚æœæ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ï¼Œå°è¯•å…¶ä»–å¯ç”¨æ¨¡å‹
    if total_models > 1:
        print(f"ğŸ”„ æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ï¼Œå°è¯•å…¶ä»–å¯ç”¨æ¨¡å‹...")
        return _try_alternative_models(config, device)
    
    raise Exception(f"æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: ç®€åŒ–åŠ è½½ã€GPUåŠ è½½ã€CPUåŠ è½½éƒ½å¤±è´¥")


def _try_alternative_models(config, device):
    """å°è¯•å…¶ä»–å¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
    # é‡æ–°æœç´¢æ‰€æœ‰å¯ç”¨æ¨¡å‹
    local_models = search_local_model_cache()
    
    if not local_models['found'] or local_models['count'] == 0:
        raise Exception("æ²¡æœ‰å…¶ä»–å¯ç”¨æ¨¡å‹")
    
    current_model_path = config['model_path']
    
    # å°è¯•å…¶ä»–æ¨¡å‹
    for model in local_models['models']:
        if model['path'] == current_model_path:
            continue  # è·³è¿‡å·²ç»å¤±è´¥çš„æ¨¡å‹
            
        print(f"ğŸ”„ å°è¯•å¤‡ç”¨æ¨¡å‹: {model['model_type']}")
        print(f"ğŸ“ è·¯å¾„: {model['path']}")
        
        # æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§
        test_result = test_model_availability(model['path'])
        if not test_result['available']:
            print(f"âŒ å¤‡ç”¨æ¨¡å‹ {model['model_type']} ä¸å¯ç”¨: {test_result['error']}")
            continue
        
        # ä¼˜å…ˆçº§1: ç®€åŒ–åŠ è½½ï¼ˆæœ€å¿«æœ€ç¨³å®šï¼‰
        print(f"ğŸ”„ å°è¯•ç®€åŒ–åŠ è½½å¤‡ç”¨æ¨¡å‹ {model['model_type']}...")
        model_obj, status, load_time = _load_model_simple(model['path'], device='cpu', timeout=20)
        
        if status == "success" and model_obj is not None:
            print(f"âœ… å¤‡ç”¨æ¨¡å‹ {model['model_type']} ç®€åŒ–åŠ è½½æˆåŠŸ!")
            return {
                'success': True,
                'model': model_obj,
                'actual_config': {
                    'strategy': 'local_model_fallback_simple',
                    'device': 'cpu',
                    'model_path': model['path'],
                    'model_type': model['model_type'],
                    'load_time': load_time,
                    'fallback_reason': f"åŸæ¨¡å‹ {config.get('model_type', 'unknown')} åŠ è½½å¤±è´¥"
                }
            }
        
        # ä¼˜å…ˆçº§2: å¤šè¿›ç¨‹GPUåŠ è½½
        if device.startswith('cuda'):
            print(f"ğŸ”„ ç®€åŒ–åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤šè¿›ç¨‹GPUåŠ è½½å¤‡ç”¨æ¨¡å‹ {model['model_type']}...")
            model_obj, status, load_time = load_model_with_timeout(model['path'], timeout=30)
            
            if status == "success" and model_obj is not None:
                print(f"âœ… å¤‡ç”¨æ¨¡å‹ {model['model_type']} GPUåŠ è½½æˆåŠŸ!")
                return {
                    'success': True,
                    'model': model_obj,
                    'actual_config': {
                        'strategy': 'local_model_fallback_gpu',
                        'device': device,
                        'model_path': model['path'],
                        'model_type': model['model_type'],
                        'load_time': load_time,
                        'fallback_reason': f"åŸæ¨¡å‹ {config.get('model_type', 'unknown')} åŠ è½½å¤±è´¥"
                    }
                }
        
        # ä¼˜å…ˆçº§3: å¤šè¿›ç¨‹CPUåŠ è½½
        print(f"ğŸ”„ GPUåŠ è½½å¤±è´¥ï¼Œå°è¯•å¤šè¿›ç¨‹CPUåŠ è½½å¤‡ç”¨æ¨¡å‹ {model['model_type']}...")
        model_obj, status, load_time = _load_model_cpu_only(model['path'], timeout=30)
        
        if status == "success" and model_obj is not None:
            print(f"âœ… å¤‡ç”¨æ¨¡å‹ {model['model_type']} CPUåŠ è½½æˆåŠŸ!")
            return {
                'success': True,
                'model': model_obj,
                'actual_config': {
                    'strategy': 'local_model_fallback_cpu',
                    'device': 'cpu',
                    'model_path': model['path'],
                    'model_type': model['model_type'],
                    'load_time': load_time,
                    'fallback_reason': f"åŸæ¨¡å‹ {config.get('model_type', 'unknown')} åŠ è½½å¤±è´¥"
                }
            }
        else:
            print(f"âŒ å¤‡ç”¨æ¨¡å‹ {model['model_type']} æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {status}")
            continue
    
    raise Exception("æ‰€æœ‰æœ¬åœ°æ¨¡å‹éƒ½åŠ è½½å¤±è´¥")


def _execute_download_strategy(config):
    """æ‰§è¡Œä¸‹è½½æ¨¡å‹ç­–ç•¥"""
    model_name = config['model_name']
    device = config['device']
    
    print(f"ğŸŒ ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹: {model_name}")
    print(f"ğŸ¯ ç›®æ ‡è®¾å¤‡: {device}")
    
    model, status, load_time = load_model_with_timeout(model_name, timeout=60)
    
    if status == "success" and model is not None:
        return {
            'success': True,
            'model': model,
            'actual_config': {
                'strategy': 'download_model',
                'device': device,
                'model_name': model_name,
                'load_time': load_time
            }
        }
    else:
        raise Exception(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {status}")


def _execute_tfidf_strategy(config):
    """æ‰§è¡ŒTF-IDFç­–ç•¥"""
    print("ğŸ“Š åˆå§‹åŒ–TF-IDFæ¨¡å‹...")
    
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity as _sk_cos_sim
        
        tfidf_model = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))
        
        return {
            'success': True,
            'model': None,  # TF-IDFåœ¨åˆ«å¤„å¤„ç†
            'actual_config': {
                'strategy': 'tfidf',
                'device': 'cpu',
                'model_type': 'tfidf'
            }
        }
    except ImportError as e:
        raise Exception(f"TF-IDFä¾èµ–ä¸å¯ç”¨: {e}")


def _execute_minimal_strategy(config):
    """æ‰§è¡Œæœ€å°åŒ–ç­–ç•¥"""
    print("âš ï¸ æœ€å°åŒ–å¯åŠ¨æ¨¡å¼...")
    print("ğŸ’¡ ä»…æä¾›åŸºç¡€æ–‡æœ¬å¤„ç†åŠŸèƒ½")
    
    return {
        'success': True,
        'model': None,
        'actual_config': {
            'strategy': 'minimal',
            'device': 'cpu',
            'model_type': 'basic'
        }
    }


def run_intelligent_startup():
    """
    è¿è¡Œå®Œæ•´çš„æ™ºèƒ½å¯åŠ¨æµç¨‹
    è¿”å›: dict with startup results
    """
    try:
        # 1. è·å–å¯åŠ¨ç­–ç•¥
        strategy_result = intelligent_startup_strategy()
        
        # 2. æ‰§è¡Œç­–ç•¥
        execution_result = execute_startup_strategy(strategy_result)
        
        return {
            'success': execution_result['success'],
            'strategy_result': strategy_result,
            'execution_result': execution_result
        }
        
    except Exception as e:
        print(f"âŒ æ™ºèƒ½å¯åŠ¨æµç¨‹å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'strategy_result': None,
            'execution_result': None
        } 