# æå‰åœ¨å­è¿›ç¨‹ä¸­æŠ‘åˆ¶åˆå§‹åŒ–æ—¥å¿—ï¼Œé¿å…é‡å¤å™ªå£°
try:
    import os as _early_os
    import multiprocessing as _early_mp
    if _early_os.getenv('SUPPRESS_INIT_LOGS', '0') == '1' and getattr(_early_mp.current_process(), 'name', 'MainProcess') != 'MainProcess':
        def print(*args, **kwargs):  # type: ignore[override]
            pass
except Exception:
    pass

print("ğŸ”§ [DEBUG] å¼€å§‹å¯¼å…¥utils.pyæ¨¡å—...")

import os
import re
import sys
import difflib
print("ğŸ”§ [DEBUG] åŸºç¡€æ¨¡å—å¯¼å…¥å®Œæˆ (os, re, sys, difflib)")

try:
    import diff_match_patch as dmp_module
    _DMP_AVAILABLE = True
    print("ğŸ”§ [DEBUG] diff-match-patch å¯¼å…¥æˆåŠŸ")
except ImportError:
    _DMP_AVAILABLE = False
    print("ğŸ”§ [DEBUG] diff-match-patch ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ difflib ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")

print("ğŸ”§ [DEBUG] å‡†å¤‡å¯¼å…¥sentence_transformers (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
try:
    from sentence_transformers import SentenceTransformer, util
    print("ğŸ”§ [DEBUG] sentence_transformers å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"ğŸ”§ [DEBUG] sentence_transformers å¯¼å…¥å¤±è´¥: {e}")
    raise

print("ğŸ”§ [DEBUG] å¼€å§‹å¯¼å…¥torch...")
import torch
print("ğŸ”§ [DEBUG] torch å¯¼å…¥æˆåŠŸ")

print("ğŸ”§ [DEBUG] å¼€å§‹å¯¼å…¥nltk...")
try:
    import nltk
    from nltk.tokenize import sent_tokenize, word_tokenize
    print("ğŸ”§ [DEBUG] nltk å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"ğŸ”§ [DEBUG] nltk å¯¼å…¥å¤±è´¥: {e}")

print("ğŸ”§ [DEBUG] å¼€å§‹å¯¼å…¥å…¶ä»–ä¾èµ–...")
import yaml
import pandas as pd
from collections import Counter
import uuid
from docx import Document
import textract
import json
import shutil
import multiprocessing
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import functools
import warnings
from multiprocessing import Process, Queue
import time
print("ğŸ”§ [DEBUG] æ‰€æœ‰å¯¼å…¥å®Œæˆ")

# æŠ‘åˆ¶multiprocessingç›¸å…³çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="multiprocessing")
warnings.filterwarnings("ignore", message="resource_tracker: process died unexpectedly")
warnings.filterwarnings("ignore", message="Some resources might leak")

# å½»åº•è§„é¿Python3.8+åœ¨joblib/lokyä¸‹å¶å‘çš„resource_tracker KeyErrorå™ªå£°
try:
    import multiprocessing.resource_tracker as _rt  # type: ignore
    # ä»…æ‰“ä¸€æ¬¡è¡¥ä¸
    if not getattr(_rt, "_patched_ignore_loky_semlock", False):
        _orig_register = _rt.register
        _orig_unregister = _rt.unregister

        def _safe_register(name, rtype):
            # lokyè‡ªè¡Œç®¡ç†semlockï¼Œé¿å…é‡å¤æ³¨å†Œ
            if rtype == 'semlock':
                return
            try:
                return _orig_register(name, rtype)
            except Exception:
                # é¿å…ä»»ä½•æ³¨å†Œé˜¶æ®µçš„å¼‚å¸¸å½±å“ä¸»æµç¨‹
                return

        def _safe_unregister(name, rtype):
            try:
                return _orig_unregister(name, rtype)
            except Exception:
                # å…³é”®ç‚¹ï¼šå¿½ç•¥é€€å‡ºé˜¶æ®µçš„KeyErrorç­‰å¼‚å¸¸
                return

        _rt.register = _safe_register  # type: ignore
        _rt.unregister = _safe_unregister  # type: ignore
        _rt._patched_ignore_loky_semlock = True  # type: ignore
except Exception:
    # å¦‚æœæ‰“è¡¥ä¸å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
    pass

# è®¾ç½®ç¯å¢ƒå˜é‡æ¥å‡å°‘è­¦å‘Š
os.environ['PYTHONWARNINGS'] = 'ignore::UserWarning'

# æ›´ä¸¥æ ¼çš„ä¸»è¿›ç¨‹æ£€æµ‹ï¼šåªæœ‰åœ¨çœŸæ­£çš„ä¸»è¿›ç¨‹å¯åŠ¨æ—¶æ‰æ˜¾ç¤ºåˆå§‹åŒ–æ—¥å¿—
def is_main_initialization_process():
    """æ£€æµ‹æ˜¯å¦ä¸ºä¸»åˆå§‹åŒ–è¿›ç¨‹ï¼Œé¿å…åœ¨å¤šè¿›ç¨‹ä»»åŠ¡ä¸­é‡å¤æ˜¾ç¤ºæ—¥å¿—"""
    print(f"ğŸ”§ [DEBUG] æ£€æŸ¥ä¸»è¿›ç¨‹çŠ¶æ€: SUPPRESS_INIT_LOGS={os.getenv('SUPPRESS_INIT_LOGS', '0')}")
    
    # å¦‚æœå·²ç»è®¾ç½®äº†æŠ‘åˆ¶æ ‡å¿—ï¼Œç›´æ¥è¿”å›False
    if _should_suppress_logs:
        print("ğŸ”§ [DEBUG] æŠ‘åˆ¶æ ‡å¿—å·²è®¾ç½®ï¼Œè·³è¿‡åˆå§‹åŒ–")
        return False
    
    # æ£€æŸ¥æ˜¯å¦åœ¨multiprocessingå­è¿›ç¨‹ä¸­
    if hasattr(multiprocessing.current_process(), 'name'):
        process_name = multiprocessing.current_process().name
        print(f"ğŸ”§ [DEBUG] å½“å‰è¿›ç¨‹å: {process_name}")
        if process_name != 'MainProcess':
            print("ğŸ”§ [DEBUG] éä¸»è¿›ç¨‹ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return False
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Flaskåº”ç”¨å¯åŠ¨è¿‡ç¨‹ä¸­
    is_flask_main = __name__ == '__main__' or os.getenv('FLASK_ENV') or 'werkzeug' in sys.modules
    print(f"ğŸ”§ [DEBUG] Flaskä¸»è¿›ç¨‹æ£€æŸ¥: __name__={__name__}, FLASK_ENV={os.getenv('FLASK_ENV')}, werkzeug in sys.modules={'werkzeug' in sys.modules}")
    print(f"ğŸ”§ [DEBUG] æœ€ç»ˆåˆ¤æ–­ç»“æœ: {is_flask_main}")
    
    return is_flask_main

# åœ¨æ¨¡å—å¯¼å…¥æ—¶å°±æ£€æŸ¥æ˜¯å¦åº”è¯¥æŠ‘åˆ¶æ—¥å¿—ï¼Œé¿å…åœ¨å­è¿›ç¨‹ä¸­é‡å¤æ˜¾ç¤º
_should_suppress_logs = os.getenv('SUPPRESS_INIT_LOGS', '0') == '1'

# å¦‚æœè®¾ç½®äº†æŠ‘åˆ¶æ ‡å¿—ï¼Œç›´æ¥è·³è¿‡æ‰€æœ‰åˆå§‹åŒ–æ—¥å¿—
if _should_suppress_logs:
    # é™é»˜æ¨¡å¼ï¼šä¸æ‰“å°ä»»ä½•åˆå§‹åŒ–æ—¥å¿—
    def silent_print(*args, **kwargs):
        pass
    # ä¸´æ—¶æ›¿æ¢printå‡½æ•°
    _original_print = print
    print = silent_print
import psutil
import time

# è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸ºspawnï¼Œè§£å†³CUDA forké—®é¢˜
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯
    pass

# æ¨èå¤šçº¿ç¨‹è®¾ç½®ï¼ˆé¿å…ä»…å•æ ¸ï¼‰
try:
    CPU_CORES = multiprocessing.cpu_count()
    WORKER_THREADS = min(CPU_CORES, 16)  # å¢åŠ å·¥ä½œçº¿ç¨‹æ•°
    
    # å¼ºåˆ¶è®¾ç½®æ‰€æœ‰ç›¸å…³çš„çº¿ç¨‹æ•°ç¯å¢ƒå˜é‡
    os.environ['OMP_NUM_THREADS'] = str(WORKER_THREADS)
    os.environ['MKL_NUM_THREADS'] = str(WORKER_THREADS)
    os.environ['NUMEXPR_NUM_THREADS'] = str(WORKER_THREADS)
    os.environ['OPENBLAS_NUM_THREADS'] = str(WORKER_THREADS)
    os.environ['VECLIB_MAXIMUM_THREADS'] = str(WORKER_THREADS)
    os.environ['NUMBA_NUM_THREADS'] = str(WORKER_THREADS)
    
    # è®¾ç½®PyTorchçº¿ç¨‹æ•°
    torch.set_num_threads(WORKER_THREADS)
    torch.set_num_interop_threads(WORKER_THREADS)
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°å¤šçº¿ç¨‹é…ç½®ä¿¡æ¯
    if is_main_initialization_process():
        print(f"æ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•°: {CPU_CORES}")
        print(f"ä½¿ç”¨å·¥ä½œçº¿ç¨‹æ•°: {WORKER_THREADS}")
        print(f"ç¯å¢ƒå˜é‡è®¾ç½®: OMP_NUM_THREADS={os.environ.get('OMP_NUM_THREADS')}")
except Exception as _e:
    print(f"å¤šçº¿ç¨‹é…ç½®å¤±è´¥: {_e}")

# å…è®¸åœ¨æ— ç½‘ç»œæ—¶é™çº§NLTKä¸æ¨¡å‹ä¾èµ–
print("ğŸ”§ [DEBUG] å¼€å§‹æ£€æŸ¥NLTKèµ„æº...")
try:
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰NLTKæ•°æ®
    try:
        from nltk.tokenize import sent_tokenize, word_tokenize
        # å¿«é€Ÿæµ‹è¯•æ˜¯å¦å¯ç”¨
        sent_tokenize("Test sentence.")
        word_tokenize("Test sentence.")
        print("ğŸ”§ [DEBUG] NLTKèµ„æºå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        _nltk_ok = True
    except LookupError:
        # æ•°æ®ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½
        print("ğŸ”§ [DEBUG] NLTKèµ„æºä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        
        # ä½¿ç”¨è¶…æ—¶æœºåˆ¶ä¸‹è½½NLTKæ•°æ®
        import signal
        
        def timeout_handler(signum, frame):
            raise TimeoutError("NLTKä¸‹è½½è¶…æ—¶")
        
        try:
            # è®¾ç½®30ç§’è¶…æ—¶
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(30)
            
            print("ğŸ”§ [DEBUG] ä¸‹è½½punktæ•°æ®åŒ…...")
            nltk.download('punkt', quiet=True)
            print("ğŸ”§ [DEBUG] ä¸‹è½½averaged_perceptron_taggeræ•°æ®åŒ…...")
            nltk.download('averaged_perceptron_tagger', quiet=True)
            
            signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            print("ğŸ”§ [DEBUG] NLTKèµ„æºä¸‹è½½å®Œæˆ")
            _nltk_ok = True
            
        except (TimeoutError, Exception) as download_error:
            signal.alarm(0)  # ç¡®ä¿å–æ¶ˆè¶…æ—¶
            print(f"ğŸ”§ [DEBUG] NLTKä¸‹è½½å¤±è´¥æˆ–è¶…æ—¶: {download_error}")
            print("ğŸ”§ [DEBUG] å°†ä½¿ç”¨ç®€æ˜“åˆ†è¯ä½œä¸ºåå¤‡æ–¹æ¡ˆ")
            _nltk_ok = False
except Exception as _e:
    print(f"ğŸ”§ [DEBUG] NLTKèµ„æºä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€æ˜“åˆ†è¯é™çº§: {_e}")
    _nltk_ok = False

print("ğŸ”§ [DEBUG] NLTKæ£€æŸ¥å®Œæˆ")

# ç®€æ˜“åˆ†å¥/åˆ†è¯é™çº§å®ç°
print("ğŸ”§ [DEBUG] å¯¼å…¥reæ¨¡å—...")
import re as _re
print("ğŸ”§ [DEBUG] å®šä¹‰ç®€æ˜“åˆ†è¯å‡½æ•°...")

def _simple_sent_tokenize(text: str):
    return [s for s in _re.split(r"(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+", text) if s.strip()]

print("ğŸ”§ [DEBUG] å®šä¹‰_simple_sent_tokenizeå®Œæˆ")

def _simple_word_tokenize(text: str):
    return [w for w in _re.findall(r"[\w\-]+", text.lower())]

print("ğŸ”§ [DEBUG] å®šä¹‰_simple_word_tokenizeå®Œæˆ")

# æ ¹æ®å¯ç”¨æ€§é€‰æ‹©åˆ†è¯å™¨
print("ğŸ”§ [DEBUG] é…ç½®åˆ†è¯å™¨...")
_sent_tokenize = sent_tokenize if _nltk_ok else _simple_sent_tokenize
_word_tokenize = word_tokenize if _nltk_ok else _simple_word_tokenize
print("ğŸ”§ [DEBUG] åˆ†è¯å™¨é…ç½®å®Œæˆ")

print("ğŸ”§ [DEBUG] å®šä¹‰monitor_cpu_usageå‡½æ•°...")
def monitor_cpu_usage():
    """ç›‘æ§CPUä½¿ç”¨æƒ…å†µ"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
        avg_cpu = sum(cpu_percent) / len(cpu_percent)
        active_cores = sum(1 for cpu in cpu_percent if cpu > 10)
        print(f"CPUä½¿ç”¨ç‡: å¹³å‡ {avg_cpu:.1f}%, æ´»è·ƒæ ¸å¿ƒæ•°: {active_cores}/{len(cpu_percent)}")
        
        # æ˜¾ç¤ºæ¯ä¸ªæ ¸å¿ƒçš„ä½¿ç”¨æƒ…å†µ
        if len(cpu_percent) <= 16:  # åªæ˜¾ç¤ºå‰16ä¸ªæ ¸å¿ƒ
            core_usage = [f"{i}:{cpu:.1f}%" for i, cpu in enumerate(cpu_percent[:16])]
            print(f"æ ¸å¿ƒä½¿ç”¨è¯¦æƒ…: {', '.join(core_usage)}")
        
        return avg_cpu, active_cores
    except Exception as e:
        print(f"CPUç›‘æ§å¤±è´¥: {e}")
        return 0, 0

def monitor_gpu_memory():
    """ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        if torch.cuda.is_available():
            gpu_memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            gpu_memory_free = gpu_memory_total - gpu_memory_used
            print(f"GPUå†…å­˜: å·²ç”¨ {gpu_memory_used:.2f}GB, æ€»è®¡ {gpu_memory_total:.2f}GB, å¯ç”¨ {gpu_memory_free:.2f}GB")
            return gpu_memory_used, gpu_memory_total, gpu_memory_free
        else:
            print("GPUä¸å¯ç”¨")
            return 0, 0, 0
    except Exception as e:
        print(f"GPUç›‘æ§å¤±è´¥: {e}")
        return 0, 0, 0

def cleanup_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print("GPUå†…å­˜å·²æ¸…ç†")
    except Exception as e:
        print(f"GPUå†…å­˜æ¸…ç†å¤±è´¥: {e}")

def extract_text_parallel(file_paths, max_workers=None):
    """å¹¶è¡Œæå–å¤šä¸ªæ–‡ä»¶çš„æ–‡æœ¬"""
    if max_workers is None:
        max_workers = min(multiprocessing.cpu_count(), 8)
    
    def extract_single_file(file_path):
        try:
            lines, html, temp_path = extract_formatted_text(file_path, is_storage_file=True)
            return file_path, lines, html, temp_path, None
        except Exception as e:
            return file_path, None, None, None, str(e)
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(extract_single_file, file_paths))
    
    return results

# é…ç½®
UPLOAD_FOLDER = 'Uploads'
STORAGE_FOLDER = 'storage'
SIMILARITIES_FILE = 'similarities.json'
ALLOWED_EXTENSIONS = {'pdf', 'docx', 'doc', 'txt', 'yaml', 'yml', 'xlsx', 'xls'}
MAX_SENTENCES_PER_SEGMENT = 20
SIMILARITY_THRESHOLD = 0.95

# å…¨å±€ä»»åŠ¡å–æ¶ˆçŠ¶æ€å­˜å‚¨
CANCELLED_TASKS = set()
_CANCELLED_TASKS_LOCK = threading.Lock()

# ä»»åŠ¡è¶…æ—¶æœºåˆ¶
TASK_TIMEOUTS = {}  # {task_id: start_time}
_TASK_TIMEOUTS_LOCK = threading.Lock()
TASK_TIMEOUT_SECONDS = 300  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚åˆå¤§æ–‡ä»¶å¤„ç†ä»»åŠ¡

# å¤šç”¨æˆ·ç¯å¢ƒç®¡ç†
USER_SESSIONS = {}  # {user_id: {tasks: set, last_activity: timestamp, ip: str}}
_USER_SESSIONS_LOCK = threading.Lock()
TASK_TO_USER = {}  # {task_id: user_id}
_TASK_TO_USER_LOCK = threading.Lock()

# ç³»ç»Ÿèµ„æºç›‘æ§
SYSTEM_RESOURCES = {
    'active_tasks': 0,
    'active_users': 0,
    'cpu_usage': 0.0,
    'memory_usage': 0.0,
    'gpu_memory_usage': 0.0
}
_SYSTEM_RESOURCES_LOCK = threading.Lock()

# è¿›ç¨‹ç®¡ç†æœºåˆ¶
ACTIVE_PROCESSES = {}  # {task_id: [process_objects]}
_ACTIVE_PROCESSES_LOCK = threading.Lock()

# è¶…æ—¶æ£€æŸ¥çº¿ç¨‹
_timeout_checker_thread = None
_timeout_checker_running = False
_timeout_checker_lock = threading.Lock()

# ç”¨æˆ·æ¸…ç†çº¿ç¨‹
_user_cleanup_thread = None
_user_cleanup_running = False
_user_cleanup_lock = threading.Lock()

def is_task_cancelled(task_id):
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ"""
    if not task_id:
        return False
    with _CANCELLED_TASKS_LOCK:
        return task_id in CANCELLED_TASKS

def mark_task_cancelled(task_id):
    """æ ‡è®°ä»»åŠ¡ä¸ºå·²å–æ¶ˆ"""
    if task_id:
        with _CANCELLED_TASKS_LOCK:
            CANCELLED_TASKS.add(task_id)
            print(f"ä»»åŠ¡ {task_id} å·²æ ‡è®°ä¸ºå–æ¶ˆ")
            # å¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹
            terminate_task_processes(task_id)

def clear_cancelled_task(task_id):
    """æ¸…é™¤ä»»åŠ¡çš„å–æ¶ˆçŠ¶æ€"""
    if task_id:
        with _CANCELLED_TASKS_LOCK:
            CANCELLED_TASKS.discard(task_id)

def get_user_id_from_request(request):
    """ä»è¯·æ±‚ä¸­è·å–ç”¨æˆ·IDï¼ˆåŸºäºIPåœ°å€å’ŒUser-Agentï¼‰"""
    import hashlib
    ip = request.remote_addr or 'unknown'
    user_agent = request.headers.get('User-Agent', 'unknown')
    user_string = f"{ip}_{user_agent}"
    return hashlib.md5(user_string.encode()).hexdigest()[:16]

def register_user_session(user_id, request):
    """æ³¨å†Œç”¨æˆ·ä¼šè¯"""
    with _USER_SESSIONS_LOCK:
        if user_id not in USER_SESSIONS:
            USER_SESSIONS[user_id] = {
                'tasks': set(),
                'last_activity': time.time(),
                'ip': request.remote_addr or 'unknown',
                'user_agent': request.headers.get('User-Agent', 'unknown')
            }
        else:
            USER_SESSIONS[user_id]['last_activity'] = time.time()
            USER_SESSIONS[user_id]['ip'] = request.remote_addr or 'unknown'
        
        # æ›´æ–°ç³»ç»Ÿèµ„æºç»Ÿè®¡
        with _SYSTEM_RESOURCES_LOCK:
            SYSTEM_RESOURCES['active_users'] = len(USER_SESSIONS)
    
    print(f"ç”¨æˆ·ä¼šè¯å·²æ³¨å†Œ: {user_id} (IP: {request.remote_addr})")

def register_task_to_user(task_id, user_id):
    """å°†ä»»åŠ¡æ³¨å†Œåˆ°ç”¨æˆ·"""
    with _TASK_TO_USER_LOCK:
        TASK_TO_USER[task_id] = user_id
    
    with _USER_SESSIONS_LOCK:
        if user_id in USER_SESSIONS:
            USER_SESSIONS[user_id]['tasks'].add(task_id)
            USER_SESSIONS[user_id]['last_activity'] = time.time()
    
    # æ›´æ–°ç³»ç»Ÿèµ„æºç»Ÿè®¡
    with _SYSTEM_RESOURCES_LOCK:
        SYSTEM_RESOURCES['active_tasks'] = len(TASK_TIMEOUTS)
    
    print(f"ä»»åŠ¡ {task_id} å·²æ³¨å†Œåˆ°ç”¨æˆ· {user_id}")

def cancel_user_tasks(user_id):
    """å–æ¶ˆæŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰ä»»åŠ¡"""
    cancelled_count = 0
    
    with _USER_SESSIONS_LOCK:
        if user_id not in USER_SESSIONS:
            return 0
        
        user_tasks = list(USER_SESSIONS[user_id]['tasks'])
    
    print(f"ç”¨æˆ· {user_id} é¡µé¢åˆ·æ–°ï¼šå¼€å§‹å–æ¶ˆ {len(user_tasks)} ä¸ªä»»åŠ¡")
    
    # ä¿å­˜ä»»åŠ¡åˆ—è¡¨ç”¨äºè¿›ç¨‹æ¸…ç†
    tasks_for_cleanup = user_tasks.copy()
    
    for task_id in user_tasks:
        print(f"ç”¨æˆ· {user_id} é¡µé¢åˆ·æ–°ï¼šå–æ¶ˆä»»åŠ¡ {task_id}")
        mark_task_cancelled(task_id)
        cancelled_count += 1
        
        # æ¸…ç†ä»»åŠ¡è®¡æ—¶
        with _TASK_TIMEOUTS_LOCK:
            TASK_TIMEOUTS.pop(task_id, None)
        
        # æ¸…ç†ä»»åŠ¡åˆ°ç”¨æˆ·çš„æ˜ å°„
        with _TASK_TO_USER_LOCK:
            TASK_TO_USER.pop(task_id, None)
        
        # ç»ˆæ­¢ä»»åŠ¡ç›¸å…³è¿›ç¨‹
        terminate_task_processes(task_id)
        clear_task_processes(task_id)
    
    # æ¸…ç†ç”¨æˆ·ä¼šè¯ä¸­çš„ä»»åŠ¡
    with _USER_SESSIONS_LOCK:
        if user_id in USER_SESSIONS:
            USER_SESSIONS[user_id]['tasks'].clear()
            USER_SESSIONS[user_id]['last_activity'] = time.time()
    
    if cancelled_count > 0:
        print(f"ç”¨æˆ· {user_id} é¡µé¢åˆ·æ–°ï¼šå·²å–æ¶ˆ {cancelled_count} ä¸ªä»»åŠ¡")
        
        # æ›´æ–°ç³»ç»Ÿèµ„æºç»Ÿè®¡
        with _SYSTEM_RESOURCES_LOCK:
            SYSTEM_RESOURCES['active_tasks'] = len(TASK_TIMEOUTS)
        
        # ä½¿ç”¨ä¿å­˜çš„ä»»åŠ¡åˆ—è¡¨è¿›è¡Œè¿›ç¨‹æ¸…ç†
        killed_count = force_cleanup_user_processes_with_tasks(user_id, tasks_for_cleanup)
        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªè¿›ç¨‹")
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿è¿›ç¨‹è¢«æ¸…ç†
        time.sleep(0.5)
        
        # å†æ¬¡å¼ºåˆ¶æ¸…ç†multiprocessingè¿›ç¨‹
        kill_all_multiprocessing_processes()
        
        # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰pt_main_threadè¿›ç¨‹
        force_kill_pt_main_thread_processes_with_tasks(user_id, tasks_for_cleanup)
        
        # é¢å¤–æ¸…ç†ï¼šæ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹ï¼ˆé™¤äº†ä¸»æœåŠ¡è¿›ç¨‹ï¼‰
        force_kill_all_pt_main_thread_processes_safe()
        
        print(f"ç”¨æˆ· {user_id} é¡µé¢åˆ·æ–°ï¼šå·²å®Œæˆè¿›ç¨‹æ¸…ç†")
    
    return cancelled_count

def cancel_all_running_tasks():
    """å–æ¶ˆæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼ˆç”¨äºé¡µé¢åˆ·æ–°æ£€æµ‹ï¼‰"""
    cancelled_count = 0
    with _TASK_TIMEOUTS_LOCK:
        # è·å–æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ID
        running_tasks = list(TASK_TIMEOUTS.keys())
        
    for task_id in running_tasks:
        print(f"é¡µé¢åˆ·æ–°æ£€æµ‹ï¼šå–æ¶ˆæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ {task_id}")
        mark_task_cancelled(task_id)
        cancelled_count += 1
        
        # æ¸…ç†ä»»åŠ¡è®¡æ—¶
        with _TASK_TIMEOUTS_LOCK:
            TASK_TIMEOUTS.pop(task_id, None)
        
        # ç»ˆæ­¢ä»»åŠ¡ç›¸å…³è¿›ç¨‹
        terminate_task_processes(task_id)
        clear_task_processes(task_id)
    
    if cancelled_count > 0:
        print(f"é¡µé¢åˆ·æ–°æ£€æµ‹ï¼šå·²å–æ¶ˆ {cancelled_count} ä¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")
        
        # ç«‹å³å¼ºåˆ¶æ¸…ç†æ‰€æœ‰multiprocessingè¿›ç¨‹
        print("é¡µé¢åˆ·æ–°æ£€æµ‹ï¼šå¼€å§‹å¼ºåˆ¶æ¸…ç†multiprocessingè¿›ç¨‹...")
        kill_all_multiprocessing_processes()
        
        # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰Pythonè¿›ç¨‹
        print("é¡µé¢åˆ·æ–°æ£€æµ‹ï¼šå¼€å§‹å¼ºåˆ¶æ¸…ç†Pythonè¿›ç¨‹...")
        force_kill_all_python_processes()
        
        # é¢å¤–ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿è¿›ç¨‹è¢«æ¸…ç†
        time.sleep(0.5)
        
        print("é¡µé¢åˆ·æ–°æ£€æµ‹ï¼šè¿›ç¨‹æ¸…ç†å®Œæˆ")
    
    return cancelled_count

def force_cleanup_user_processes(user_id):
    """å¼ºåˆ¶æ¸…ç†æŒ‡å®šç”¨æˆ·çš„è¿›ç¨‹"""
    try:
        import psutil
        import signal
        current_pid = os.getpid()
        killed_count = 0
        
        # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰ä»»åŠ¡
        with _USER_SESSIONS_LOCK:
            if user_id not in USER_SESSIONS:
                return 0
            user_tasks = list(USER_SESSIONS[user_id]['tasks'])
        
        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå¼€å§‹æ¸…ç†ä»»åŠ¡ {user_tasks}")
        
        # ç¬¬ä¸€è½®ï¼šæŸ¥æ‰¾å¹¶ç»ˆæ­¢è¯¥ç”¨æˆ·çš„ç›´æ¥å­è¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['ppid'] == current_pid and proc.info['name'] == 'python':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    # æ£€æŸ¥æ˜¯å¦æ˜¯multiprocessingç›¸å…³è¿›ç¨‹
                    if any(task_id in cmdline for task_id in user_tasks):
                        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå‘ç°ç›¸å…³è¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        proc.terminate()
                        killed_count += 1
                        # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            print(f"è¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"è¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        # ç¬¬äºŒè½®ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä»»åŠ¡IDçš„è¿›ç¨‹ï¼ˆåŒ…æ‹¬å­¤å„¿è¿›ç¨‹ï¼‰
        for task_id in user_tasks:
            for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
                try:
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    if task_id in cmdline and proc.info['name'] == 'python':
                        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå‘ç°ä»»åŠ¡ {task_id} çš„è¿›ç¨‹ {proc.info['pid']} (PPID: {proc.info['ppid']})")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"è¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                    pass
        
        # ç¬¬ä¸‰è½®ï¼šå¼ºåˆ¶æ¸…ç†æ‰€æœ‰pt_main_threadè¿›ç¨‹ï¼ˆè¿™äº›é€šå¸¸æ˜¯PyTorchç›¸å…³è¿›ç¨‹ï¼‰
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'pt_main_thread':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„è¿›ç¨‹
                    if any(task_id in cmdline for task_id in user_tasks):
                        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå‘ç°pt_main_threadè¿›ç¨‹ {proc.info['pid']}")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªè¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†æ—¶å‡ºé”™: {e}")
        return 0

def force_cleanup_user_processes_with_tasks(user_id, user_tasks):
    """ä½¿ç”¨æŒ‡å®šä»»åŠ¡åˆ—è¡¨å¼ºåˆ¶æ¸…ç†ç”¨æˆ·è¿›ç¨‹"""
    try:
        import psutil
        import signal
        current_pid = os.getpid()
        killed_count = 0
        
        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå¼€å§‹æ¸…ç†ä»»åŠ¡ {user_tasks}")
        
        # ç¬¬ä¸€è½®ï¼šæŸ¥æ‰¾å¹¶ç»ˆæ­¢è¯¥ç”¨æˆ·çš„ç›´æ¥å­è¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['ppid'] == current_pid and proc.info['name'] == 'python':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    # æ£€æŸ¥æ˜¯å¦æ˜¯multiprocessingç›¸å…³è¿›ç¨‹
                    if any(task_id in cmdline for task_id in user_tasks):
                        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå‘ç°ç›¸å…³è¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        proc.terminate()
                        killed_count += 1
                        # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            print(f"è¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"è¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        # ç¬¬äºŒè½®ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä»»åŠ¡IDçš„è¿›ç¨‹ï¼ˆåŒ…æ‹¬å­¤å„¿è¿›ç¨‹ï¼‰
        for task_id in user_tasks:
            for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
                try:
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    if task_id in cmdline and proc.info['name'] == 'python':
                        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå‘ç°ä»»åŠ¡ {task_id} çš„è¿›ç¨‹ {proc.info['pid']} (PPID: {proc.info['ppid']})")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"è¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                    pass
        
        # ç¬¬ä¸‰è½®ï¼šå¼ºåˆ¶æ¸…ç†æ‰€æœ‰pt_main_threadè¿›ç¨‹ï¼ˆè¿™äº›é€šå¸¸æ˜¯PyTorchç›¸å…³è¿›ç¨‹ï¼‰
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'pt_main_thread':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„è¿›ç¨‹
                    if any(task_id in cmdline for task_id in user_tasks):
                        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå‘ç°pt_main_threadè¿›ç¨‹ {proc.info['pid']}")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªè¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"ç”¨æˆ· {user_id} è¿›ç¨‹æ¸…ç†æ—¶å‡ºé”™: {e}")
        return 0

def force_kill_pt_main_thread_processes(user_id):
    """å¼ºåˆ¶æ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹"""
    try:
        import psutil
        killed_count = 0
        
        # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰ä»»åŠ¡
        with _USER_SESSIONS_LOCK:
            if user_id not in USER_SESSIONS:
                return 0
            user_tasks = list(USER_SESSIONS[user_id]['tasks'])
        
        print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹ï¼šå¼€å§‹æ¸…ç†ä»»åŠ¡ {user_tasks}")
        
        # æŸ¥æ‰¾æ‰€æœ‰pt_main_threadè¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'pt_main_thread':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„è¿›ç¨‹
                    if any(task_id in cmdline for task_id in user_tasks):
                        print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†ï¼šå‘ç°pt_main_threadè¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªè¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

def force_kill_pt_main_thread_processes_with_tasks(user_id, user_tasks):
    """ä½¿ç”¨æŒ‡å®šä»»åŠ¡åˆ—è¡¨å¼ºåˆ¶æ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹"""
    try:
        import psutil
        killed_count = 0
        current_pid = os.getpid()
        
        print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹ï¼šå¼€å§‹æ¸…ç†ä»»åŠ¡ {user_tasks}")
        
        # æŸ¥æ‰¾æ‰€æœ‰pt_main_threadè¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'pt_main_thread':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    
                    # è·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹
                    if proc.info['pid'] == current_pid:
                        continue
                    
                    # è·³è¿‡åŒ…å«app.pyæˆ–start_app.pyçš„è¿›ç¨‹ï¼ˆä¸»æœåŠ¡è¿›ç¨‹ï¼‰
                    if 'app.py' in cmdline or 'start_app.py' in cmdline:
                        continue
                    
                    # åªå¤„ç†å­è¿›ç¨‹
                    if proc.info['ppid'] != current_pid:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„è¿›ç¨‹
                    if any(task_id in cmdline for task_id in user_tasks):
                        print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†ï¼šå‘ç°pt_main_threadè¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªè¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"ç”¨æˆ· {user_id} å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

def force_kill_all_pt_main_thread_processes():
    """å¼ºåˆ¶æ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹ï¼ˆæ¿€è¿›æ–¹æ³•ï¼‰"""
    try:
        import psutil
        killed_count = 0
        current_pid = os.getpid()
        
        print("å¼ºåˆ¶æ¸…ç†ï¼šå¼€å§‹æ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹")
        
        # æŸ¥æ‰¾æ‰€æœ‰pt_main_threadè¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'pt_main_thread':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    
                    # è·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹
                    if proc.info['pid'] == current_pid:
                        print(f"å¼ºåˆ¶æ¸…ç†ï¼šè·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹ {proc.info['pid']}")
                        continue
                    
                    # è·³è¿‡åŒ…å«app.pyæˆ–start_app.pyçš„è¿›ç¨‹ï¼ˆä¸»æœåŠ¡è¿›ç¨‹ï¼‰
                    if 'app.py' in cmdline or 'start_app.py' in cmdline:
                        print(f"å¼ºåˆ¶æ¸…ç†ï¼šè·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        continue
                    
                    # åªæ€æ­»å­è¿›ç¨‹
                    if proc.info['ppid'] == current_pid:
                        print(f"å¼ºåˆ¶æ¸…ç†ï¼šå‘ç°å­pt_main_threadè¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        proc.terminate()
                        killed_count += 1
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                            proc.kill()
                            try:
                                proc.wait(timeout=1)
                            except psutil.TimeoutExpired:
                                print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
                    else:
                        print(f"å¼ºåˆ¶æ¸…ç†ï¼šè·³è¿‡éå­è¿›ç¨‹ {proc.info['pid']} (PPID: {proc.info['ppid']})")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"å¼ºåˆ¶æ¸…ç†ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªpt_main_threadè¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"å¼ºåˆ¶æ¸…ç†pt_main_threadè¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

def cleanup_task_specific_processes(task_id):
    """æ¸©å’Œåœ°æ¸…ç†ç‰¹å®šä»»åŠ¡ç›¸å…³çš„è¿›ç¨‹ï¼Œä¸å½±å“ç³»ç»Ÿè¿›ç¨‹"""
    try:
        import psutil
        killed_count = 0
        current_pid = os.getpid()
        
        print(f"æ¸©å’Œæ¸…ç†ï¼šå¼€å§‹æ¸…ç†ä»»åŠ¡ {task_id} çš„ç›¸å…³è¿›ç¨‹")
        
        # åªæŸ¥æ‰¾æ˜ç¡®åŒ…å«ä»»åŠ¡IDçš„è¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['pid'] == current_pid:
                    continue
                    
                cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                
                # åªæ¸…ç†æ˜ç¡®åŒ…å«ä»»åŠ¡IDçš„è¿›ç¨‹
                if task_id in cmdline and proc.info['name'] == 'python':
                    # é¢å¤–æ£€æŸ¥ï¼šè·³è¿‡é‡è¦çš„ç³»ç»Ÿè¿›ç¨‹
                    if any(system_keyword in cmdline for system_keyword in [
                        'resource_tracker', 'forkserver', 'semlock_tracker', 'app.py', 'start_app.py'
                    ]):
                        print(f"æ¸©å’Œæ¸…ç†ï¼šè·³è¿‡ç³»ç»Ÿè¿›ç¨‹ {proc.info['pid']}: {cmdline[:100]}...")
                        continue
                    
                    print(f"æ¸©å’Œæ¸…ç†ï¼šå‘ç°ä»»åŠ¡è¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                    proc.terminate()
                    killed_count += 1
                    try:
                        proc.wait(timeout=2)
                    except psutil.TimeoutExpired:
                        print(f"æ¸©å’Œæ¸…ç†ï¼šè¿›ç¨‹ {proc.info['pid']} æœªå“åº”ï¼Œè·³è¿‡å¼ºåˆ¶æ€æ­»")
                        # æ¸©å’Œæ¸…ç†æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ kill()
                        
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"æ¸©å’Œæ¸…ç†ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªä»»åŠ¡ç›¸å…³è¿›ç¨‹")
        else:
            print("æ¸©å’Œæ¸…ç†ï¼šæœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„ä»»åŠ¡è¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"æ¸©å’Œæ¸…ç†ä»»åŠ¡è¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

def force_kill_all_pt_main_thread_processes_safe():
    """å®‰å…¨åœ°æ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹ï¼ˆé™¤äº†ä¸»æœåŠ¡è¿›ç¨‹ï¼‰"""
    try:
        import psutil
        killed_count = 0
        current_pid = os.getpid()
        
        print("å®‰å…¨æ¸…ç†ï¼šå¼€å§‹æ€æ­»æ‰€æœ‰pt_main_threadè¿›ç¨‹ï¼ˆé™¤äº†ä¸»æœåŠ¡è¿›ç¨‹ï¼‰")
        
        # æŸ¥æ‰¾æ‰€æœ‰pt_main_threadè¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'ppid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'pt_main_thread':
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    
                    # è·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹
                    if proc.info['pid'] == current_pid:
                        print(f"å®‰å…¨æ¸…ç†ï¼šè·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹ {proc.info['pid']}")
                        continue
                    
                    # è·³è¿‡åŒ…å«app.pyæˆ–start_app.pyçš„è¿›ç¨‹ï¼ˆä¸»æœåŠ¡è¿›ç¨‹ï¼‰
                    if 'app.py' in cmdline or 'start_app.py' in cmdline:
                        print(f"å®‰å…¨æ¸…ç†ï¼šè·³è¿‡ä¸»æœåŠ¡è¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                        continue
                    
                    # æ€æ­»æ‰€æœ‰å…¶ä»–pt_main_threadè¿›ç¨‹
                    print(f"å®‰å…¨æ¸…ç†ï¼šå‘ç°pt_main_threadè¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                    proc.terminate()
                    killed_count += 1
                    try:
                        proc.wait(timeout=1)
                    except psutil.TimeoutExpired:
                        print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                        proc.kill()
                        try:
                            proc.wait(timeout=1)
                        except psutil.TimeoutExpired:
                            print(f"pt_main_threadè¿›ç¨‹ {proc.info['pid']} æ— æ³•è¢«æ€æ­»")
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        if killed_count > 0:
            print(f"å®‰å…¨æ¸…ç†ï¼šå·²ç»ˆæ­¢ {killed_count} ä¸ªpt_main_threadè¿›ç¨‹")
        
        return killed_count
    except Exception as e:
        print(f"å®‰å…¨æ¸…ç†pt_main_threadè¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

def get_system_resources():
    """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    with _SYSTEM_RESOURCES_LOCK:
        return SYSTEM_RESOURCES.copy()

def update_system_resources():
    """æ›´æ–°ç³»ç»Ÿèµ„æºç»Ÿè®¡"""
    try:
        import psutil
        
        # æ›´æ–°CPUå’Œå†…å­˜ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        
        # æ›´æ–°GPUå†…å­˜ä½¿ç”¨ç‡
        gpu_memory = 0.0
        try:
            import torch
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / (1024**3)  # GB
        except:
            pass
        
        with _SYSTEM_RESOURCES_LOCK:
            SYSTEM_RESOURCES['cpu_usage'] = cpu_percent
            SYSTEM_RESOURCES['memory_usage'] = memory.percent
            SYSTEM_RESOURCES['gpu_memory_usage'] = gpu_memory
            SYSTEM_RESOURCES['active_tasks'] = len(TASK_TIMEOUTS)
            SYSTEM_RESOURCES['active_users'] = len(USER_SESSIONS)
        
        return SYSTEM_RESOURCES.copy()
    except Exception as e:
        print(f"æ›´æ–°ç³»ç»Ÿèµ„æºæ—¶å‡ºé”™: {e}")
        return SYSTEM_RESOURCES.copy()

def cleanup_inactive_users():
    """æ¸…ç†éæ´»è·ƒç”¨æˆ·ä¼šè¯"""
    current_time = time.time()
    inactive_threshold = 3600  # 1å°æ—¶æ— æ´»åŠ¨åˆ™æ¸…ç†
    cleaned_count = 0
    
    with _USER_SESSIONS_LOCK:
        inactive_users = []
        for user_id, session in USER_SESSIONS.items():
            if current_time - session['last_activity'] > inactive_threshold:
                inactive_users.append(user_id)
        
        for user_id in inactive_users:
            # å–æ¶ˆè¯¥ç”¨æˆ·çš„æ‰€æœ‰ä»»åŠ¡
            user_tasks = list(session['tasks'])
            for task_id in user_tasks:
                mark_task_cancelled(task_id)
                with _TASK_TIMEOUTS_LOCK:
                    TASK_TIMEOUTS.pop(task_id, None)
                with _TASK_TO_USER_LOCK:
                    TASK_TO_USER.pop(task_id, None)
                terminate_task_processes(task_id)
                clear_task_processes(task_id)
            
            # åˆ é™¤ç”¨æˆ·ä¼šè¯
            del USER_SESSIONS[user_id]
            cleaned_count += 1
            print(f"æ¸…ç†éæ´»è·ƒç”¨æˆ·: {user_id} (IP: {session['ip']})")
    
    if cleaned_count > 0:
        print(f"å·²æ¸…ç† {cleaned_count} ä¸ªéæ´»è·ƒç”¨æˆ·ä¼šè¯")
        # æ›´æ–°ç³»ç»Ÿèµ„æºç»Ÿè®¡
        with _SYSTEM_RESOURCES_LOCK:
            SYSTEM_RESOURCES['active_users'] = len(USER_SESSIONS)
    
    return cleaned_count

def start_user_cleanup_checker():
    """å¯åŠ¨ç”¨æˆ·æ¸…ç†æ£€æŸ¥å™¨"""
    global _user_cleanup_thread, _user_cleanup_running
    
    with _user_cleanup_lock:
        if _user_cleanup_running:
            return
        
        _user_cleanup_running = True
        _user_cleanup_thread = threading.Thread(target=_user_cleanup_worker, daemon=True)
        _user_cleanup_thread.start()
        print("ç”¨æˆ·æ¸…ç†æ£€æŸ¥å™¨å·²å¯åŠ¨")

def stop_user_cleanup_checker():
    """åœæ­¢ç”¨æˆ·æ¸…ç†æ£€æŸ¥å™¨"""
    global _user_cleanup_running
    
    with _user_cleanup_lock:
        _user_cleanup_running = False
        print("ç”¨æˆ·æ¸…ç†æ£€æŸ¥å™¨å·²åœæ­¢")

def _user_cleanup_worker():
    """ç”¨æˆ·æ¸…ç†å·¥ä½œçº¿ç¨‹"""
    global _user_cleanup_running
    
    while _user_cleanup_running:
        try:
            # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡éæ´»è·ƒç”¨æˆ·
            time.sleep(300)
            if _user_cleanup_running:
                cleaned_count = cleanup_inactive_users()
                if cleaned_count > 0:
                    print(f"å®šæœŸæ¸…ç†ï¼šå·²æ¸…ç† {cleaned_count} ä¸ªéæ´»è·ƒç”¨æˆ·")
                # æ£€æŸ¥é•¿æ—¶é—´éšè—é¡µé¢çš„ç”¨æˆ·ä»»åŠ¡
                cleanup_hidden_page_tasks()
        except Exception as e:
            print(f"ç”¨æˆ·æ¸…ç†æ£€æŸ¥å™¨å‡ºé”™: {e}")
            time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­

def cleanup_hidden_page_tasks():
    """æ¸…ç†é•¿æ—¶é—´éšè—é¡µé¢çš„ç”¨æˆ·ä»»åŠ¡"""
    try:
        current_time = time.time()
        hidden_threshold = 60  # é¡µé¢éšè—è¶…è¿‡60ç§’çš„ä»»åŠ¡å°†è¢«å–æ¶ˆ
        
        with _USER_SESSIONS_LOCK:
            for user_id, session_info in list(USER_SESSIONS.items()):
                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰éšè—é¡µé¢æ ‡è®°
                if session_info.get('page_hidden', False):
                    hidden_time = session_info.get('page_hidden_time', 0)
                    if current_time - hidden_time > hidden_threshold:
                        # å–æ¶ˆè¯¥ç”¨æˆ·çš„æ‰€æœ‰ä»»åŠ¡
                        user_tasks = list(session_info['tasks'])
                        if user_tasks:
                            print(f"ç”¨æˆ· {user_id} é¡µé¢éšè—è¶…è¿‡{hidden_threshold}ç§’ï¼Œå–æ¶ˆ {len(user_tasks)} ä¸ªä»»åŠ¡")
                            for task_id in user_tasks:
                                mark_task_cancelled(task_id)
                            # æ¸…ç†ç”¨æˆ·ä¼šè¯
                            del USER_SESSIONS[user_id]
    except Exception as e:
        print(f"æ¸…ç†éšè—é¡µé¢ä»»åŠ¡æ—¶å‡ºé”™: {e}")

def mark_user_page_hidden(user_id):
    """æ ‡è®°ç”¨æˆ·é¡µé¢ä¸ºéšè—çŠ¶æ€"""
    with _USER_SESSIONS_LOCK:
        if user_id in USER_SESSIONS:
            USER_SESSIONS[user_id]['page_hidden'] = True
            USER_SESSIONS[user_id]['page_hidden_time'] = time.time()

def mark_user_page_visible(user_id):
    """æ ‡è®°ç”¨æˆ·é¡µé¢ä¸ºå¯è§çŠ¶æ€"""
    with _USER_SESSIONS_LOCK:
        if user_id in USER_SESSIONS:
            USER_SESSIONS[user_id]['page_hidden'] = False
            USER_SESSIONS[user_id].pop('page_hidden_time', None)

def cleanup_old_cancelled_tasks():
    """æ¸…ç†è¿‡æœŸçš„å–æ¶ˆä»»åŠ¡ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ—¶é—´æˆ³æœºåˆ¶æ¥æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡
    # æš‚æ—¶ä¿æŒç®€å•ï¼Œåœ¨ä»»åŠ¡å®Œæˆæ—¶æ¸…ç†
    pass

def start_task_timer(task_id):
    """å¼€å§‹ä»»åŠ¡è®¡æ—¶"""
    if task_id:
        with _TASK_TIMEOUTS_LOCK:
            TASK_TIMEOUTS[task_id] = time.time()
            print(f"ä»»åŠ¡ {task_id} å¼€å§‹è®¡æ—¶")
            # å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼Œå¯åŠ¨è¶…æ—¶æ£€æŸ¥å™¨
            if len(TASK_TIMEOUTS) == 1:
                start_timeout_checker()

def check_task_timeout(task_id):
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¶…æ—¶"""
    if not task_id:
        return False
    
    with _TASK_TIMEOUTS_LOCK:
        if task_id in TASK_TIMEOUTS:
            start_time = TASK_TIMEOUTS[task_id]
            elapsed = time.time() - start_time
            if elapsed > TASK_TIMEOUT_SECONDS:
                print(f"ä»»åŠ¡ {task_id} è¶…æ—¶ ({elapsed:.1f}ç§’)ï¼Œè‡ªåŠ¨å–æ¶ˆå¹¶ç»ˆæ­¢è¿›ç¨‹")
                mark_task_cancelled(task_id)
                # å¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹
                terminate_task_processes(task_id)
                return True
    return False

def clear_task_timer(task_id):
    """æ¸…é™¤ä»»åŠ¡è®¡æ—¶"""
    if task_id:
        with _TASK_TIMEOUTS_LOCK:
            TASK_TIMEOUTS.pop(task_id, None)
            print(f"ä»»åŠ¡ {task_id} è®¡æ—¶å·²æ¸…é™¤")
            # å¦‚æœæ²¡æœ‰ä»»åŠ¡äº†ï¼Œåœæ­¢è¶…æ—¶æ£€æŸ¥å™¨
            if len(TASK_TIMEOUTS) == 0:
                stop_timeout_checker()

def register_active_process(task_id, process):
    """æ³¨å†Œæ´»è·ƒè¿›ç¨‹"""
    if task_id and process:
        with _ACTIVE_PROCESSES_LOCK:
            if task_id not in ACTIVE_PROCESSES:
                ACTIVE_PROCESSES[task_id] = []
            ACTIVE_PROCESSES[task_id].append(process)
            print(f"ä»»åŠ¡ {task_id} æ³¨å†Œäº†è¿›ç¨‹ {process.pid}")

def terminate_task_processes(task_id):
    """å¼ºåˆ¶ç»ˆæ­¢ä»»åŠ¡çš„æ‰€æœ‰è¿›ç¨‹"""
    if not task_id:
        return
    
    with _ACTIVE_PROCESSES_LOCK:
        if task_id in ACTIVE_PROCESSES:
            processes = ACTIVE_PROCESSES[task_id]
            for process in processes:
                try:
                    if process.is_alive():
                        print(f"å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ {process.pid}")
                        process.terminate()
                        process.join(timeout=5)  # ç­‰å¾…5ç§’
                        if process.is_alive():
                            print(f"è¿›ç¨‹ {process.pid} æœªå“åº”ï¼Œå¼ºåˆ¶æ€æ­»")
                            process.kill()
                except Exception as e:
                    print(f"ç»ˆæ­¢è¿›ç¨‹æ—¶å‡ºé”™: {e}")
            del ACTIVE_PROCESSES[task_id]
            print(f"ä»»åŠ¡ {task_id} çš„æ‰€æœ‰è¿›ç¨‹å·²ç»ˆæ­¢")

def clear_task_processes(task_id):
    """æ¸…é™¤ä»»åŠ¡è¿›ç¨‹è®°å½•"""
    if task_id:
        with _ACTIVE_PROCESSES_LOCK:
            ACTIVE_PROCESSES.pop(task_id, None)

def _timeout_checker_worker():
    """è¶…æ—¶æ£€æŸ¥å·¥ä½œçº¿ç¨‹"""
    global _timeout_checker_running
    while _timeout_checker_running:
        try:
            with _TASK_TIMEOUTS_LOCK:
                current_time = time.time()
                expired_tasks = []
                for task_id, start_time in TASK_TIMEOUTS.items():
                    if current_time - start_time > TASK_TIMEOUT_SECONDS:
                        expired_tasks.append(task_id)
                
                for task_id in expired_tasks:
                    elapsed_time = current_time - TASK_TIMEOUTS[task_id]
                    print(f"è¶…æ—¶æ£€æŸ¥å™¨å‘ç°ä»»åŠ¡ {task_id} è¶…æ—¶ï¼Œå¼ºåˆ¶ç»ˆæ­¢ (è¿è¡Œæ—¶é—´: {elapsed_time:.1f}ç§’, è¶…æ—¶é˜ˆå€¼: {TASK_TIMEOUT_SECONDS}ç§’)")
                    mark_task_cancelled(task_id)
                    TASK_TIMEOUTS.pop(task_id, None)
                    # å¼ºåˆ¶æ¸…ç†è¯¥ä»»åŠ¡çš„æ‰€æœ‰è¿›ç¨‹
                    terminate_task_processes(task_id)
                    # é¢å¤–æ¸…ç†æ‰€æœ‰multiprocessingè¿›ç¨‹
                    kill_all_multiprocessing_processes()
                    # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰Pythonè¿›ç¨‹
                    force_kill_all_python_processes()
            
            time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
        except Exception as e:
            print(f"è¶…æ—¶æ£€æŸ¥å™¨å‡ºé”™: {e}")
            time.sleep(5)

def start_timeout_checker():
    """å¯åŠ¨è¶…æ—¶æ£€æŸ¥çº¿ç¨‹"""
    global _timeout_checker_thread, _timeout_checker_running
    
    with _timeout_checker_lock:
        if not _timeout_checker_running:
            _timeout_checker_running = True
            _timeout_checker_thread = threading.Thread(target=_timeout_checker_worker, daemon=True)
            _timeout_checker_thread.start()
            print("è¶…æ—¶æ£€æŸ¥å™¨å·²å¯åŠ¨")

def stop_timeout_checker():
    """åœæ­¢è¶…æ—¶æ£€æŸ¥çº¿ç¨‹"""
    global _timeout_checker_running
    
    with _timeout_checker_lock:
        _timeout_checker_running = False
        print("è¶…æ—¶æ£€æŸ¥å™¨å·²åœæ­¢")

def force_cleanup_all_processes():
    """å¼ºåˆ¶æ¸…ç†æ‰€æœ‰æ´»è·ƒè¿›ç¨‹"""
    with _ACTIVE_PROCESSES_LOCK:
        for task_id, processes in list(ACTIVE_PROCESSES.items()):
            print(f"å¼ºåˆ¶æ¸…ç†ä»»åŠ¡ {task_id} çš„ {len(processes)} ä¸ªè¿›ç¨‹")
            for process in processes:
                try:
                    if process.is_alive():
                        print(f"å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ {process.pid}")
                        process.terminate()
                        process.join(timeout=2)
                        if process.is_alive():
                            print(f"è¿›ç¨‹ {process.pid} æœªå“åº”ï¼Œå¼ºåˆ¶æ€æ­»")
                            process.kill()
                except Exception as e:
                    print(f"æ¸…ç†è¿›ç¨‹æ—¶å‡ºé”™: {e}")
        ACTIVE_PROCESSES.clear()
        print("æ‰€æœ‰è¿›ç¨‹å·²å¼ºåˆ¶æ¸…ç†")

def kill_all_multiprocessing_processes():
    """æ™ºèƒ½æ€æ­»åº”ç”¨ç›¸å…³çš„multiprocessingè¿›ç¨‹ï¼Œä¿æŠ¤ç³»ç»Ÿé‡è¦è¿›ç¨‹"""
    try:
        import psutil
        current_pid = os.getpid()
        killed_count = 0
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'ppid']):
            try:
                if proc.info['name'] == 'python' and proc.info['pid'] != current_pid:
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    
                    # æ›´ç²¾ç¡®çš„è¿›ç¨‹è¯†åˆ«ï¼šåªæ€æ­»æˆ‘ä»¬åº”ç”¨ç›¸å…³çš„è¿›ç¨‹
                    should_kill = False
                    
                    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬åº”ç”¨çš„å·¥ä½œè¿›ç¨‹
                    if 'spawn_main' in cmdline and any(keyword in cmdline for keyword in [
                        'storage_compare', 'keyword_search', 'single_compare', 'doc_similarity'
                    ]):
                        should_kill = True
                        print(f"å‘ç°åº”ç”¨å·¥ä½œè¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                    
                    # 2. è·³è¿‡é‡è¦çš„ç³»ç»Ÿè¿›ç¨‹ï¼ˆresource_trackerç­‰ï¼‰
                    elif 'resource_tracker' in cmdline:
                        print(f"è·³è¿‡ç³»ç»Ÿèµ„æºè·Ÿè¸ªè¿›ç¨‹ {proc.info['pid']}")
                        should_kill = False
                    
                    # 3. æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬åº”ç”¨çš„ç›´æ¥å­è¿›ç¨‹
                    elif proc.info['ppid'] == current_pid and 'multiprocessing' in cmdline:
                        should_kill = True
                        print(f"å‘ç°åº”ç”¨å­è¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                    
                    if should_kill:
                        proc.terminate()
                        killed_count += 1
                        # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢
                        try:
                            proc.wait(timeout=2)
                        except psutil.TimeoutExpired:
                            print(f"è¿›ç¨‹ {proc.info['pid']} æœªå“åº”terminateï¼Œå¼ºåˆ¶æ€æ­»")
                            proc.kill()
                            
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                pass
        
        print(f"å·²æ€æ­» {killed_count} ä¸ªmultiprocessingè¿›ç¨‹")
        return killed_count
    except Exception as e:
        print(f"æ€æ­»multiprocessingè¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

def force_kill_all_python_processes():
    """æ™ºèƒ½æ€æ­»åº”ç”¨ç›¸å…³çš„Pythonè¿›ç¨‹ï¼Œä¿æŠ¤ç³»ç»Ÿé‡è¦è¿›ç¨‹"""
    try:
        import psutil
        current_pid = os.getpid()
        killed_count = 0
        
        # è¿›è¡Œå¤šæ¬¡æ¸…ç†ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½è¢«ç»ˆæ­¢
        for attempt in range(3):
            print(f"ç¬¬ {attempt + 1} æ¬¡æ¸…ç†Pythonè¿›ç¨‹...")
            current_killed = 0
            
            for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'ppid']):
                try:
                    if proc.info['name'] == 'python' and proc.info['pid'] != current_pid:
                        cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                        
                        # æ›´ç²¾ç¡®çš„è¿›ç¨‹è¯†åˆ«ï¼šåªæ€æ­»æˆ‘ä»¬åº”ç”¨ç›¸å…³çš„è¿›ç¨‹
                        should_kill = False
                        
                        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬åº”ç”¨çš„å·¥ä½œè¿›ç¨‹
                        if any(keyword in cmdline for keyword in ['storage_compare', 'keyword_search', 'single_compare', 'doc_similarity']):
                            should_kill = True
                        
                        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯spawn_mainè¿›ç¨‹ï¼ˆä½†è·³è¿‡resource_trackerï¼‰
                        elif 'spawn_main' in cmdline and 'resource_tracker' not in cmdline:
                            should_kill = True
                        
                        # 3. è·³è¿‡é‡è¦çš„ç³»ç»Ÿè¿›ç¨‹
                        elif any(system_keyword in cmdline for system_keyword in ['resource_tracker', 'forkserver', 'semlock_tracker']):
                            print(f"è·³è¿‡ç³»ç»Ÿè¿›ç¨‹ {proc.info['pid']}: {cmdline[:100]}...")
                            should_kill = False
                        
                        # 4. æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬åº”ç”¨çš„ç›´æ¥å­è¿›ç¨‹
                        elif proc.info['ppid'] == current_pid and 'multiprocessing' in cmdline:
                            should_kill = True
                        
                        if should_kill:
                            print(f"å¼ºåˆ¶æ€æ­»Pythonè¿›ç¨‹ {proc.info['pid']}ï¼Œå‘½ä»¤è¡Œ: {cmdline[:100]}...")
                            try:
                                proc.kill()  # ç›´æ¥ä½¿ç”¨killï¼Œä¸ç­‰å¾…
                                current_killed += 1
                            except psutil.NoSuchProcess:
                                pass  # è¿›ç¨‹å·²ç»ä¸å­˜åœ¨
                                
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            killed_count += current_killed
            print(f"ç¬¬ {attempt + 1} æ¬¡æ¸…ç†æ€æ­»äº† {current_killed} ä¸ªè¿›ç¨‹")
            
            if current_killed == 0:
                break  # æ²¡æœ‰è¿›ç¨‹éœ€è¦æ¸…ç†äº†
            
            time.sleep(1)  # ç­‰å¾…1ç§’å†è¿›è¡Œä¸‹ä¸€æ¬¡æ¸…ç†
        
        print(f"æ€»å…±æ€æ­»äº† {killed_count} ä¸ªPythonè¿›ç¨‹")
        return killed_count
    except Exception as e:
        print(f"å¼ºåˆ¶æ€æ­»Pythonè¿›ç¨‹æ—¶å‡ºé”™: {e}")
        return 0

# å¤šæ ¸å¤„ç†é…ç½®
CPU_CORES = multiprocessing.cpu_count()

# åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
if is_main_initialization_process():
    print(f"æ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•°: {CPU_CORES}")
MAX_WORKERS = min(CPU_CORES, 8)  # é™åˆ¶æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
if is_main_initialization_process():
    print(f"ä½¿ç”¨å·¥ä½œçº¿ç¨‹æ•°: {MAX_WORKERS}")

# åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
if is_main_initialization_process():
    print("\n" + "â³" + "="*58 + "â³")
    print("ğŸ”§ ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–...")
    print("ğŸ“‹ æ­£åœ¨æ£€æµ‹ç¡¬ä»¶é…ç½®å’ŒåŠ è½½AIæ¨¡å‹")
    print("â±ï¸  è¿™å¯èƒ½éœ€è¦10-30ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…")
    print("ğŸ’¡ é¦–æ¬¡è¿è¡Œæˆ–ç½‘ç»œè¾ƒæ…¢æ—¶å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´")
    print("â³" + "="*58 + "â³")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(os.path.join(UPLOAD_FOLDER, 'temp'), exist_ok=True)
os.makedirs(STORAGE_FOLDER, exist_ok=True)

# åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒGPUä¸æœ¬åœ°ç¦»çº¿
def get_device():
    """è·å–æœ€ä½³å¯ç”¨è®¾å¤‡"""
    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„ä¸»è¿›ç¨‹æ£€æµ‹
    should_log = is_main_initialization_process()
    
    if should_log:
        print("ğŸ” å¼€å§‹ç¡¬ä»¶è®¾å¤‡æ£€æµ‹...")
        print("  â”œâ”€ æ£€æµ‹CUDAæ”¯æŒ...")
    
    if torch.cuda.is_available():
        if should_log:
            print("  â”œâ”€ âœ… CUDAå¯ç”¨")
            cuda_version = torch.version.cuda
            print(f"  â”œâ”€ ğŸ“‹ CUDAç‰ˆæœ¬: {cuda_version}")
            print("  â”œâ”€ ğŸ”§ é…ç½®GPUè®¾å¤‡...")
        
        torch.cuda.set_device(0)
        device = 'cuda:0'
        
        if should_log:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"  â”œâ”€ ğŸ® GPUè®¾å¤‡: {gpu_name}")
            print(f"  â”œâ”€ ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB")
            print("  â””â”€ âœ… GPUè®¾å¤‡é…ç½®å®Œæˆ")
    else:
        if should_log:
            print("  â”œâ”€ âŒ CUDAä¸å¯ç”¨")
            print("  â”œâ”€ ğŸ”„ åˆ‡æ¢åˆ°CPUæ¨¡å¼")
        device = 'cpu'
        if should_log:
            print("  â””â”€ âœ… CPUè®¾å¤‡é…ç½®å®Œæˆ")
    
    return device

print("ğŸ”§ [DEBUG] å¼€å§‹è·å–è®¾å¤‡ä¿¡æ¯...")
device = get_device()
print(f"ğŸ”§ [DEBUG] è®¾å¤‡è·å–å®Œæˆ: {device}")

# é†’ç›®æ˜¾ç¤ºç¡¬ä»¶æ£€æµ‹ä¿¡æ¯ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰
print("ğŸ”§ [DEBUG] å‡†å¤‡æ˜¾ç¤ºç¡¬ä»¶æ£€æµ‹ä¿¡æ¯...")
if is_main_initialization_process():
    print("\n" + "="*60)
    if device.startswith('cuda'):
        print("ğŸ” GPUç¡¬ä»¶æ£€æµ‹")
        print(f"ğŸ–¥ï¸  GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print("âœ… GPUå¯ç”¨ï¼Œä¼˜å…ˆä¸ºæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        print("ğŸ” CPUç¡¬ä»¶æ£€æµ‹")
        print(f"âš™ï¸  CPUæ ¸å¿ƒæ•°: {CPU_CORES}")
        print(f"ğŸ”§ å·¥ä½œçº¿ç¨‹æ•°: {MAX_WORKERS}")
        print("ğŸ’¡ å°†ä½¿ç”¨CPUå¤šçº¿ç¨‹è®¡ç®—")
    print(f"ğŸ¯ é»˜è®¤è®¾å¤‡: {device}")
    print("="*60)

# å‡†å¤‡åå¤‡ï¼šå½“æ— æ³•åœ¨çº¿ä¸‹è½½æ¨¡å‹æ—¶ï¼Œä½¿ç”¨TF-IDF
_model = None
_use_tfidf_fallback = False
_model_source = None  # è®°å½•æ¨¡å‹æ¥æºï¼š'online_download', 'local_cache', 'local_path', 'tfidf_fallback', 'fast_start'
_startup_mode = None  # è®°å½•å¯åŠ¨æ¨¡å¼ï¼š'normal', 'fast_start', 'force_tfidf'
_model_load_time = 0  # è®°å½•æ¨¡å‹åŠ è½½æ—¶é—´

# å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šæœ¬åœ°æ¨¡å‹ç›®å½•æˆ–é•œåƒåç§°
MODEL_NAME_OR_PATH = os.environ.get('ST_MODEL_PATH', 'all-mpnet-base-v2')
LOCAL_FILES_ONLY = os.environ.get('HF_LOCAL_ONLY', '0') == '1'
HF_MIRROR = os.environ.get('HF_ENDPOINT')  # å¦‚ https://hf-mirror.com

# --- æ™ºèƒ½å¯åŠ¨é€»è¾‘ï¼ˆåœ¨æ¨¡å—å¯¼å…¥æ—¶æ‰§è¡Œï¼‰---

print("ğŸ”§ [DEBUG] æ£€æŸ¥æ˜¯å¦ä¸ºä¸»åˆå§‹åŒ–è¿›ç¨‹...")
# åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œå¯åŠ¨é€»è¾‘ï¼Œé¿å…åœ¨å­è¿›ç¨‹ä¸­é‡å¤åˆå§‹åŒ–
if is_main_initialization_process():
    print("ğŸ”§ [DEBUG] ç¡®è®¤ä¸ºä¸»è¿›ç¨‹ï¼Œå¼€å§‹æ‰§è¡Œå¯åŠ¨é€»è¾‘")
    print("\nğŸš€ å¼€å§‹æ™ºèƒ½å¯åŠ¨æµç¨‹...")
    
    try:
        print("ğŸ”§ [DEBUG] å‡†å¤‡å¯¼å…¥å¯åŠ¨ç®¡ç†å™¨...")
        # å¯¼å…¥å¯åŠ¨ç®¡ç†å™¨
        from startup_manager import run_intelligent_startup
        print("ğŸ”§ [DEBUG] å¯åŠ¨ç®¡ç†å™¨å¯¼å…¥æˆåŠŸ")
        
        print("ğŸ”§ [DEBUG] å¼€å§‹è¿è¡Œæ™ºèƒ½å¯åŠ¨æµç¨‹...")
        # è¿è¡Œæ™ºèƒ½å¯åŠ¨æµç¨‹
        startup_result = run_intelligent_startup()
        print("ğŸ”§ [DEBUG] æ™ºèƒ½å¯åŠ¨æµç¨‹å®Œæˆ")
        
        # 3. æ›´æ–°å…¨å±€å˜é‡
        if startup_result['success']:
            execution_result = startup_result['execution_result']
            _model = execution_result['model']
            actual_config = execution_result['actual_config']
            
            # è®¾ç½®æ¨¡å‹æ¥æºå’Œå¯åŠ¨æ¨¡å¼
            strategy_to_source = {
                'local_model': 'local_cache',
                'download_model': 'online_download', 
                'tfidf': 'tfidf_fallback',
                'minimal': 'basic_fallback'
            }
            
            _model_source = strategy_to_source.get(actual_config['strategy'], 'unknown')
            _startup_mode = 'normal' if _model is not None else 'fast_start'
            _model_load_time = actual_config.get('load_time', 0)
            
            # é…ç½®TF-IDF
            if actual_config['strategy'] in ['tfidf', 'minimal']:
                _use_tfidf_fallback = True
                try:
                    from sklearn.feature_extraction.text import TfidfVectorizer
                    from sklearn.metrics.pairwise import cosine_similarity as _sk_cos_sim
                    _tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))
                    print("âœ… TF-IDFå¤‡ç”¨æ–¹æ¡ˆå·²åˆå§‹åŒ–")
                except Exception as e:
                    print(f"âš ï¸ TF-IDFåˆå§‹åŒ–è­¦å‘Š: {e}")
                    _tfidf = None
            else:
                _use_tfidf_fallback = False
                _tfidf = None
            
            print(f"\nâœ… å¯åŠ¨å®Œæˆ!")
            print(f"ğŸ“‹ æœ€ç»ˆé…ç½®: {actual_config}")
            
        else:
            raise Exception(f"å¯åŠ¨ç­–ç•¥æ‰§è¡Œå¤±è´¥: {startup_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        print(f"âŒ æ™ºèƒ½å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ”„ æ‰§è¡Œç´§æ€¥åå¤‡å¯åŠ¨...")
        
        # ç´§æ€¥åå¤‡ï¼šæœ€ç®€å•çš„TF-IDFæ¨¡å¼
        _model = None
        _model_source = 'emergency_fallback'
        _startup_mode = 'emergency'
        _model_load_time = 0
        _use_tfidf_fallback = True
        
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity as _sk_cos_sim
            _tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 1))
            print("âœ… ç´§æ€¥TF-IDFæ¨¡å¼å¯åŠ¨æˆåŠŸ")
        except Exception:
            print("âŒ ç´§æ€¥å¯åŠ¨ä¹Ÿå¤±è´¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
            _tfidf = None
else:
    # åœ¨å­è¿›ç¨‹ä¸­ï¼Œè®¾ç½®é»˜è®¤å€¼
    _model = None
    _use_tfidf_fallback = True
    _model_source = 'subprocess_default'
    _startup_mode = 'subprocess'
    _model_load_time = 0
    _tfidf = None

def find_cached_model(model_name):
    """æŸ¥æ‰¾ç¼“å­˜ä¸­çš„æ¨¡å‹ç›®å½•"""
    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„ä¸»è¿›ç¨‹æ£€æµ‹
    should_log = is_main_initialization_process()
    
    def log_if_main(message):
        """åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°æ—¥å¿—"""
        if should_log:
            print(message)
    
    log_if_main("ğŸ” æœç´¢æœ¬åœ°ç¼“å­˜æ¨¡å‹...")
    
    possible_cache_dirs = [
        os.path.expanduser('~/.cache/huggingface/hub'),
        os.path.expanduser('~/.cache/huggingface/transformers'),
        '/data/doc_similarity_env/cache/huggingface/hub',
        '/root/.cache/huggingface/hub',
        os.path.join(os.getcwd(), '.cache', 'huggingface', 'hub')  # å½“å‰ç›®å½•ä¸‹çš„ç¼“å­˜
    ]
    
    for i, cache_dir in enumerate(possible_cache_dirs, 1):
        log_if_main(f"  â”œâ”€ æ£€æŸ¥ç›®å½• {i}/{len(possible_cache_dirs)}: {cache_dir}")
        
        if os.path.exists(cache_dir):
            log_if_main("  â”‚  â”œâ”€ âœ… ç›®å½•å­˜åœ¨")
            try:
                items = os.listdir(cache_dir)
                log_if_main(f"  â”‚  â”œâ”€ ğŸ“ å‘ç° {len(items)} ä¸ªé¡¹ç›®")
                
                # æŸ¥æ‰¾åŒ…å«æ¨¡å‹åç§°çš„ç›®å½•
                found_models = []
                for item in items:
                    if model_name.replace('/', '--') in item or model_name in item:
                        found_models.append(item)
                
                if found_models:
                    print(f"  â”‚  â”œâ”€ ğŸ¯ æ‰¾åˆ° {len(found_models)} ä¸ªç›¸å…³æ¨¡å‹:")
                    for model in found_models:
                        print(f"  â”‚  â”‚  â””â”€ {model}")
                    
                    for model in found_models:
                        model_path = os.path.join(cache_dir, model)
                        print(f"  â”‚  â”œâ”€ ğŸ” éªŒè¯æ¨¡å‹: {model}")
                        
                        if os.path.isdir(model_path):
                            print("  â”‚  â”‚  â”œâ”€ âœ… æ˜¯ç›®å½•")
                            # æŸ¥æ‰¾åŒ…å«config.jsonæˆ–pytorch_model.binçš„å­ç›®å½•
                            for root, dirs, files in os.walk(model_path):
                                if any(f in files for f in ['config.json', 'pytorch_model.bin', 'model.safetensors']):
                                    print(f"  â”‚  â”‚  â”œâ”€ âœ… æ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹æ–‡ä»¶")
                                    print(f"  â”‚  â”‚  â””â”€ ğŸ“ è·¯å¾„: {root}")
                                    print("  â””â”€ ğŸ‰ æ¨¡å‹éªŒè¯æˆåŠŸ!")
                                    return root
                            print("  â”‚  â”‚  â””â”€ âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹æ–‡ä»¶")
                        else:
                            print("  â”‚  â”‚  â””â”€ âŒ ä¸æ˜¯ç›®å½•")
                else:
                    print("  â”‚  â””â”€ âŒ æœªæ‰¾åˆ°ç›¸å…³æ¨¡å‹")
                    
            except PermissionError as e:
                print(f"  â”‚  â””â”€ âŒ æƒé™ä¸è¶³: {e}")
                continue
            except Exception as e:
                print(f"  â”‚  â””â”€ âŒ æ£€æŸ¥å¤±è´¥: {e}")
                continue
        else:
            print("  â”‚  â””â”€ âŒ ç›®å½•ä¸å­˜åœ¨")
    
    print("  â””â”€ ğŸ’” æœªæ‰¾åˆ°ä»»ä½•ç¼“å­˜æ¨¡å‹")
    return None

def create_simple_embedding_model():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„åŸºäºTF-IDFçš„åµŒå…¥æ¨¡å‹ä½œä¸ºåå¤‡"""
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        class SimpleTfidfModel:
            def __init__(self):
                self.vectorizer = TfidfVectorizer(
                    max_features=10000,
                    ngram_range=(1, 2),
                    stop_words='english' if 'english' in str(self) else None
                )
                self.device = 'cpu'
                # åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°TF-IDFæ¨¡å‹åˆ›å»ºä¿¡æ¯
                if is_main_initialization_process():
                    print("åˆ›å»ºäº†ç®€å•çš„TF-IDFåµŒå…¥æ¨¡å‹")
            
            def encode(self, sentences, convert_to_tensor=False, **kwargs):
                if isinstance(sentences, str):
                    sentences = [sentences]
                
                # æ‹Ÿåˆå¹¶è½¬æ¢æ–‡æœ¬
                vectors = self.vectorizer.fit_transform(sentences)
                
                if convert_to_tensor:
                    # è½¬æ¢ä¸ºtorch tensor
                    import torch
                    return torch.tensor(vectors.toarray(), dtype=torch.float32)
                else:
                    return vectors.toarray()
        
        return SimpleTfidfModel()
    except ImportError:
        print("æ— æ³•åˆ›å»ºTF-IDFåå¤‡æ¨¡å‹ï¼Œç¼ºå°‘sklearn")
        return None
    except Exception as e:
        print(f"åˆ›å»ºTF-IDFåå¤‡æ¨¡å‹å¤±è´¥: {e}")
        return None

def load_sentence_transformer_with_timeout(model_path, load_kwargs, timeout=30):
    """å¸¦è¶…æ—¶çš„æ¨¡å‹åŠ è½½"""
    import signal
    import threading
    
    result = {'model': None, 'error': None}
    
    def load_model():
        try:
            from sentence_transformers import SentenceTransformer
            # åœ¨å­è¿›ç¨‹ä¸­æŠ‘åˆ¶æ¨¡å‹åŠ è½½æ—¥å¿—
            if not is_main_initialization_process():
                import warnings
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    result['model'] = SentenceTransformer(model_path, **load_kwargs)
            else:
                result['model'] = SentenceTransformer(model_path, **load_kwargs)
        except Exception as e:
            result['error'] = e
    
    # åˆ›å»ºåŠ è½½çº¿ç¨‹
    thread = threading.Thread(target=load_model)
    thread.daemon = True
    thread.start()
    thread.join(timeout=timeout)
    
    if thread.is_alive():
        print(f"æ¨¡å‹åŠ è½½è¶…æ—¶ ({timeout}ç§’)ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æ¨¡å‹æ–‡ä»¶æŸå")
        return None, "åŠ è½½è¶…æ—¶"
    
    return result['model'], result['error']

def check_internet_connection(url="https://huggingface.co", timeout=2):
    """æ£€æŸ¥æ˜¯å¦èƒ½è®¿é—®Hugging Face"""
    should_log = is_main_initialization_process()
    
    if should_log:
        print("ğŸŒ æ£€æµ‹ç½‘ç»œè¿æ¥...")
        print(f"  â”œâ”€ ç›®æ ‡åœ°å€: {url}")
        print(f"  â”œâ”€ è¶…æ—¶è®¾ç½®: {timeout}ç§’")
        print("  â”œâ”€ ğŸ”— å°è¯•è¿æ¥...")
    
    try:
        import urllib.request
        urllib.request.urlopen(url, timeout=timeout)
        if should_log:
            print("  â””â”€ âœ… ç½‘ç»œè¿æ¥æˆåŠŸ")
        return True
    except Exception as e:
        if should_log:
            print(f"  â””â”€ âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {type(e).__name__}")
        return False

def print_download_instructions():
    """æ‰“å°ä¸‹è½½è¯´æ˜"""
    print("\n" + "="*80)
    print("ğŸ“‹ æ¨¡å‹ä¸‹è½½è¯´æ˜")
    print("="*80)
    print("ç”±äºç½‘ç»œæ— æ³•è®¿é—®Hugging Faceï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼š")
    print()
    print("ğŸ”— ä¸‹è½½åœ°å€:")
    print("  æ–¹æ³•1 - Hugging Faceå®˜æ–¹:")
    print("    https://huggingface.co/sentence-transformers/all-mpnet-base-v2")
    print("  æ–¹æ³•2 - é•œåƒç«™ç‚¹:")
    print("    https://hf-mirror.com/sentence-transformers/all-mpnet-base-v2")
    print("  æ–¹æ³•3 - ModelScope:")
    print("    https://modelscope.cn/models/sentence-transformers/all-mpnet-base-v2")
    print()
    print("ğŸ“ å­˜æ”¾è·¯å¾„:")
    cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
    model_dir = os.path.join(cache_dir, 'models--sentence-transformers--all-mpnet-base-v2', 'snapshots', 'latest')
    print(f"    {model_dir}")
    print()
    print("ğŸ“¦ éœ€è¦çš„æ–‡ä»¶:")
    print("  - config.json")
    print("  - pytorch_model.bin æˆ– model.safetensors")
    print("  - tokenizer.json")
    print("  - tokenizer_config.json")
    print("  - vocab.txt")
    print("  - sentence_bert_config.json")
    print("  - config_sentence_transformers.json")
    print()
    print("ğŸ’¡ ä¸‹è½½å®Œæˆåï¼Œé‡æ–°å¯åŠ¨åº”ç”¨å³å¯è‡ªåŠ¨åŠ è½½æ¨¡å‹")
    print("="*80)

def auto_download_model(model_name="sentence-transformers/all-mpnet-base-v2"):
    """è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"""
    should_log = is_main_initialization_process()
    
    if should_log:
        print(f"ğŸ“¥ å¼€å§‹è‡ªåŠ¨ä¸‹è½½æ¨¡å‹...")
        print(f"  â”œâ”€ æ¨¡å‹åç§°: {model_name}")
        print("  â”œâ”€ ğŸ”§ é…ç½®ä¸‹è½½ç¯å¢ƒ...")
    
    try:
        from sentence_transformers import SentenceTransformer
        # ä¸´æ—¶å…³é—­ç¦»çº¿æ¨¡å¼ä»¥å…è®¸ä¸‹è½½
        os.environ.pop('TRANSFORMERS_OFFLINE', None)
        os.environ.pop('HF_HUB_OFFLINE', None)
        if should_log:
            print("  â”œâ”€ âœ… ç¯å¢ƒé…ç½®å®Œæˆ")
        
        if should_log:
            print("  â”œâ”€ â³ å¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
            print("  â”‚  â”œâ”€ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")
            print("  â”‚  â””â”€ è¯·ä¿æŒç½‘ç»œè¿æ¥ç¨³å®š")
        
        model = SentenceTransformer(model_name, device=device)
        
        if should_log:
            print("  â”œâ”€ âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
            print("  â””â”€ ğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model
    except Exception as e:
        print(f"  â””â”€ âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
        return None

def load_model_with_timeout(model_name, timeout=45):
    """
    åœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹ï¼Œå¹¶è®¾ç½®è¶…æ—¶ä¿æŠ¤ã€‚
    è¿™æ˜¯è§£å†³æ¨¡å‹åŠ è½½å¡æ­»çš„å…³é”®ã€‚
    """
    import time
    from multiprocessing import Process, Queue

    def worker(q, model_name_to_load, device):
        """åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œçš„åŠ è½½å‡½æ•°"""
        try:
            # åœ¨å­è¿›ç¨‹ä¸­ï¼Œæ—¥å¿—æ˜¯ä¸å¯è§çš„ï¼Œä½†åŠ è½½ä»åœ¨è¿›è¡Œ
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_name_to_load, device=device)
            q.put(model)
        except Exception as e:
            # å°†å¼‚å¸¸ä¿¡æ¯æ”¾å…¥é˜Ÿåˆ—ï¼Œä»¥ä¾¿ä¸»è¿›ç¨‹å¯ä»¥æ•è·
            q.put(e)

    q = Queue()
    p = Process(target=worker, args=(q, model_name, device))
    p.daemon = True
    
    print(f"ğŸ”„ å¼€å§‹åŠ è½½SentenceTransformeræ¨¡å‹: '{model_name}' (è¶…æ—¶: {timeout}ç§’)...")
    start_time = time.time()
    
    p.start()
    p.join(timeout)
    
    elapsed_time = time.time() - start_time

    if p.is_alive():
        print(f"âŒ æ¨¡å‹åŠ è½½è¶…æ—¶ï¼(è¶…è¿‡ {timeout} ç§’)")
        p.terminate()  # å¼ºåˆ¶ç»ˆæ­¢å­è¿›ç¨‹
        p.join()
        return None, "timeout", elapsed_time

    if not q.empty():
        result = q.get()
        if isinstance(result, Exception):
            print(f"âŒ æ¨¡å‹åŠ è½½æ—¶å‘ç”Ÿé”™è¯¯: {result}")
            return None, str(result), elapsed_time
        else:
            # æˆåŠŸåŠ è½½
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨GPUä¸Š (å¦‚æœé€‚ç”¨)
            model_device = "CPU"
            if torch.cuda.is_available():
                try:
                    # å°è¯•å°†ä¸€ä¸ªæµ‹è¯•å¼ é‡æ”¾åˆ°æ¨¡å‹è®¾å¤‡ä¸Šï¼ŒéªŒè¯å¯ç”¨æ€§
                    test_tensor = torch.tensor([1]).to(result.device)
                    model_device = f"GPU ({result.device})"
                except Exception:
                    model_device = "CPU (GPUéªŒè¯å¤±è´¥)"
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! (è€—æ—¶: {elapsed_time:.2f}ç§’, è¿è¡Œäº: {model_device})")
            return result, "success", elapsed_time
    
    return None, "unknown_error", elapsed_time

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•å"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å™ªå£°ã€é¡µçœ‰é¡µè„šï¼Œä¿ç•™æ ¼å¼"""
    if not text:
        return text
    text = re.sub(r'Page \d+ of \d+|Confidential|Â© \d{4}.*?\|', '', text)
    return text.strip()

def extract_formatted_text(file_path, is_storage_file=False, task_id=None):
    """æå–æ–‡ä»¶æ–‡æœ¬å¹¶ä¿ç•™æ ¼å¼ï¼Œè¿”å›è¡Œåˆ—è¡¨ã€HTMLæ ¼å¼å†…å®¹å’Œä¸´æ—¶æ–‡ä»¶è·¯å¾„"""
    try:
        ext = file_path.rsplit('.', 1)[1].lower()
        lines = []
        html_content = []
        temp_file_path = file_path  # é»˜è®¤ä¸ç§»åŠ¨æ–‡ä»¶

        # å¦‚æœæ˜¯ä¸Šä¼ æ–‡ä»¶ï¼Œå¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•ï¼›å¦‚æœæ˜¯å­˜å‚¨åº“æ–‡ä»¶ï¼Œç›´æ¥è¯»å–
        if not is_storage_file:
            temp_file_path = os.path.join(UPLOAD_FOLDER, 'temp', f"{uuid.uuid4()}.{ext}")
            os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
            shutil.copy(file_path, temp_file_path)  # å¤åˆ¶è€Œéç§»åŠ¨

        if ext in {'yaml', 'yml'}:
            with open(temp_file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                text = yaml.dump(data, allow_unicode=True, sort_keys=False, default_flow_style=False)
                lines = text.splitlines()
                html_content = [f"<pre>{line}</pre>" for line in lines]

        elif ext in {'xlsx', 'xls'}:
            try:
                print(f"å¼€å§‹å¤„ç†Excelæ–‡ä»¶: {temp_file_path}")
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                if is_task_cancelled(task_id):
                    print(f"ä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆï¼Œåœæ­¢å¤„ç†Excelæ–‡ä»¶")
                    return None, None, None
                
                if ext == 'xlsx':
                    # å¯¹äºxlsxæ–‡ä»¶ï¼Œä½¿ç”¨openpyxlå¼•æ“
                    print("ä½¿ç”¨openpyxlå¼•æ“å¤„ç†xlsxæ–‡ä»¶")
                    df = pd.read_excel(temp_file_path, engine='openpyxl')
                else:
                    # å¯¹äºxlsæ–‡ä»¶ï¼Œå°è¯•å¤šç§æ–¹æ³•
                    df = None
                    
                    # æ–¹æ³•1ï¼šå°è¯•xlrdå¼•æ“
                    try:
                        print("å°è¯•ä½¿ç”¨xlrdå¼•æ“å¤„ç†xlsæ–‡ä»¶")
                        df = pd.read_excel(temp_file_path, engine='xlrd')
                        print("xlrdå¼•æ“æˆåŠŸ")
                    except Exception as e:
                        print(f"xlrdå¼•æ“å¤±è´¥: {e}")
                        
                        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                        if is_task_cancelled(task_id):
                            print(f"ä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆï¼Œåœæ­¢å¤„ç†Excelæ–‡ä»¶")
                            return None, None, None
                        
                        # æ–¹æ³•2ï¼šå°è¯•openpyxlå¼•æ“
                        try:
                            print("å°è¯•ä½¿ç”¨openpyxlå¼•æ“å¤„ç†xlsæ–‡ä»¶")
                            df = pd.read_excel(temp_file_path, engine='openpyxl')
                            print("openpyxlå¼•æ“æˆåŠŸ")
                        except Exception as e:
                            print(f"openpyxlå¼•æ“å¤±è´¥: {e}")
                            
                            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                            if is_task_cancelled(task_id):
                                print(f"ä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆï¼Œåœæ­¢å¤„ç†Excelæ–‡ä»¶")
                                return None, None, None
                            
                            # æ–¹æ³•3ï¼šå°è¯•ä¸æŒ‡å®šå¼•æ“
                            try:
                                print("å°è¯•ä½¿ç”¨é»˜è®¤å¼•æ“å¤„ç†xlsæ–‡ä»¶")
                                df = pd.read_excel(temp_file_path)
                                print("é»˜è®¤å¼•æ“æˆåŠŸ")
                            except Exception as e:
                                print(f"é»˜è®¤å¼•æ“å¤±è´¥: {e}")
                                
                                # æ–¹æ³•4ï¼šå°è¯•ä½¿ç”¨xlwingsï¼ˆä»…Windowsç¯å¢ƒï¼‰
                                if os.name == 'nt':  # åªåœ¨Windowsç¯å¢ƒä¸‹å°è¯•xlwings
                                    try:
                                        import xlwings as xw
                                        print("å°è¯•ä½¿ç”¨xlwingså¤„ç†xlsæ–‡ä»¶")
                                        app = xw.App(visible=False)
                                        wb = app.books.open(temp_file_path)
                                        ws = wb.sheets[0]
                                        data = ws.used_range.value
                                        wb.close()
                                        app.quit()
                                        
                                        if data:
                                            # å°†xlwingsæ•°æ®è½¬æ¢ä¸ºDataFrame
                                            df = pd.DataFrame(data[1:], columns=data[0])
                                            print("xlwingsæˆåŠŸ")
                                    except ImportError:
                                        print("xlwingsä¸å¯ç”¨")
                                    except Exception as e:
                                        print(f"xlwingså¤±è´¥: {e}")
                                else:
                                    print("Linuxç¯å¢ƒè·³è¿‡xlwings")
                                
                                # æ–¹æ³•5ï¼šå°è¯•ä½¿ç”¨pyexcelï¼ˆè·¨å¹³å°ï¼‰
                                if df is None:
                                    try:
                                        import pyexcel as pe
                                        print("å°è¯•ä½¿ç”¨pyexcelå¤„ç†xlsæ–‡ä»¶")
                                        sheet = pe.get_sheet(file_name=temp_file_path)
                                        data = sheet.to_array()
                                        if data:
                                            # å°†pyexcelæ•°æ®è½¬æ¢ä¸ºDataFrame
                                            df = pd.DataFrame(data[1:], columns=data[0])
                                            print("pyexcelæˆåŠŸ")
                                    except ImportError:
                                        print("pyexcelä¸å¯ç”¨")
                                    except Exception as e:
                                        print(f"pyexcelå¤±è´¥: {e}")
                                
                                # æ–¹æ³•6ï¼šå°è¯•ä½¿ç”¨xlutilsï¼ˆéœ€è¦xlrdé…åˆï¼‰
                                if df is None:
                                    try:
                                        import xlutils
                                        from xlutils.copy import copy
                                        from xlrd import open_workbook
                                        print("å°è¯•ä½¿ç”¨xlutilså¤„ç†xlsæ–‡ä»¶")
                                        rb = open_workbook(temp_file_path, formatting_info=True)
                                        # è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                                        sheet = rb.sheet_by_index(0)
                                        data = []
                                        for row_idx in range(sheet.nrows):
                                            row_data = []
                                            for col_idx in range(sheet.ncols):
                                                cell_value = sheet.cell_value(row_idx, col_idx)
                                                row_data.append(cell_value)
                                            data.append(row_data)
                                        
                                        if data:
                                            df = pd.DataFrame(data[1:], columns=data[0])
                                            print("xlutilsæˆåŠŸ")
                                    except ImportError:
                                        print("xlutilsä¸å¯ç”¨")
                                    except Exception as e:
                                        print(f"xlutilså¤±è´¥: {e}")
                                
                                # æ–¹æ³•7ï¼šå°è¯•ä½¿ç”¨LibreOfficeå‘½ä»¤è¡Œå·¥å…·ï¼ˆLinuxç¯å¢ƒï¼‰
                                if df is None and os.name != 'nt':
                                    try:
                                        import subprocess
                                        import tempfile
                                        print("å°è¯•ä½¿ç”¨LibreOfficeå‘½ä»¤è¡Œå·¥å…·å¤„ç†xlsæ–‡ä»¶")
                                        
                                        # åˆ›å»ºä¸´æ—¶ç›®å½•
                                        with tempfile.TemporaryDirectory() as temp_dir:
                                            # ä½¿ç”¨LibreOfficeè½¬æ¢ä¸ºCSV
                                            cmd = [
                                                'libreoffice', '--headless', '--convert-to', 'csv',
                                                '--outdir', temp_dir, temp_file_path
                                            ]
                                            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                                            
                                            if result.returncode == 0:
                                                # æŸ¥æ‰¾ç”Ÿæˆçš„CSVæ–‡ä»¶
                                                csv_files = [f for f in os.listdir(temp_dir) if f.endswith('.csv')]
                                                if csv_files:
                                                    csv_path = os.path.join(temp_dir, csv_files[0])
                                                    df = pd.read_csv(csv_path, encoding='utf-8')
                                                    print("LibreOfficeå‘½ä»¤è¡Œå·¥å…·æˆåŠŸ")
                                    except (ImportError, subprocess.TimeoutExpired, FileNotFoundError) as e:
                                        print(f"LibreOfficeå‘½ä»¤è¡Œå·¥å…·å¤±è´¥: {e}")
                                    except Exception as e:
                                        print(f"LibreOfficeå‘½ä»¤è¡Œå·¥å…·å¤±è´¥: {e}")
                
                if df is None or df.empty:
                    print("æ— æ³•è¯»å–Excelæ–‡ä»¶æ•°æ®")
                    return None, None, None
                
                print(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œ{len(df.columns)}åˆ—")
                
                # å†æ¬¡æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                if is_task_cancelled(task_id):
                    print(f"ä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆï¼Œåœæ­¢å¤„ç†Excelæ–‡ä»¶")
                    return None, None, None
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¯¹å¤§æ–‡ä»¶è¿›è¡Œä¼˜åŒ–å¤„ç†
                total_cells = len(df) * len(df.columns)
                is_large_file = total_cells > 100000  # è¶…è¿‡10ä¸‡ä¸ªå•å…ƒæ ¼è§†ä¸ºå¤§æ–‡ä»¶
                
                if is_large_file:
                    print(f"æ£€æµ‹åˆ°å¤§æ–‡ä»¶ï¼ˆ{total_cells}ä¸ªå•å…ƒæ ¼ï¼‰ï¼Œä½¿ç”¨ä¼˜åŒ–å¤„ç†ç­–ç•¥")
                    
                    # å¯¹äºå¤§æ–‡ä»¶ï¼Œåªå¤„ç†å‰1000è¡Œå’Œå‰100åˆ—ï¼Œé¿å…å†…å­˜å’Œæ€§èƒ½é—®é¢˜
                    max_rows = min(1000, len(df))
                    max_cols = min(100, len(df.columns))
                    
                    print(f"å¤§æ–‡ä»¶ä¼˜åŒ–ï¼šå¤„ç†å‰{max_rows}è¡Œå’Œå‰{max_cols}åˆ—")
                    df_optimized = df.iloc[:max_rows, :max_cols]
                    
                    # å¤„ç†æ•°æ®
                    if is_task_cancelled(task_id):
                        print(f"ä»»åŠ¡ {task_id} åœ¨æ•°æ®è½¬æ¢å‰å·²è¢«å–æ¶ˆ")
                        return None, None, None
                    
                    text = df_optimized.to_csv(index=False, sep='\t')
                    
                    if is_task_cancelled(task_id):
                        print(f"ä»»åŠ¡ {task_id} åœ¨æ–‡æœ¬åˆ†å‰²å‰å·²è¢«å–æ¶ˆ")
                        return None, None, None
                    
                    lines = text.splitlines()
                    
                    if is_task_cancelled(task_id):
                        print(f"ä»»åŠ¡ {task_id} åœ¨HTMLç”Ÿæˆå‰å·²è¢«å–æ¶ˆ")
                        return None, None, None
                    
                    html_table = df_optimized.to_html(index=False, border=1, classes='table')
                    html_content = [html_table]
                    
                    print(f"å¤§æ–‡ä»¶ä¼˜åŒ–å¤„ç†å®Œæˆï¼Œæå–äº†{len(lines)}è¡Œæ–‡æœ¬ï¼ˆåŸå§‹æ–‡ä»¶ï¼š{len(df)}è¡Œ{len(df.columns)}åˆ—ï¼‰")
                else:
                    # å¤„ç†æ•°æ®
                    if is_task_cancelled(task_id):
                        print(f"ä»»åŠ¡ {task_id} åœ¨æ•°æ®è½¬æ¢å‰å·²è¢«å–æ¶ˆ")
                        return None, None, None
                    
                    text = df.to_csv(index=False, sep='\t')
                    
                    if is_task_cancelled(task_id):
                        print(f"ä»»åŠ¡ {task_id} åœ¨æ–‡æœ¬åˆ†å‰²å‰å·²è¢«å–æ¶ˆ")
                        return None, None, None
                    
                    lines = text.splitlines()
                    
                    if is_task_cancelled(task_id):
                        print(f"ä»»åŠ¡ {task_id} åœ¨HTMLç”Ÿæˆå‰å·²è¢«å–æ¶ˆ")
                        return None, None, None
                    
                    html_table = df.to_html(index=False, border=1, classes='table')
                    html_content = [html_table]
                
                print(f"Excelæ–‡ä»¶å¤„ç†å®Œæˆï¼Œæå–äº†{len(lines)}è¡Œæ–‡æœ¬")
                
            except Exception as e:
                print(f"Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return None, None, None

        elif ext == 'docx':
            doc = Document(temp_file_path)
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    lines.append(text)
                    style = para.style.name
                    html_tag = 'p'
                    if style.startswith('Heading'):
                        html_tag = f'h{style[-1]}'
                    elif para.style.name == 'List Bullet' or para.text.strip().startswith(('-', '*')):
                        html_content.append(f'<li>{text}</li>')
                    else:
                        html_content.append(f'<{html_tag}>{text}</{html_tag}>')
            for table in doc.tables:
                html_table = '<table border="1" class="table">'
                for row in table.rows:
                    html_table += '<tr>'
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        lines.append(cell_text)
                        html_table += f'<td>{cell_text}</td>'
                    html_table += '</tr>'
                html_table += '</table>'
                html_content.append(html_table)

        elif ext == 'pdf':
            # å°è¯•å¤šç§PDFæå–æ–¹æ³•
            extracted = False
            
            # æ–¹æ³•1ï¼šä½¿ç”¨textractï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import textract
                text = textract.process(temp_file_path).decode('utf-8')
                cleaned_text = clean_text(text)
                lines = cleaned_text.splitlines()
                html_content = [f"<pre>{line}</pre>" for line in lines]
                extracted = True
                print("ä½¿ç”¨textractæˆåŠŸæå–PDFæ–‡æœ¬")
            except ImportError:
                print("textractä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
            except Exception as e:
                print(f"textractæå–PDFå¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šä½¿ç”¨pdfminer.six
            if not extracted:
                try:
                    from pdfminer.high_level import extract_text
                    text = extract_text(temp_file_path)
                    cleaned_text = clean_text(text)
                    lines = cleaned_text.splitlines()
                    html_content = [f"<pre>{line}</pre>" for line in lines]
                    extracted = True
                    print("ä½¿ç”¨pdfminer.sixæˆåŠŸæå–PDFæ–‡æœ¬")
                except Exception as e:
                    print(f"pdfminer.sixæå–PDFå¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨pdfplumber
            if not extracted:
                try:
                    import pdfplumber
                    text = ""
                    with pdfplumber.open(temp_file_path) as pdf:
                        for page in pdf.pages:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                    cleaned_text = clean_text(text)
                    lines = cleaned_text.splitlines()
                    html_content = [f"<pre>{line}</pre>" for line in lines]
                    extracted = True
                    print("ä½¿ç”¨pdfplumberæˆåŠŸæå–PDFæ–‡æœ¬")
                except ImportError:
                    print("pdfplumberä¸å¯ç”¨")
                except Exception as e:
                    print(f"pdfplumberæå–PDFå¤±è´¥: {e}")
            
            # æ–¹æ³•4ï¼šä½¿ç”¨PyPDF2
            if not extracted:
                try:
                    import PyPDF2
                    text = ""
                    with open(temp_file_path, 'rb') as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        for page in pdf_reader.pages:
                            text += page.extract_text() + "\n"
                    cleaned_text = clean_text(text)
                    lines = cleaned_text.splitlines()
                    html_content = [f"<pre>{line}</pre>" for line in lines]
                    extracted = True
                    print("ä½¿ç”¨PyPDF2æˆåŠŸæå–PDFæ–‡æœ¬")
                except ImportError:
                    print("PyPDF2ä¸å¯ç”¨")
                except Exception as e:
                    print(f"PyPDF2æå–PDFå¤±è´¥: {e}")
            
            if not extracted:
                print("æ‰€æœ‰PDFæå–æ–¹æ³•éƒ½å¤±è´¥")
                return None, None, None
        
        elif ext == 'doc':
            # å°è¯•å¤šç§DOCæ–‡ä»¶æå–æ–¹æ³•
            extracted = False
            
            # æ–¹æ³•1ï¼šå°è¯•ä½¿ç”¨python-docxå¤„ç†DOCæ–‡ä»¶ï¼ˆæœ‰æ—¶ä¹Ÿèƒ½å¤„ç†.docï¼‰
            try:
                doc = Document(temp_file_path)
                for para in doc.paragraphs:
                    text = para.text.strip()
                    if text:
                        lines.append(text)
                        html_content.append(f"<p>{text}</p>")
                for table in doc.tables:
                    html_table = '<table border="1" class="table">'
                    for row in table.rows:
                        html_table += '<tr>'
                        for cell in row.cells:
                            cell_text = cell.text.strip()
                            lines.append(cell_text)
                            html_table += f'<td>{cell_text}</td>'
                        html_table += '</tr>'
                    html_table += '</table>'
                    html_content.append(html_table)
                extracted = True
                print("ä½¿ç”¨python-docxæˆåŠŸå¤„ç†DOCæ–‡ä»¶")
            except Exception as e:
                print(f"python-docxå¤„ç†DOCå¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šä½¿ç”¨textractï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if not extracted:
                try:
                    import textract
                    text = textract.process(temp_file_path).decode('utf-8')
                    cleaned_text = clean_text(text)
                    lines = cleaned_text.splitlines()
                    html_content = [f"<pre>{line}</pre>" for line in lines]
                    extracted = True
                    print("ä½¿ç”¨textractæˆåŠŸå¤„ç†DOCæ–‡ä»¶")
                except ImportError:
                    print("textractä¸å¯ç”¨")
                except Exception as e:
                    print(f"textractå¤„ç†DOCå¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨docx2txtï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if not extracted:
                try:
                    import docx2txt
                    text = docx2txt.process(temp_file_path)
                    if text:
                        cleaned_text = clean_text(text)
                        lines = cleaned_text.splitlines()
                        html_content = [f"<pre>{line}</pre>" for line in lines]
                        extracted = True
                        print("ä½¿ç”¨docx2txtæˆåŠŸå¤„ç†DOCæ–‡ä»¶")
                except ImportError:
                    print("docx2txtä¸å¯ç”¨")
                except Exception as e:
                    print(f"docx2txtå¤„ç†DOCå¤±è´¥: {e}")

            # æ–¹æ³•4ï¼šä½¿ç”¨antiwordï¼ˆLinuxå¸¸ç”¨ï¼‰
            if not extracted:
                try:
                    import subprocess
                    result = subprocess.run(['antiword', temp_file_path], capture_output=True, text=True, check=True)
                    text = result.stdout
                    if text:
                        cleaned_text = clean_text(text)
                        lines = cleaned_text.splitlines()
                        html_content = [f"<pre>{line}</pre>" for line in lines]
                        extracted = True
                        print("ä½¿ç”¨antiwordæˆåŠŸå¤„ç†DOCæ–‡ä»¶")
                except Exception as e:
                    print(f"antiwordå¤„ç†DOCå¤±è´¥: {e}")

            # æ–¹æ³•5ï¼šä½¿ç”¨catdocå…œåº•
            if not extracted:
                try:
                    import subprocess
                    result = subprocess.run(['catdoc', '-w', temp_file_path], capture_output=True, text=True, check=True)
                    text = result.stdout
                    if text:
                        cleaned_text = clean_text(text)
                        lines = cleaned_text.splitlines()
                        html_content = [f"<pre>{line}</pre>" for line in lines]
                        extracted = True
                        print("ä½¿ç”¨catdocæˆåŠŸå¤„ç†DOCæ–‡ä»¶")
                except Exception as e:
                    print(f"catdocå¤„ç†DOCå¤±è´¥: {e}")
            
            if not extracted:
                print("æ‰€æœ‰DOCæå–æ–¹æ³•éƒ½å¤±è´¥")
                return None, None, None

        elif ext == 'txt':
            with open(temp_file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                cleaned_text = clean_text(text)
                lines = cleaned_text.splitlines()
                html_content = [f"<pre>{line}</pre>" for line in lines]

        else:
            return None, None, None

        return lines, html_content, temp_file_path

    except Exception as e:
        print(f"æå–æ–‡æœ¬å¤±è´¥: {e}")
        return None, None, None

def segment_text(text, max_sentences=MAX_SENTENCES_PER_SEGMENT):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ®µè½"""
    try:
        sentences = _sent_tokenize(text)
    except Exception:
        sentences = _simple_sent_tokenize(text)
    segments = []
    for i in range(0, len(sentences), max_sentences):
        segment = ' '.join(sentences[i:i + max_sentences])
        if segment.strip():
            segments.append(segment)
    return segments if segments else [text]

# ç›¸ä¼¼åº¦è®¡ç®—ï¼šä¼˜å…ˆSTæ¨¡å‹ï¼Œå¤±è´¥åˆ™TF-IDF
try:
    from sentence_transformers import util as _st_util
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦å‡½æ•°
    import torch
    import torch.nn.functional as F
    class SimpleUtil:
        @staticmethod
        def cos_sim(a, b):
            if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):
                # å¤„ç†ä¸åŒç»´åº¦çš„å¼ é‡
                if a.dim() == 1 and b.dim() == 1:
                    # ä¸¤ä¸ªä¸€ç»´å¼ é‡ï¼Œç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0), dim=-1)
                elif a.dim() == 1 and b.dim() == 2:
                    # aæ˜¯ä¸€ç»´ï¼Œbæ˜¯äºŒç»´ï¼Œæ‰©å±•açš„ç»´åº¦
                    return F.cosine_similarity(a.unsqueeze(0), b, dim=-1)
                elif a.dim() == 2 and b.dim() == 1:
                    # aæ˜¯äºŒç»´ï¼Œbæ˜¯ä¸€ç»´ï¼Œæ‰©å±•bçš„ç»´åº¦
                    return F.cosine_similarity(a, b.unsqueeze(0), dim=-1)
                elif a.dim() == 2 and b.dim() == 2:
                    # ä¸¤ä¸ªäºŒç»´å¼ é‡ï¼Œè®¡ç®—çŸ©é˜µç›¸ä¼¼åº¦
                    # ä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    a_norm = F.normalize(a, p=2, dim=1)
                    b_norm = F.normalize(b, p=2, dim=1)
                    return torch.mm(a_norm, b_norm.t())
                else:
                    # å…¶ä»–æƒ…å†µï¼Œå°è¯•é»˜è®¤å¤„ç†
                    return F.cosine_similarity(a, b, dim=-1)
            else:
                # å¦‚æœä¸æ˜¯tensorï¼Œå°è¯•è½¬æ¢
                if not isinstance(a, torch.Tensor):
                    a = torch.tensor(a, dtype=torch.float32)
                if not isinstance(b, torch.Tensor):
                    b = torch.tensor(b, dtype=torch.float32)
                return F.cosine_similarity(a, b, dim=-1)
    _st_util = SimpleUtil()

def _similarity_st(segments1, segments2):
    sims = []
    with torch.no_grad():
        for seg1 in segments1:
            for seg2 in segments2:
                e1 = _model.encode(seg1, convert_to_tensor=True)
                e2 = _model.encode(seg2, convert_to_tensor=True)
                sim = _st_util.cos_sim(e1, e2)[0][0].item()
                sims.append(sim)
    return sims

def _similarity_tfidf(segments1, segments2):
    if _tfidf is None:
        return [0.0]
    sims = []
    for seg1 in segments1:
        for seg2 in segments2:
            mats = _tfidf.fit_transform([seg1, seg2])
            sim = float(_sk_cos_sim(mats[0], mats[1])[0, 0])
            sims.append(sim)
    return sims


def compute_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆåˆ†æ®µå¤„ç†ï¼Œç»“åˆå·®å¼‚åˆ†æï¼‰ã€‚
    ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§æ‰¹é‡ç¼–ç å¹¶çŸ©é˜µç›¸ä¼¼åº¦ï¼Œå……åˆ†åˆ©ç”¨BLAS/OMPå¤šæ ¸æˆ–GPUã€‚"""
    if not text1 or not text2:
        return 0.0
    
    # é¢„å¤„ç†ï¼šè§„èŒƒåŒ–æ–‡æœ¬
    text1_norm = normalize_whitespace(text1)
    text2_norm = normalize_whitespace(text2)
    
    # å¦‚æœæ–‡æœ¬å®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›1.0
    if text1_norm == text2_norm:
        return 1.0
    
    # å¿«é€Ÿå­—ç¬¦çº§å·®å¼‚åˆ†æï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
    import difflib
    total_orig_chars = len(text1_norm)
    total_new_chars = len(text2_norm)
    total_chars = max(total_orig_chars, total_new_chars, 1)
    
    print(f"ğŸ”¢ ç›¸ä¼¼åº¦è®¡ç®—: åŸæ–‡{total_orig_chars}å­—ç¬¦, æ–°æ–‡{total_new_chars}å­—ç¬¦")
    
    # ä½¿ç”¨difflibè¿›è¡Œç®€å•çš„å­—ç¬¦çº§å·®å¼‚ç»Ÿè®¡
    matcher = difflib.SequenceMatcher(None, text1_norm, text2_norm)
    similarity_ratio = matcher.ratio()
    
    # åŸºäºå­—ç¬¦å·®å¼‚çš„ç›¸ä¼¼åº¦
    diff_based_similarity = similarity_ratio
    print(f"ğŸ”¢ å­—ç¬¦çº§ç›¸ä¼¼åº¦: {diff_based_similarity:.4f}")
    
    # è®¡ç®—è¡Œçº§ç›¸ä¼¼åº¦ï¼ˆå¯¹è¡¨æ ¼æ–‡æ¡£æ›´å‡†ç¡®ï¼‰
    lines1 = [line.strip() for line in text1_norm.split('\n') if line.strip()]
    lines2 = [line.strip() for line in text2_norm.split('\n') if line.strip()]
    
    line_matcher = difflib.SequenceMatcher(None, lines1, lines2)
    line_based_similarity = line_matcher.ratio()
    print(f"ğŸ”¢ è¡Œçº§ç›¸ä¼¼åº¦: {line_based_similarity:.4f}")
    
    # å¯¹äºæœ‰æ˜æ˜¾å·®å¼‚çš„æ–‡æ¡£ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„ç›¸ä¼¼åº¦
    if diff_based_similarity < 0.98 or line_based_similarity < 0.98:
        # å–æ›´ä½çš„ç›¸ä¼¼åº¦å€¼ï¼Œç¡®ä¿æ•æ„Ÿæ€§
        strict_similarity = min(diff_based_similarity, line_based_similarity)
        
        # é•¿åº¦å·®å¼‚æƒ©ç½š
        length_diff = abs(total_orig_chars - total_new_chars)
        if length_diff > 0:
            length_penalty = 1.0 - (length_diff / total_chars * 0.5)
            strict_similarity *= max(0.7, length_penalty)
            print(f"ğŸ”¢ é•¿åº¦å·®å¼‚æƒ©ç½š: {length_penalty:.4f}")
        
        print(f"ğŸ”¢ ä¸¥æ ¼ç›¸ä¼¼åº¦ï¼ˆæå‰è¿”å›ï¼‰: {strict_similarity:.4f}")
        return max(0.0, min(1.0, strict_similarity))
    
    # ä¼ ç»Ÿçš„æ®µè½ç›¸ä¼¼åº¦è®¡ç®—
    segments1 = segment_text(text1)
    segments2 = segment_text(text2)
    
    # æ˜¾ç¤ºè®¡ç®—æ¨¡å¼ä¿¡æ¯ï¼ˆä»…åœ¨è°ƒè¯•æ—¶ï¼‰
    if os.environ.get('DEBUG_SIMILARITY', '0') == '1':
        if _model is not None and hasattr(_model, 'device'):
            device_info = getattr(_model, 'device', 'unknown')
            if str(device_info).startswith('cuda'):
                print(f"ğŸš€ GPUåŠ é€Ÿè®¡ç®—ç›¸ä¼¼åº¦ - è®¾å¤‡: {device_info}")
            else:
                print(f"ğŸ–¥ï¸  CPUå¤šçº¿ç¨‹è®¡ç®—ç›¸ä¼¼åº¦ - æ®µè½æ•°: {len(segments1)}x{len(segments2)}")
        elif _use_tfidf_fallback:
            print(f"ğŸ“Š TF-IDFè®¡ç®—ç›¸ä¼¼åº¦ - æ®µè½æ•°: {len(segments1)}x{len(segments2)}")

    if _model is not None:
        # SentenceTransformer è·¯å¾„ï¼ˆGPUæˆ–CPUï¼‰
        try:
            with torch.no_grad():
                # æ¿€è¿›çš„å†…å­˜ç®¡ç†ç­–ç•¥
                if torch.cuda.is_available():
                    # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                    gpu_memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                    gpu_memory_free = gpu_memory_total - gpu_memory_used
                    
                    # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
                    if gpu_memory_free < 1.0:  # å°äº1GBå¯ç”¨å†…å­˜
                        max_batch_size = 4
                    elif gpu_memory_free < 2.0:  # å°äº2GBå¯ç”¨å†…å­˜
                        max_batch_size = 8
                    elif gpu_memory_free < 3.0:  # å°äº3GBå¯ç”¨å†…å­˜
                        max_batch_size = 16
                    else:
                        max_batch_size = 32
                else:
                    max_batch_size = 64
                
                batch_size = min(max_batch_size, max(len(segments1), len(segments2)))
                
                # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œè¿›ä¸€æ­¥å‡å°‘æ‰¹æ¬¡å¤§å°
                if len(segments1) > 30 or len(segments2) > 30:
                    batch_size = min(batch_size, 8)
                if len(segments1) > 100 or len(segments2) > 100:
                    batch_size = min(batch_size, 4)
                
                # æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰deviceå±æ€§ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è®¾å¤‡
                model_device = getattr(_model, 'device', device)
                emb1 = _model.encode(segments1, convert_to_tensor=True, show_progress_bar=False, 
                                   batch_size=batch_size)
                emb2 = _model.encode(segments2, convert_to_tensor=True, show_progress_bar=False, 
                                   batch_size=batch_size)
                
                # ç¡®ä¿å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if hasattr(emb1, 'to'):
                    emb1 = emb1.to(model_device)
                if hasattr(emb2, 'to'):
                    emb2 = emb2.to(model_device)
                # ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ (len1 x len2)
                sim_mat = _st_util.cos_sim(emb1, emb2)
                max_sim = float(sim_mat.max().item()) if sim_mat.numel() > 0 else 0.0
                
                # ç«‹å³æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        except Exception as e:
            print(f"STæ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€å¯¹: {e}")
            sims = []
            with torch.no_grad():
                for s1 in segments1:
                    try:
                        e1 = _model.encode(s1, convert_to_tensor=True, batch_size=1)
                        for s2 in segments2:
                            try:
                                e2 = _model.encode(s2, convert_to_tensor=True, batch_size=1)
                                # ç¡®ä¿ä¸¤ä¸ªå¼ é‡éƒ½æ˜¯2Dçš„
                                if e1.dim() == 1:
                                    e1 = e1.unsqueeze(0)
                                if e2.dim() == 1:
                                    e2 = e2.unsqueeze(0)
                                sim = float(_st_util.cos_sim(e1, e2)[0][0].item())
                                sims.append(sim)
                                # æ¸…ç†ä¸­é—´å˜é‡
                                del e2
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                            except Exception as e2:
                                print(f"é€å¯¹è®¡ç®—å¤±è´¥ (s2): {e2}")
                                continue
                        # æ¸…ç†ä¸­é—´å˜é‡
                        del e1
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e1:
                        print(f"é€å¯¹è®¡ç®—å¤±è´¥ (s1): {e1}")
                        continue
            max_sim = max(sims) if sims else 0.0
    else:
        # TF-IDF å›é€€è·¯å¾„ï¼ˆæ‰¹é‡æ„é€ å†ä¸¤ä¸¤è®¡ç®—ï¼‰
        if _tfidf is None:
            return 0.0
        try:
            docs = segments1 + segments2
            mats = _tfidf.fit_transform(docs)  # (n1+n2, vocab)
            n1 = len(segments1)
            A = mats[:n1]
            B = mats[n1:]
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ A * B^T
            from sklearn.metrics.pairwise import cosine_similarity as _sk_cos_sim
            sim_mat = _sk_cos_sim(A, B)
            max_sim = float(sim_mat.max()) if sim_mat.size > 0 else 0.0
        except Exception as e:
            print(f"TF-IDFæ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            # å›é€€åˆ°é€å¯¹
            sims = []
            for s1 in segments1:
                for s2 in segments2:
                    try:
                        mats = _tfidf.fit_transform([s1, s2])
                        sims.append(float((_sk_cos_sim(mats[0], mats[1]))[0, 0]))
                    except Exception:
                        sims.append(0.0)
            max_sim = max(sims) if sims else 0.0

    # ç»„åˆç›¸ä¼¼åº¦ï¼šå–å·®å¼‚åˆ†æç›¸ä¼¼åº¦å’Œä¼ ç»Ÿç›¸ä¼¼åº¦çš„åŠ æƒå¹³å‡
    # å¯¹äºæœ‰æ˜æ˜¾å·®å¼‚çš„æ–‡æ¡£ï¼Œå·®å¼‚åˆ†ææ›´å‡†ç¡®
    # å¯¹äºå†…å®¹ç›¸è¿‘çš„æ–‡æ¡£ï¼Œä¼ ç»Ÿæ–¹æ³•æ›´å‡†ç¡®
    if abs(diff_based_similarity - 1.0) > 0.01:
        # æœ‰å·®å¼‚æ—¶ï¼Œæ›´ä¾èµ–å·®å¼‚åˆ†æ
        final_similarity = 0.6 * diff_based_similarity + 0.4 * max_sim
    else:
        # æ— å·®å¼‚æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        final_similarity = max_sim
    
    # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´å†…
    final_similarity = max(0.0, min(1.0, final_similarity))
    
    if final_similarity > SIMILARITY_THRESHOLD:
        print(f"è­¦å‘Šï¼šç›¸ä¼¼åº¦ {final_similarity:.4f} è¶…è¿‡é˜ˆå€¼ {SIMILARITY_THRESHOLD}")
    
    return final_similarity

def get_common_words(text1, text2, top_n=20):
    """è·å–ä¸¤ä¸ªæ–‡æœ¬ä¸­ç›¸åŒä¸”é«˜é¢‘æ¬¡å‡ºç°çš„è¯æ±‡ï¼Œæ’é™¤çº¯æ•°å­—"""
    try:
        words1 = [word.lower() for word in _word_tokenize(text1) if word.lower().isalnum() and not word.isdigit()]
        words2 = [word.lower() for word in _word_tokenize(text2) if word.lower().isalnum() and not word.isdigit()]
    except Exception:
        words1 = _simple_word_tokenize(text1)
        words2 = _simple_word_tokenize(text2)
        words1 = [w for w in words1 if w.isalnum() and not w.isdigit()]
        words2 = [w for w in words2 if w.isalnum() and not w.isdigit()]
    common_words = set(words1) & set(words2)
    from collections import Counter as _Counter
    word_counts1 = _Counter(word for word in words1 if word in common_words)
    word_counts2 = _Counter(word for word in words2 if word in common_words)
    combined_counts = {word: min(word_counts1[word], word_counts2[word]) for word in common_words}
    return sorted(combined_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]

def get_similarity_reason(text1, text2, common_words, similarity):
    """åŠ¨æ€ç”Ÿæˆå®šåˆ¶åŒ–çš„ç›¸ä¼¼åŸå› """
    if not common_words:
        return "ç›¸ä¼¼åº¦ä½ï¼Œæ–‡ä»¶å†…å®¹å¯èƒ½æ— æ˜¾è‘—ç›¸å…³æ€§ã€‚"
    similarity_percent = similarity * 100
    reason = f"ç›¸ä¼¼åº¦ä¸º {similarity_percent:.2f}%ï¼Œ"
    if similarity_percent > 50:
        reason += "é«˜åº¦ç›¸å…³ï¼Œä¸»è¦ç”±ä»¥ä¸‹å…±äº«è¯æ±‡é©±åŠ¨ï¼š"
    elif similarity_percent < 30:
        reason += "ä½ç›¸å…³ï¼Œå¯èƒ½ç”±å°‘é‡å…±äº«è¯æ±‡å¼•èµ·ï¼Œå†…å®¹å·®å¼‚è¾ƒå¤§ï¼š"
    else:
        reason += "éƒ¨åˆ†ç›¸å…³ï¼Œä¸»è¦ç”±ä»¥ä¸‹å…±äº«è¯æ±‡å¼•èµ·ï¼š"
    total_words = len(set(word for word, _ in common_words))
    significant_words = [(word, count) for word, count in common_words if count > 2 or (count / total_words) > 0.1]
    if significant_words:
        key_terms = ", ".join(word for word, _ in significant_words[:3])
        reason += f" {key_terms} ç­‰ï¼Œ"
        avg_length = sum(len(word) for word, _ in significant_words) / len(significant_words)
        if avg_length > 5 and any(len(word) > 6 for word, _ in significant_words):
            reason += "å¯èƒ½åæ˜ æŠ€æœ¯æˆ–ä¸“ä¸šé¢†åŸŸç›¸å…³å†…å®¹ã€‚"
        elif len(significant_words) / total_words > 0.3:
            reason += "å¯èƒ½åæ˜ ç‰¹å®šä¸»é¢˜æˆ–é…ç½®ç›¸å…³å†…å®¹ã€‚"
        else:
            reason += "å¯èƒ½æ¶‰åŠå¤šç§ä¸»é¢˜ï¼Œç›¸å…³æ€§éœ€è¿›ä¸€æ­¥éªŒè¯ã€‚"
    else:
        reason += "å…±äº«è¯æ±‡åˆ†å¸ƒä¸é›†ä¸­ï¼Œå†…å®¹ç›¸å…³æ€§ä¸æ˜æ˜¾ã€‚"
    return reason

def load_similarities():
    """åŠ è½½ç°æœ‰ç›¸ä¼¼åº¦JSON"""
    if os.path.exists(SIMILARITIES_FILE):
        with open(SIMILARITIES_FILE, 'r') as f:
            return json.load(f)
    return {}

def save_similarities(data):
    """ä¿å­˜ç›¸ä¼¼åº¦JSON"""
    with open(SIMILARITIES_FILE, 'w') as f:
        json.dump(data, f, indent=4)

def smart_split_text(text, max_chunk_size=1000):
    """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¼˜å…ˆæŒ‰æ®µè½ã€å¥å­ã€è¡Œåˆ†å‰²"""
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    current_chunk = ""
    lines = text.splitlines()
    
    for line in lines:
        # å¦‚æœå½“å‰è¡ŒåŠ ä¸Šå½“å‰å—è¶…è¿‡æœ€å¤§å¤§å°ï¼Œä¿å­˜å½“å‰å—
        if len(current_chunk) + len(line) + 1 > max_chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = line
        else:
            if current_chunk:
                current_chunk += "\n" + line
            else:
                current_chunk = line
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks


def compute_differences(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„å·®å¼‚ï¼ˆæ™ºèƒ½åˆ†å‰²ï¼Œå¤šçº§åˆ«æ¯”è¾ƒï¼‰"""
    if not text1 and not text2:
        return []
    
    # ğŸš¨ ä¸´æ—¶ç¦ç”¨è¡¨æ ¼æ£€æµ‹ï¼Œä½¿ç”¨å¼ºåˆ¶å­—ç¬¦çº§é€è¡Œå·®å¼‚
    print("ğŸ”¥ ä½¿ç”¨å¼ºåˆ¶å­—ç¬¦çº§é€è¡Œå·®å¼‚ç®—æ³•")
    return compute_character_level_line_differences(text1, text2)

def compute_character_level_line_differences(text1, text2):
    """
    å­—ç¬¦çº§é€è¡Œå·®å¼‚æ£€æµ‹ - å½»åº•è§£å†³å¤§æ®µå·®å¼‚é—®é¢˜
    """
    print("ğŸ”¥ å¯åŠ¨å­—ç¬¦çº§é€è¡Œå·®å¼‚æ£€æµ‹")
    
    # ğŸš¨ é—®é¢˜å‘ç°ï¼šæ–‡æ¡£ä¸­çš„\nè¢«å½“æˆäº†ä¸€æ•´è¡Œï¼
    # è§£å†³æ–¹æ¡ˆï¼šå…ˆæŒ‰çœŸå®è¡Œåˆ†å‰²ï¼Œç„¶åè¿›ä¸€æ­¥åˆ†å‰²é•¿è¡Œ
    
    def split_into_real_lines(text):
        """å°†æ–‡æœ¬åˆ†å‰²æˆçœŸå®çš„è¡Œï¼Œå¤„ç†æ–‡æœ¬ä¸­ä»¥å­—é¢å½¢å¼å­˜åœ¨çš„"\\n"ä¸è¿‡é•¿è¡Œã€‚
        ç›®æ ‡ï¼šå°½é‡æŠŠæ®µè½æ‹†åˆ°å¥å­çº§/æ¡ç›®çº§ï¼Œä»¥é¿å…å¤§æ®µå·®å¼‚ã€‚
        """
        import re
        # å…ˆæŠŠå­—é¢"\\n"æ ‡å‡†åŒ–ä¸ºçœŸæ­£çš„æ¢è¡Œ
        normalized_text = text.replace('\\r\\n', '\n').replace('\\n', '\n')
        # å†æŒ‰æ¢è¡Œåˆ‡åˆ†
        initial_lines = normalized_text.split('\n')
        real_lines = []

        for line in initial_lines:
            curr = line.strip()
            if not curr:
                continue

            # è‹¥è¯¥è¡Œä»å¾ˆé•¿ï¼Œåˆ†ä¸¤çº§ï¼šå¥å­çº§ -> çŸ­è¯­çº§
            if len(curr) > 100:
                # å¥å­çº§ï¼šæŒ‰ä¸­æ–‡/è‹±æ–‡æ ‡ç‚¹åˆ‡åˆ†ï¼Œå¹¶ä¿ç•™æ ‡ç‚¹åˆ°ç‰‡æ®µå°¾éƒ¨
                sentence_parts = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›;:,ï¼Œã€])\s*', curr)
                for sent in sentence_parts:
                    s = sent.strip()
                    if not s:
                        continue
                    if len(s) > 100:
                        # çŸ­è¯­çº§ï¼šè¿›ä¸€æ­¥æŒ‰é¡¿å·/é€—å·/å†’å·/ç©ºç™½å—ç­‰åˆ‡åˆ†
                        phrase_parts = re.split(r'[ã€ï¼Œ,ï¼š:ï¼›;]|\s{2,}|\t+', s)
                        for ph in phrase_parts:
                            p = ph.strip()
                            if not p:
                                continue
                            # ä»è¿‡é•¿åˆ™æŒ‰æ•°å­—/å•ä½æ··åˆæ¨¡å¼è¿›ä¸€æ­¥åˆ‡ï¼ˆè¡¨æ ¼å¼æ•°æ®å‹å¥½ï¼‰
                            if len(p) > 100:
                                number_parts = re.split(r'(?:\d+\.?\d*\s*){1,}', p)
                                for np in number_parts:
                                    nps = np.strip()
                                    if nps:
                                        real_lines.append(nps)
                            else:
                                real_lines.append(p)
                    else:
                        real_lines.append(s)
            else:
                real_lines.append(curr)

        return real_lines

    def split_into_clauses(s: str):
        """æŠŠä¸€è¡Œåˆ‡æˆæ›´å°çš„è¯­ä¹‰ç‰‡æ®µï¼ˆå¥å­/çŸ­è¯­/è¡¨æ ¼å•å…ƒï¼‰ã€‚"""
        import re
        if not s:
            return []
        # è§„èŒƒåŒ–å­—é¢"\\n"å¹¶å»ä¸¤ç«¯ç©ºç™½
        s = s.replace('\\r\\n', '\n').replace('\\n', '\n').strip()
        # å…ˆæŒ‰å¥å­ç»“æŸç¬¦/æ¢è¡Œåˆ‡åˆ†ï¼ˆä¿ç•™æ ‡ç‚¹åœ¨ç‰‡æ®µå°¾éƒ¨ï¼‰
        parts = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›;])\s*|\n+', s)
        clauses = []
        for part in parts:
            t = part.strip()
            if not t:
                continue
            if len(t) > 80:
                # å¯¹è¾ƒé•¿ç‰‡æ®µè¿›ä¸€æ­¥ç”¨é€—å·/é¡¿å·/å†’å·ç­‰ç»†åˆ†
                subparts = re.split(r'[ï¼Œã€,:ï¼š;ï¼›]|\s{2,}|\t+', t)
                for sp in subparts:
                    sp = sp.strip()
                    if sp:
                        clauses.append(sp)
            else:
                clauses.append(t)
        return clauses

    def create_clause_level_diffs(orig_line: str, new_line: str, o_idx: int, n_idx: int):
        """åœ¨åŒä¸€è¡Œå†…ï¼ŒåŸºäºå­å¥ï¼ˆå¥å­/çŸ­è¯­ï¼‰ç”Ÿæˆæœ€å°å·®å¼‚å—ã€‚"""
        from difflib import SequenceMatcher as _Seq
        diffs = []
        orig_clauses = split_into_clauses(orig_line)
        new_clauses = split_into_clauses(new_line)
        matcher_c = _Seq(None, orig_clauses, new_clauses)
        for ctag, oi1, oi2, nj1, nj2 in matcher_c.get_opcodes():
            if ctag == 'equal':
                continue
            original_chunk = ''.join(orig_clauses[oi1:oi2]) if oi2 > oi1 else ''
            new_chunk = ''.join(new_clauses[nj1:nj2]) if nj2 > nj1 else ''
            if not original_chunk and not new_chunk:
                continue
            if ctag == 'delete':
                d_type = 'deleted'
            elif ctag == 'insert':
                d_type = 'added'
            else:
                d_type = 'modified'
            diffs.append({
                'id': str(uuid.uuid4()),
                'type': d_type,
                'original': (original_chunk[:200] + ('...' if len(original_chunk) > 200 else '')) if original_chunk else '',
                'new': (new_chunk[:200] + ('...' if len(new_chunk) > 200 else '')) if new_chunk else '',
                'start_idx_orig': o_idx,
                'end_idx_orig': o_idx + (1 if original_chunk else 0),
                'start_idx_new': n_idx,
                'end_idx_new': n_idx + (1 if new_chunk else 0),
                'line_numbers_orig': [o_idx] if original_chunk else [],
                'line_numbers_new': [n_idx] if new_chunk else []
            })
        return diffs
    
    lines1 = split_into_real_lines(text1)
    lines2 = split_into_real_lines(text2)
    
    print(f"ğŸ”¥ åŸæ–‡{len(lines1)}è¡Œï¼Œæ–°æ–‡{len(lines2)}è¡Œ")
    
    differences = []
    
    # ä½¿ç”¨difflibè¿›è¡Œè¡Œçº§æ¯”è¾ƒ
    from difflib import SequenceMatcher
    matcher = SequenceMatcher(None, lines1, lines2)
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
            
        print(f"ğŸ”¥ å‘ç°{tag}å·®å¼‚: åŸæ–‡{i1}-{i2}è¡Œ, æ–°æ–‡{j1}-{j2}è¡Œ")
        
        # å¯¹äºæ¯ä¸ªå·®å¼‚å—ï¼Œå¼ºåˆ¶åˆ›å»ºé€è¡Œå·®å¼‚
        if tag == 'delete':
            # åˆ é™¤ï¼šåŸæ–‡æœ‰å†…å®¹ï¼Œæ–°æ–‡æ²¡æœ‰
            for line_idx in range(i1, i2):
                line_content = lines1[line_idx]
                if line_content.strip():  # åªå¤„ç†éç©ºè¡Œ
                    diff = {
                        'id': str(uuid.uuid4()),
                        'type': 'deleted',
                        'original': line_content[:200] + ('...' if len(line_content) > 200 else ''),  # ğŸ”¥ é™åˆ¶å†…å®¹é•¿åº¦
                        'new': '',
                        'start_idx_orig': line_idx,
                        'end_idx_orig': line_idx + 1,
                        'start_idx_new': j1,
                        'end_idx_new': j1,
                        'line_numbers_orig': [line_idx],
                        'line_numbers_new': []
                    }
                    differences.append(diff)
                    print(f"ğŸ”¥ åˆ é™¤è¡Œ{line_idx}: {line_content[:50]}...")
                    
        elif tag == 'insert':
            # æ’å…¥ï¼šæ–°æ–‡æœ‰å†…å®¹ï¼ŒåŸæ–‡æ²¡æœ‰
            for line_idx in range(j1, j2):
                line_content = lines2[line_idx]
                if line_content.strip():  # åªå¤„ç†éç©ºè¡Œ
                    diff = {
                        'id': str(uuid.uuid4()),
                        'type': 'added',
                        'original': '',
                        'new': line_content[:200] + ('...' if len(line_content) > 200 else ''),  # ğŸ”¥ é™åˆ¶å†…å®¹é•¿åº¦
                        'start_idx_orig': i1,
                        'end_idx_orig': i1,
                        'start_idx_new': line_idx,
                        'end_idx_new': line_idx + 1,
                        'line_numbers_orig': [],
                        'line_numbers_new': [line_idx]
                    }
                    differences.append(diff)
                    print(f"ğŸ”¥ æ–°å¢è¡Œ{line_idx}: {line_content[:50]}...")
                    
        elif tag == 'replace':
            # æ›¿æ¢ï¼šé€è¡Œæ¯”è¾ƒå†…å®¹
            orig_lines = lines1[i1:i2]
            new_lines = lines2[j1:j2]
            max_lines = max(len(orig_lines), len(new_lines))
            
            print(f"ğŸ”¥ æ›¿æ¢å—ï¼šåŸæ–‡{len(orig_lines)}è¡Œ -> æ–°æ–‡{len(new_lines)}è¡Œï¼Œå¼ºåˆ¶é€è¡Œåˆ†è§£")
            
            for idx in range(max_lines):
                orig_line = orig_lines[idx] if idx < len(orig_lines) else ""
                new_line = new_lines[idx] if idx < len(new_lines) else ""

                # è¡Œç¼ºå¤±/æ–°å¢ï¼šä¿æŒè¡Œçº§ç²’åº¦
                if not orig_line and new_line:
                    diff = {
                        'id': str(uuid.uuid4()),
                        'type': 'added',
                        'original': '',
                        'new': new_line[:200] + ('...' if len(new_line) > 200 else ''),
                        'start_idx_orig': i1 + idx if idx < len(orig_lines) else i2,
                        'end_idx_orig': i1 + idx if idx < len(orig_lines) else i2,
                        'start_idx_new': j1 + idx if idx < len(new_lines) else j2,
                        'end_idx_new': j1 + idx + 1 if idx < len(new_lines) else j2,
                        'line_numbers_orig': [],
                        'line_numbers_new': [j1 + idx] if idx < len(new_lines) else []
                    }
                    differences.append(diff)
                    print(f"ğŸ”¥ addedå·®å¼‚ åŸæ–‡è¡Œ{i1 + idx} vs æ–°æ–‡è¡Œ{j1 + idx}")
                    continue
                if orig_line and not new_line:
                    diff = {
                        'id': str(uuid.uuid4()),
                        'type': 'deleted',
                        'original': orig_line[:200] + ('...' if len(orig_line) > 200 else ''),
                        'new': '',
                        'start_idx_orig': i1 + idx if idx < len(orig_lines) else i2,
                        'end_idx_orig': i1 + idx + 1 if idx < len(orig_lines) else i2,
                        'start_idx_new': j1 + idx if idx < len(new_lines) else j2,
                        'end_idx_new': j1 + idx if idx < len(new_lines) else j2,
                        'line_numbers_orig': [i1 + idx] if idx < len(orig_lines) else [],
                        'line_numbers_new': []
                    }
                    differences.append(diff)
                    print(f"ğŸ”¥ deletedå·®å¼‚ åŸæ–‡è¡Œ{i1 + idx} vs æ–°æ–‡è¡Œ{j1 + idx}")
                    continue

                # åŒè¡Œä¿®æ”¹ï¼šè¿›è¡Œå­å¥/çŸ­è¯­çº§ç»†åˆ†ï¼Œæœ€å°åŒ–å·®å¼‚å—
                if orig_line != new_line:
                    clause_diffs = create_clause_level_diffs(orig_line, new_line, i1 + idx, j1 + idx)
                    # å›é€€ï¼šè‹¥å­å¥çº§æœªäº§ç”Ÿå·®å¼‚ï¼Œåˆ™é€€å›å•è¡Œmodified
                    if not clause_diffs:
                        differences.append({
                            'id': str(uuid.uuid4()),
                            'type': 'modified',
                            'original': orig_line[:200] + ('...' if len(orig_line) > 200 else ''),
                            'new': new_line[:200] + ('...' if len(new_line) > 200 else ''),
                            'start_idx_orig': i1 + idx,
                            'end_idx_orig': i1 + idx + 1,
                            'start_idx_new': j1 + idx,
                            'end_idx_new': j1 + idx + 1,
                            'line_numbers_orig': [i1 + idx],
                            'line_numbers_new': [j1 + idx]
                        })
                        print(f"ğŸ”¥ modifiedå·®å¼‚ åŸæ–‡è¡Œ{i1 + idx} vs æ–°æ–‡è¡Œ{j1 + idx}")
                    else:
                        differences.extend(clause_diffs)
                        print(f"ğŸ”¥ å­å¥çº§ç»†åˆ†å®Œæˆ åŸæ–‡è¡Œ{i1 + idx} vs æ–°æ–‡è¡Œ{j1 + idx} â†’ ç”Ÿæˆ{len(clause_diffs)}ä¸ªå·®å¼‚ç‰‡æ®µ")
    
    print(f"ğŸ”¥ å­—ç¬¦çº§åˆ†æå®Œæˆï¼Œç”Ÿæˆ{len(differences)}ä¸ªé€è¡Œå·®å¼‚")
    return differences
    
    # åŸæœ‰é€»è¾‘ï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰
    # is_structured_doc = detect_structured_document(text1, text2)
    # is_table_doc = detect_table_document(text1, text2)
    # print(f"ğŸ” æ–‡æ¡£ç±»å‹æ£€æµ‹: ç»“æ„åŒ–={is_structured_doc}, è¡¨æ ¼={is_table_doc}")
    # if is_structured_doc:
    #     print("ğŸ“Š ä½¿ç”¨ç»“æ„åŒ–æ–‡æ¡£å·®å¼‚ç®—æ³•")
    #     return compute_structured_document_differences(text1, text2)
    # else:
    #     print("ğŸ“ ä½¿ç”¨æ™®é€šæ–‡æœ¬å·®å¼‚ç®—æ³•")
    #     return compute_general_text_differences(text1, text2)

def detect_structured_document(text1, text2):
    """æ£€æµ‹æ˜¯å¦æ˜¯ç»“æ„åŒ–æ–‡æ¡£ï¼ˆå¦‚Wordæ–‡æ¡£ï¼‰"""
    # æ£€æµ‹æ ‡å‡†ï¼š
    # 1. æœ‰è¾ƒå¤šçš„çŸ­è¡Œï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ã€åˆ—è¡¨é¡¹ç­‰ï¼‰
    # 2. æœ‰æ˜æ˜¾çš„æ®µè½ç»“æ„
    # 3. å†…å®¹ç›¸å¯¹ç®€æ´ï¼Œä¸æ˜¯å¤§æ®µè¿ç»­æ–‡æœ¬
    # 4. åŒ…å«è¡¨æ ¼ç±»æ•°æ®ç‰¹å¾
    
    def analyze_text_structure(text):
        lines = text.splitlines()
        if len(lines) < 2:
            return False
            
        short_lines = sum(1 for line in lines if len(line.strip()) < 50 and len(line.strip()) > 0)
        total_lines = len([line for line in lines if line.strip()])
        
        if total_lines == 0:
            return False
            
        short_line_ratio = short_lines / total_lines
        avg_line_length = sum(len(line.strip()) for line in lines if line.strip()) / total_lines
        
        # æ£€æµ‹è¡¨æ ¼ç‰¹å¾
        numeric_lines = sum(1 for line in lines if any(char.isdigit() for char in line) and len(line.strip()) > 0)
        numeric_ratio = numeric_lines / total_lines if total_lines > 0 else 0
        
        # æ£€æµ‹é‡å¤æ¨¡å¼ï¼ˆè¡¨æ ¼è¡Œï¼‰
        pattern_lines = 0
        for line in lines:
            if line.strip() and ('\t' in line or line.count(' ') > 5):
                pattern_lines += 1
        pattern_ratio = pattern_lines / total_lines if total_lines > 0 else 0
        
        # å¦‚æœçŸ­è¡Œæ¯”ä¾‹é«˜ä¸”å¹³å‡è¡Œé•¿é€‚ä¸­ï¼Œæˆ–è€…æœ‰å¤§é‡æ•°å­—å’Œè¡¨æ ¼æ¨¡å¼ï¼Œå¯èƒ½æ˜¯ç»“æ„åŒ–æ–‡æ¡£
        return ((short_line_ratio > 0.3 and 20 < avg_line_length < 100) or 
                (numeric_ratio > 0.4) or 
                (pattern_ratio > 0.3))
    
    return analyze_text_structure(text1) or analyze_text_structure(text2)

def detect_table_document(text1, text2):
    """æ£€æµ‹æ˜¯å¦æ˜¯è¡¨æ ¼å¯†é›†å‹æ–‡æ¡£"""
    import re
    
    def has_table_characteristics(text):
        lines = text.splitlines()
        total_lines = len([line for line in lines if line.strip()])
        if total_lines == 0:
            return False
            
        # æ£€æµ‹æ•°å­—è¡Œæ¯”ä¾‹
        numeric_lines = sum(1 for line in lines if re.search(r'\d+.*\d+', line))
        numeric_ratio = numeric_lines / total_lines
        
        # æ£€æµ‹åˆ¶è¡¨ç¬¦æˆ–å¤šç©ºæ ¼åˆ†éš”
        structured_lines = sum(1 for line in lines if '\t' in line or re.search(r'\s{2,}', line))
        structured_ratio = structured_lines / total_lines
        
        # æ£€æµ‹ä»·æ ¼ã€æ•°é‡ç­‰è¡¨æ ¼ç‰¹å¾
        table_pattern_lines = sum(1 for line in lines if re.search(r'(\d+\.?\d*\s+){2,}', line))
        table_ratio = table_pattern_lines / total_lines
        
        print(f"ğŸ“‹ è¡¨æ ¼æ£€æµ‹: æ•°å­—è¡Œæ¯”ä¾‹={numeric_ratio:.2f}, ç»“æ„è¡Œæ¯”ä¾‹={structured_ratio:.2f}, è¡¨æ ¼æ¨¡å¼æ¯”ä¾‹={table_ratio:.2f}")
        
        # å¤§å¹…é™ä½é˜ˆå€¼ï¼Œå¼ºåˆ¶å¯ç”¨è¡¨æ ¼æ¨¡å¼
        result = numeric_ratio > 0.1 or structured_ratio > 0.2 or table_ratio > 0.05 or total_lines > 20
        print(f"ğŸ“‹ è¡¨æ ¼æ£€æµ‹ç»“æœ: {result} (å¼ºåˆ¶è¡¨æ ¼æ¨¡å¼)")
        return result
    
    return has_table_characteristics(text1) or has_table_characteristics(text2)

def compute_structured_document_differences(text1, text2):
    """è®¡ç®—ç»“æ„åŒ–æ–‡æ¡£çš„å·®å¼‚ï¼ˆä¼˜åŒ–çš„Wordæ–‡æ¡£æ¯”å¯¹ç®—æ³•ï¼‰"""
    # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†ï¼Œè§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    text1_normalized = normalize_whitespace(text1)
    text2_normalized = normalize_whitespace(text2)
    
    # æ£€æµ‹æ˜¯å¦æ˜¯è¡¨æ ¼å¯†é›†å‹æ–‡æ¡£
    is_table_doc = detect_table_document(text1, text2)
    
    if is_table_doc:
        print("ğŸ“‹ æ£€æµ‹åˆ°è¡¨æ ¼æ–‡æ¡£ï¼Œä½¿ç”¨è¡Œçº§æ¯”è¾ƒç®—æ³•")
        # è¡¨æ ¼æ–‡æ¡£ä½¿ç”¨è¡Œçº§æ¯”è¾ƒ
        return compute_table_document_differences(text1_normalized, text2_normalized)
    else:
        print("ğŸ“„ æ£€æµ‹åˆ°æ™®é€šç»“æ„åŒ–æ–‡æ¡£ï¼Œä½¿ç”¨æ®µè½çº§æ¯”è¾ƒç®—æ³•")
        # æ™®é€šç»“æ„åŒ–æ–‡æ¡£ä½¿ç”¨æ®µè½çº§æ¯”è¾ƒ
        return compute_paragraph_document_differences(text1_normalized, text2_normalized)

def compute_table_document_differences(text1, text2):
    """ä¸“é—¨å¤„ç†è¡¨æ ¼å¯†é›†å‹æ–‡æ¡£çš„å·®å¼‚ - å¼ºåˆ¶é€è¡Œæ¯”è¾ƒæ¨¡å¼"""
    lines1 = text1.splitlines()
    lines2 = text2.splitlines()
    
    print(f"ğŸ“‹ è¡¨æ ¼æ–‡æ¡£è¡Œæ•°: åŸæ–‡{len(lines1)}è¡Œ, æ–°æ–‡{len(lines2)}è¡Œ")
    print(f"ğŸ“‹ å¯ç”¨å¼ºåˆ¶é€è¡Œæ¯”è¾ƒæ¨¡å¼")
    
    # è¿‡æ»¤ç©ºè¡Œï¼Œä½†ä¿ç•™ç´¢å¼•æ˜ å°„
    non_empty_lines1 = []
    non_empty_lines2 = []
    line_map1 = {}  # æ˜ å°„è¿‡æ»¤åç´¢å¼•åˆ°åŸå§‹ç´¢å¼•
    line_map2 = {}
    
    for i, line in enumerate(lines1):
        if line.strip():
            line_map1[len(non_empty_lines1)] = i
            non_empty_lines1.append(line.strip())
    
    for i, line in enumerate(lines2):
        if line.strip():
            line_map2[len(non_empty_lines2)] = i
            non_empty_lines2.append(line.strip())
    
    print(f"ğŸ“‹ è¿‡æ»¤ç©ºè¡Œå: åŸæ–‡{len(non_empty_lines1)}è¡Œ, æ–°æ–‡{len(non_empty_lines2)}è¡Œ")
    
    # å¼ºåˆ¶ä½¿ç”¨é€è¡Œæ¯”è¾ƒï¼Œä¸å…è®¸ä»»ä½•å¤§å—å·®å¼‚
    differences = []
    
    # ç›´æ¥è°ƒç”¨ç»†åˆ†å‡½æ•°ï¼Œè·³è¿‡åˆå§‹çš„å¤§å—æ¯”è¾ƒ
    print(f"ğŸ“‹ ç›´æ¥è¿›è¡Œé€è¡Œç»†åˆ†æ¯”è¾ƒ")
    differences = split_large_table_difference(
        non_empty_lines1, non_empty_lines2, 
        0, 0, line_map1, line_map2
    )
    
    print(f"ğŸ“‹ æœ€ç»ˆç”Ÿæˆ{len(differences)}ä¸ªå·®å¼‚")
    return differences

def split_large_table_difference(lines1, lines2, offset1, offset2, line_map1, line_map2):
    """å°†å¤§çš„è¡¨æ ¼å·®å¼‚å—åˆ†è§£ä¸ºæœ€å°ç²’åº¦çš„é€è¡Œå·®å¼‚"""
    differences = []
    
    print(f"ğŸ“‹ å¼€å§‹ç»†åˆ†å·®å¼‚: åŸæ–‡{len(lines1)}è¡Œ, æ–°æ–‡{len(lines2)}è¡Œ")
    
    # ä½¿ç”¨æ›´ç²¾ç»†çš„é€è¡Œæ¯”è¾ƒç®—æ³•
    from difflib import SequenceMatcher
    matcher = SequenceMatcher(None, lines1, lines2)
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
            
        print(f"ğŸ“‹ ç»†åˆ†å·®å¼‚: {tag}, åŸæ–‡{i1}-{i2}, æ–°æ–‡{j1}-{j2}")
        
        # å¯¹äºreplaceç±»å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç† - é€è¡Œæ¯”è¾ƒæ‰¾å‡ºçœŸæ­£ä¸åŒçš„è¡Œ
        if tag == 'replace':
            print(f"ğŸ“‹ å¤„ç†replaceå—: åŸæ–‡{i1}-{i2} vs æ–°æ–‡{j1}-{j2}")
            # ä½¿ç”¨é€è¡Œæ¯”è¾ƒæ‰¾å‡ºçœŸæ­£çš„å·®å¼‚
            lines1_chunk = lines1[i1:i2]
            lines2_chunk = lines2[j1:j2]
            
            # å¯¹è¿™ä¸ªå—å†…éƒ¨å†æ¬¡ä½¿ç”¨SequenceMatcher
            chunk_matcher = SequenceMatcher(None, lines1_chunk, lines2_chunk)
            
            # å¦‚æœreplaceå—å¤ªå¤§ï¼Œå¼ºåˆ¶é€è¡Œåˆ†è§£ï¼Œä¸å†ä½¿ç”¨SequenceMatcher
            if (i2 - i1) > 10 or (j2 - j1) > 10:
                print(f"ğŸ“‹ replaceå—è¿‡å¤§ï¼Œå¼ºåˆ¶é€è¡Œåˆ†è§£")
                max_lines = max(i2 - i1, j2 - j1)
                for idx in range(max_lines):
                    line1_idx = i1 + idx if idx < (i2 - i1) else None
                    line2_idx = j1 + idx if idx < (j2 - j1) else None
                    
                    line1 = lines1[line1_idx] if line1_idx is not None else ""
                    line2 = lines2[line2_idx] if line2_idx is not None else ""
                    
                    if line1 != line2:
                        diff = create_single_line_difference(
                            line1, line2,
                            offset1 + (line1_idx if line1_idx is not None else i2),
                            offset2 + (line2_idx if line2_idx is not None else j2),
                            line_map1, line_map2
                        )
                        if diff:
                            differences.append(diff)
                            print(f"ğŸ“‹ å¼ºåˆ¶å•è¡Œå·®å¼‚: {diff['type']}, åŸæ–‡è¡Œ{diff['start_idx_orig']}, æ–°æ–‡è¡Œ{diff['start_idx_new']}")
                continue  # è·³è¿‡åé¢çš„chunk_matcheré€»è¾‘
            for chunk_tag, ci1, ci2, cj1, cj2 in chunk_matcher.get_opcodes():
                if chunk_tag == 'equal':
                    continue
                    
                print(f"ğŸ“‹ å—å†…å·®å¼‚: {chunk_tag}, å­å—{ci1}-{ci2} vs {cj1}-{cj2}")
                
                # ä¸ºæ¯ä¸ªå­å—åˆ›å»ºå•è¡Œå·®å¼‚
                max_sub_lines = max(ci2 - ci1, cj2 - cj1)
                for sub_idx in range(max_sub_lines):
                    sub_line1_idx = ci1 + sub_idx if sub_idx < (ci2 - ci1) else None
                    sub_line2_idx = cj1 + sub_idx if sub_idx < (cj2 - cj1) else None
                    
                    sub_line1 = lines1_chunk[sub_line1_idx] if sub_line1_idx is not None else ""
                    sub_line2 = lines2_chunk[sub_line2_idx] if sub_line2_idx is not None else ""
                    
                    if sub_line1 != sub_line2:
                        global_line1_idx = i1 + (sub_line1_idx if sub_line1_idx is not None else (ci2 - ci1))
                        global_line2_idx = j1 + (sub_line2_idx if sub_line2_idx is not None else (cj2 - cj1))
                        
                        diff = create_single_line_difference(
                            sub_line1, sub_line2,
                            offset1 + global_line1_idx,
                            offset2 + global_line2_idx,
                            line_map1, line_map2
                        )
                        if diff:
                            differences.append(diff)
                            print(f"ğŸ“‹ åˆ›å»ºå•è¡Œå·®å¼‚: {diff['type']}, åŸæ–‡è¡Œ{diff['start_idx_orig']}, æ–°æ–‡è¡Œ{diff['start_idx_new']}")
        else:
            # å¯¹äºdeleteå’Œinsertï¼Œç›´æ¥é€è¡Œåˆ›å»ºå·®å¼‚
            max_lines = max(i2 - i1, j2 - j1)
            for idx in range(max_lines):
                line1_idx = i1 + idx if idx < (i2 - i1) else None
                line2_idx = j1 + idx if idx < (j2 - j1) else None
                
                line1 = lines1[line1_idx] if line1_idx is not None else ""
                line2 = lines2[line2_idx] if line2_idx is not None else ""
                
                # åªæœ‰å½“è¡Œä¸åŒæ—¶æ‰åˆ›å»ºå·®å¼‚
                if line1 != line2:
                    diff = create_single_line_difference(
                        line1, line2, 
                        offset1 + (line1_idx if line1_idx is not None else i2),
                        offset2 + (line2_idx if line2_idx is not None else j2),
                        line_map1, line_map2
                    )
                    if diff:
                        differences.append(diff)
                        print(f"ğŸ“‹ åˆ›å»ºå•è¡Œå·®å¼‚: {diff['type']}, åŸæ–‡è¡Œ{diff['start_idx_orig']}, æ–°æ–‡è¡Œ{diff['start_idx_new']}")
    
    print(f"ğŸ“‹ ç»†åˆ†å®Œæˆï¼Œç”Ÿæˆ{len(differences)}ä¸ªç²¾ç»†å·®å¼‚")
    return differences

def create_single_line_difference(line1, line2, orig_offset, new_offset, line_map1, line_map2):
    """åˆ›å»ºå•è¡Œå·®å¼‚"""
    diff_id = str(uuid.uuid4())
    
    # ç¡®å®šå·®å¼‚ç±»å‹
    if line1 and line2:
        diff_type = 'modified'
    elif line2 and not line1:
        diff_type = 'added'
    elif line1 and not line2:
        diff_type = 'deleted'
    else:
        return None
    
    orig_start = line_map1.get(orig_offset, orig_offset)
    orig_end = orig_start + 1
    
    new_start = line_map2.get(new_offset, new_offset)
    new_end = new_start + 1
    
    return {
        'id': diff_id,
        'type': diff_type,
        'original': line1,
        'new': line2,
        'start_idx_orig': orig_start,
        'end_idx_orig': orig_end,
        'start_idx_new': new_start,
        'end_idx_new': new_end,
        'char_diff': None,
        'line_numbers_orig': [orig_start] if line1 else [],
        'line_numbers_new': [new_start] if line2 else [],
        'context_before_orig': [],
        'context_after_orig': [],
        'context_before_new': [],
        'context_after_new': []
    }

def create_chunk_difference(tag, chunk1, chunk2, orig_offset, new_offset, line_map1, line_map2):
    """åˆ›å»ºå°å—å·®å¼‚"""
    if not chunk1 and not chunk2:
        return None
        
    diff_id = str(uuid.uuid4())
    
    # ç¡®å®šå·®å¼‚ç±»å‹
    if tag == 'replace':
        diff_type = 'modified'
    elif tag == 'insert':
        diff_type = 'added'
    elif tag == 'delete':
        diff_type = 'deleted'
    else:
        diff_type = tag
    
    orig_start = line_map1.get(orig_offset, orig_offset)
    orig_end = line_map1.get(orig_offset + len(chunk1) - 1, orig_start) + 1 if chunk1 else orig_start
    
    new_start = line_map2.get(new_offset, new_offset)
    new_end = line_map2.get(new_offset + len(chunk2) - 1, new_start) + 1 if chunk2 else new_start
    
    return {
        'id': diff_id,
        'type': diff_type,
        'original': '\n'.join(chunk1),
        'new': '\n'.join(chunk2),
        'start_idx_orig': orig_start,
        'end_idx_orig': orig_end,
        'start_idx_new': new_start,
        'end_idx_new': new_end,
        'char_diff': None,
        'line_numbers_orig': list(range(orig_start, orig_end)),
        'line_numbers_new': list(range(new_start, new_end)),
        'context_before_orig': [],
        'context_after_orig': [],
        'context_before_new': [],
        'context_after_new': []
    }

def create_table_difference(tag, lines1, lines2, i1, i2, j1, j2, line_map1, line_map2):
    """åˆ›å»ºè¡¨æ ¼å·®å¼‚å¯¹è±¡"""
    diff_id = str(uuid.uuid4())
    
    orig_content = '\n'.join(lines1[i1:i2]) if i1 < i2 else ''
    new_content = '\n'.join(lines2[j1:j2]) if j1 < j2 else ''
    
    # ç¡®å®šå·®å¼‚ç±»å‹
    if tag == 'replace':
        diff_type = 'modified'
    elif tag == 'insert':
        diff_type = 'added'
    elif tag == 'delete':
        diff_type = 'deleted'
    else:
        diff_type = tag
    
    # è®¡ç®—åŸå§‹è¡Œå·
    orig_start = line_map1.get(i1, 0)
    orig_end = line_map1.get(i2-1, orig_start) + 1 if i2 > 0 and (i2-1) in line_map1 else orig_start + 1
    
    new_start = line_map2.get(j1, 0)
    new_end = line_map2.get(j2-1, new_start) + 1 if j2 > 0 and (j2-1) in line_map2 else new_start + 1
    
    return {
        'id': diff_id,
        'type': diff_type,
        'original': orig_content,
        'new': new_content,
        'start_idx_orig': orig_start,
        'end_idx_orig': orig_end,
        'start_idx_new': new_start,
        'end_idx_new': new_end,
        'char_diff': None,
        'line_numbers_orig': list(range(orig_start, orig_end)),
        'line_numbers_new': list(range(new_start, new_end)),
        'context_before_orig': lines1[max(0, i1-2):i1] if i1 > 0 else [],
        'context_after_orig': lines1[i2:i2+2] if i2 < len(lines1) else [],
        'context_before_new': lines2[max(0, j1-2):j1] if j1 > 0 else [],
        'context_after_new': lines2[j2:j2+2] if j2 < len(lines2) else []
    }

def compute_paragraph_document_differences(text1, text2):
    """å¤„ç†æ™®é€šç»“æ„åŒ–æ–‡æ¡£çš„å·®å¼‚"""
    # ç¬¬äºŒæ­¥ï¼šæŒ‰æ®µè½åˆ†å‰²
    paragraphs1 = split_into_paragraphs(text1)
    paragraphs2 = split_into_paragraphs(text2)
    
    # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨æ”¹è¿›çš„åºåˆ—åŒ¹é…ç®—æ³•
    matcher = difflib.SequenceMatcher(None, paragraphs1, paragraphs2)
    differences = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
        
        # å¯¹äºå¤§çš„å·®å¼‚å—ï¼Œå°è¯•è¿›ä¸€æ­¥ç»†åˆ†
        if tag == 'replace' and (i2 - i1) > 1 and (j2 - j1) > 1:
            # åœ¨æ®µè½å†…éƒ¨è¿›ä¸€æ­¥æ¯”è¾ƒ
            sub_diffs = compute_paragraph_internal_differences(
                paragraphs1[i1:i2], paragraphs2[j1:j2], i1, j1
            )
            differences.extend(sub_diffs)
        else:
            # ç”Ÿæˆæ ‡å‡†å·®å¼‚
            diff = create_paragraph_difference(
                tag, paragraphs1, paragraphs2, i1, i2, j1, j2
            )
            if diff:
                differences.append(diff)
    
    return differences

def compute_general_text_differences(text1, text2):
    """è®¡ç®—æ™®é€šæ–‡æœ¬çš„å·®å¼‚ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
    # ä¼˜åŒ–ç­–ç•¥ï¼šå§‹ç»ˆä½¿ç”¨å¤šçº§åˆ«åˆ†æï¼Œé€‰æ‹©æœ€ä½³ç²’åº¦
    line_diffs = compute_line_level_differences(text1, text2)
    sentence_diffs = compute_sentence_level_differences(text1, text2)
    paragraph_diffs = compute_paragraph_level_differences(text1, text2)
    
    # è®¡ç®—æ¯ç§æ–¹æ³•çš„è´¨é‡åˆ†æ•°ï¼ˆå·®å¼‚æ•°é‡ vs å·®å¼‚å¤§å°çš„å¹³è¡¡ï¼‰
    def calculate_quality_score(diffs):
        if not diffs:
            return 0
        
        # åˆ†æ•° = å·®å¼‚æ•°é‡ * æƒé‡ - å¹³å‡å·®å¼‚å¤§å°çš„æƒ©ç½š
        diff_count = len(diffs)
        avg_size = sum(len(d.get('original', '')) + len(d.get('new', '')) for d in diffs) / max(diff_count, 1)
        
        # å·®å¼‚æ•°é‡è¶Šå¤šè¶Šå¥½ï¼ˆæ›´ç»†ç²’åº¦ï¼‰ï¼Œä½†å¹³å‡å¤§å°è¿‡å¤§æ—¶è¦æƒ©ç½š
        quality_score = diff_count * 10 - (avg_size / 100)
        return max(0, quality_score)
    
    line_score = calculate_quality_score(line_diffs)
    sentence_score = calculate_quality_score(sentence_diffs)
    paragraph_score = calculate_quality_score(paragraph_diffs)
    
    # é€‰æ‹©æœ€ä½³çš„åˆ†å‰²æ–¹æ³•
    best_diffs = line_diffs
    best_score = line_score
    
    if sentence_score > best_score:
        best_diffs = sentence_diffs
        best_score = sentence_score
        
    if paragraph_score > best_score:
        best_diffs = paragraph_diffs
        best_score = paragraph_score
    
    # å¦‚æœæœ€ä½³ç»“æœä»ç„¶ä¸å¤Ÿå¥½ï¼ˆå·®å¼‚è¿‡å°‘ä¸”è¿‡å¤§ï¼‰ï¼Œå°è¯•æ™ºèƒ½åˆ†å‰²
    if len(best_diffs) <= 2 and best_diffs:
        avg_diff_size = sum(len(d.get('original', '')) + len(d.get('new', '')) for d in best_diffs) / len(best_diffs)
        if avg_diff_size > 800:  # å¹³å‡å·®å¼‚å¤ªå¤§
            smart_diffs = compute_smart_chunk_differences(text1, text2)
            smart_score = calculate_quality_score(smart_diffs)
            if smart_score > best_score:
                best_diffs = smart_diffs
    
    return best_diffs

def normalize_whitespace(text):
    """è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦ï¼Œå‡å°‘å› æ ¼å¼å·®å¼‚å¯¼è‡´çš„è¯¯åˆ¤"""
    import re
    # ç»Ÿä¸€æ¢è¡Œç¬¦
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½ï¼Œä½†ä¿ç•™æ®µè½é—´çš„ç©ºè¡Œ
    lines = text.split('\n')
    normalized_lines = []
    for line in lines:
        if line.strip():
            normalized_lines.append(line.strip())
        else:
            # ä¿ç•™ç©ºè¡Œï¼Œä½†åªä¿ç•™ä¸€ä¸ª
            if normalized_lines and normalized_lines[-1] != '':
                normalized_lines.append('')
    
    return '\n'.join(normalized_lines)

def split_into_paragraphs(text):
    """å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½ï¼Œæ›´æ™ºèƒ½åœ°å¤„ç†ç»“æ„åŒ–æ–‡æ¡£"""
    # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
    paragraphs = []
    current_paragraph = []
    
    lines = text.split('\n')
    for line in lines:
        if line.strip():
            current_paragraph.append(line)
        else:
            # é‡åˆ°ç©ºè¡Œï¼Œç»“æŸå½“å‰æ®µè½
            if current_paragraph:
                paragraphs.append('\n'.join(current_paragraph))
                current_paragraph = []
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph:
        paragraphs.append('\n'.join(current_paragraph))
    
    return paragraphs

def compute_paragraph_internal_differences(paras1, paras2, offset1, offset2):
    """è®¡ç®—æ®µè½å†…éƒ¨çš„ç²¾ç»†å·®å¼‚"""
    differences = []
    
    # å°†å¤šä¸ªæ®µè½åˆå¹¶æˆä¸¤ä¸ªæ–‡æœ¬å—è¿›è¡Œæ¯”è¾ƒ
    text1 = '\n'.join(paras1)
    text2 = '\n'.join(paras2)
    
    # ä½¿ç”¨å¥å­çº§åˆ«æ¯”è¾ƒ
    sentences1 = split_into_sentences(text1)
    sentences2 = split_into_sentences(text2)
    
    matcher = difflib.SequenceMatcher(None, sentences1, sentences2)

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
        
        diff_id = str(uuid.uuid4())
        
        orig_content = ''.join(sentences1[i1:i2]) if i1 < i2 else ''
        new_content = ''.join(sentences2[j1:j2]) if j1 < j2 else ''
        
        # ç¡®å®šå·®å¼‚ç±»å‹
        if tag == 'replace':
            diff_type = 'modified'
        elif tag == 'insert':
            diff_type = 'added'
        elif tag == 'delete':
            diff_type = 'deleted'
        else:
            diff_type = tag
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚
        char_diff = None
        if diff_type == 'modified' and orig_content and new_content:
            char_diff = compute_char_level_diff(orig_content, new_content)
        
        diff = {
            'id': diff_id,
            'type': diff_type,
            'original': orig_content,
            'new': new_content,
            'start_idx_orig': offset1 + i1,
            'end_idx_orig': offset1 + i2,
            'start_idx_new': offset2 + j1,
            'end_idx_new': offset2 + j2,
            'char_diff': char_diff,
            'line_numbers_orig': [],
            'line_numbers_new': [],
            'context_before_orig': sentences1[max(0, i1-1):i1] if i1 > 0 else [],
            'context_after_orig': sentences1[i2:i2+1] if i2 < len(sentences1) else [],
            'context_before_new': sentences2[max(0, j1-1):j1] if j1 > 0 else [],
            'context_after_new': sentences2[j2:j2+1] if j2 < len(sentences2) else []
        }
        
        differences.append(diff)
    
    return differences

def create_paragraph_difference(tag, paras1, paras2, i1, i2, j1, j2):
    """åˆ›å»ºæ®µè½çº§åˆ«çš„å·®å¼‚å¯¹è±¡"""
    diff_id = str(uuid.uuid4())
    
    orig_content = '\n'.join(paras1[i1:i2]) if i1 < i2 else ''
    new_content = '\n'.join(paras2[j1:j2]) if j1 < j2 else ''
    
    # ç¡®å®šå·®å¼‚ç±»å‹
    if tag == 'replace':
        diff_type = 'modified'
    elif tag == 'insert':
        diff_type = 'added'
    elif tag == 'delete':
        diff_type = 'deleted'
    else:
        diff_type = tag
    
    # è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚
    char_diff = None
    if diff_type == 'modified' and orig_content and new_content:
        char_diff = compute_char_level_diff(orig_content, new_content)
    
    return {
            'id': diff_id,
            'type': diff_type,
            'original': orig_content,
            'new': new_content,
            'start_idx_orig': i1,
            'end_idx_orig': i2,
            'start_idx_new': j1,
            'end_idx_new': j2,
            'char_diff': char_diff,
        'line_numbers_orig': list(range(i1, i2)),
        'line_numbers_new': list(range(j1, j2)),
        'context_before_orig': paras1[max(0, i1-1):i1] if i1 > 0 else [],
        'context_after_orig': paras1[i2:i2+1] if i2 < len(paras1) else [],
        'context_before_new': paras2[max(0, j1-1):j1] if j1 > 0 else [],
        'context_after_new': paras2[j2:j2+1] if j2 < len(paras2) else []
    }


def compute_sentence_level_differences(text1, text2):
    """è®¡ç®—å¥å­çº§åˆ«çš„å·®å¼‚ï¼ˆä¼˜åŒ–çš„åˆ†æ®µæ¯”å¯¹ï¼‰"""
    # ä½¿ç”¨æ”¹è¿›çš„å¥å­åˆ†å‰²ï¼Œæ›´å¥½åœ°å¤„ç†ä¸­æ–‡
    sentences1 = split_into_sentences(text1)
    sentences2 = split_into_sentences(text2)
    
    # å¦‚æœå¥å­å¤ªå°‘ï¼Œè¿”å›ç©ºç»“æœè®©å…¶ä»–æ–¹æ³•å¤„ç†
    if len(sentences1) <= 1 and len(sentences2) <= 1:
        return []
    
    matcher = difflib.SequenceMatcher(None, sentences1, sentences2)
    differences = []
    
    # ç”¨äºè®¡ç®—åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„è¡Œå·ä½ç½®
    def find_line_positions(sentences, original_text):
        line_positions = []
        lines = original_text.splitlines()
        current_line = 0
        
        for sentence in sentences:
            sentence_start_line = current_line
            # åœ¨åŸå§‹è¡Œä¸­æŸ¥æ‰¾è¿™ä¸ªå¥å­
            found = False
            for i in range(current_line, len(lines)):
                if sentence.strip() in lines[i]:
                    sentence_start_line = i
                    current_line = i + 1
                    found = True
                    break
            
            if not found:
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œç»§ç»­ä½¿ç”¨å½“å‰è¡Œå·
                sentence_start_line = current_line
                current_line += 1
                
            line_positions.append(sentence_start_line)
        
        return line_positions
    
    line_pos1 = find_line_positions(sentences1, text1)
    line_pos2 = find_line_positions(sentences2, text2)

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
        
        # ç”Ÿæˆå”¯ä¸€ID
        diff_id = str(uuid.uuid4())
        
        # è·å–å·®å¼‚å†…å®¹
        orig_content = ''.join(sentences1[i1:i2]) if i1 < i2 else ''
        new_content = ''.join(sentences2[j1:j2]) if j1 < j2 else ''
        
        # ç¡®å®šå·®å¼‚ç±»å‹
        if tag == 'replace':
            diff_type = 'modified'
        elif tag == 'insert':
            diff_type = 'added'
        elif tag == 'delete':
            diff_type = 'deleted'
        else:
            diff_type = tag
        
        # è®¡ç®—åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        start_line_orig = line_pos1[i1] if i1 < len(line_pos1) else len(text1.splitlines())
        end_line_orig = line_pos1[i2-1] + 1 if i2 > 0 and (i2-1) < len(line_pos1) else start_line_orig + 1
        
        start_line_new = line_pos2[j1] if j1 < len(line_pos2) else len(text2.splitlines())
        end_line_new = line_pos2[j2-1] + 1 if j2 > 0 and (j2-1) < len(line_pos2) else start_line_new + 1
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚
        char_diff = None
        if diff_type == 'modified' and orig_content and new_content:
            char_diff = compute_char_level_diff(orig_content, new_content)
        
        diff = {
            'id': diff_id,
            'type': diff_type,
            'original': orig_content,
            'new': new_content,
            'start_idx_orig': start_line_orig,
            'end_idx_orig': end_line_orig,
            'start_idx_new': start_line_new,
            'end_idx_new': end_line_new,
            'char_diff': char_diff,
            'line_numbers_orig': list(range(start_line_orig, end_line_orig)),
            'line_numbers_new': list(range(start_line_new, end_line_new)),
            'context_before_orig': sentences1[max(0, i1-1):i1] if i1 > 0 else [],
            'context_after_orig': sentences1[i2:i2+1] if i2 < len(sentences1) else [],
            'context_before_new': sentences2[max(0, j1-1):j1] if j1 > 0 else [],
            'context_after_new': sentences2[j2:j2+1] if j2 < len(sentences2) else []
        }
        
        differences.append(diff)

    return differences


def split_into_sentences(text):
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­ï¼ˆä¼˜åŒ–çš„ä¸­è‹±æ–‡æ··åˆå¤„ç†ï¼‰"""
    import re
    
    # æ”¹è¿›çš„å¥å­åˆ†å‰²æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´å¥½åœ°å¤„ç†ä¸­æ–‡æ ‡ç‚¹
    # å¤„ç†ä¸­æ–‡å¥å·ï¼ˆã€‚ï¼‰ã€é—®å·ï¼ˆï¼Ÿï¼‰ã€æ„Ÿå¹å·ï¼ˆï¼ï¼‰ä»¥åŠè‹±æ–‡çš„å¥å·ã€é—®å·ã€æ„Ÿå¹å·
    sentence_pattern = r'([ã€‚ï¼ï¼Ÿ.!?]+)'
    
    # åˆ†å‰²æ–‡æœ¬ï¼Œä¿ç•™åˆ†éš”ç¬¦
    parts = re.split(sentence_pattern, text)
    
    # é‡æ–°ç»„åˆå¥å­å’Œæ ‡ç‚¹ç¬¦å·
    sentences = []
    current_sentence = ""
    
    for i, part in enumerate(parts):
        if part.strip():
            if re.match(sentence_pattern, part):
                # è¿™æ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œæ·»åŠ åˆ°å½“å‰å¥å­
                current_sentence += part
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""
            else:
                # è¿™æ˜¯æ–‡æœ¬å†…å®¹
                current_sentence += part
    
    # å¤„ç†æœ€åä¸€ä¸ªå¥å­ï¼ˆå¦‚æœæ²¡æœ‰æ ‡ç‚¹ç»“å°¾ï¼‰
    if current_sentence.strip():
        sentences.append(current_sentence.strip())
    
    # è¿‡æ»¤æ‰ç©ºå¥å­å’Œåªæœ‰æ ‡ç‚¹çš„å¥å­
    result = []
    for sentence in sentences:
        cleaned = re.sub(r'^[ã€‚ï¼ï¼Ÿ.!?\s]+$', '', sentence)
        if cleaned and len(cleaned.strip()) > 1:
                result.append(sentence)
    
    # å¦‚æœåˆ†å¥ç»“æœå¤ªå°‘ï¼Œå°è¯•æŒ‰æ¢è¡Œç¬¦åˆ†å‰²
    if len(result) <= 1 and '\n' in text:
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if len(lines) > len(result):
            result = lines
    
    return result


def compute_line_level_differences(text1, text2):
    """è®¡ç®—è¡Œçº§åˆ«çš„å·®å¼‚"""
    lines1 = text1.splitlines()
    lines2 = text2.splitlines()
    matcher = difflib.SequenceMatcher(None, lines1, lines2)
    differences = []

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
        
        # ç”Ÿæˆå”¯ä¸€ID
        diff_id = str(uuid.uuid4())
        
        # è·å–å·®å¼‚å†…å®¹
        orig_content = '\n'.join(lines1[i1:i2]) if i1 < i2 else ''
        new_content = '\n'.join(lines2[j1:j2]) if j1 < j2 else ''
        
        # ç¡®å®šå·®å¼‚ç±»å‹
        if tag == 'replace':
            diff_type = 'modified'
        elif tag == 'insert':
            diff_type = 'added'
        elif tag == 'delete':
            diff_type = 'deleted'
        else:
            diff_type = tag
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚ï¼ˆç”¨äºæ›´ç²¾ç¡®çš„é«˜äº®ï¼‰
        char_diff = None
        if diff_type == 'modified' and orig_content and new_content:
            char_diff = compute_char_level_diff(orig_content, new_content)
        
        diff = {
            'id': diff_id,
            'type': diff_type,
            'original': orig_content,
            'new': new_content,
            'start_idx_orig': i1,
            'end_idx_orig': i2,
            'start_idx_new': j1,
            'end_idx_new': j2,
            'char_diff': char_diff,
            'line_numbers_orig': list(range(i1 + 1, i2 + 1)) if i1 < i2 else [],
            'line_numbers_new': list(range(j1 + 1, j2 + 1)) if j1 < j2 else [],
            'context_before_orig': lines1[max(0, i1-2):i1] if i1 > 0 else [],
            'context_after_orig': lines1[i2:i2+2] if i2 < len(lines1) else [],
            'context_before_new': lines2[max(0, j1-2):j1] if j1 > 0 else [],
            'context_after_new': lines2[j2:j2+2] if j2 < len(lines2) else []
        }
        
        differences.append(diff)

    return differences


def compute_paragraph_level_differences(text1, text2):
    """è®¡ç®—æ®µè½çº§åˆ«çš„å·®å¼‚"""
    # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
    paragraphs1 = [p.strip() for p in text1.split('\n\n') if p.strip()]
    paragraphs2 = [p.strip() for p in text2.split('\n\n') if p.strip()]
    
    matcher = difflib.SequenceMatcher(None, paragraphs1, paragraphs2)
    differences = []

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
        
        # ç”Ÿæˆå”¯ä¸€ID
        diff_id = str(uuid.uuid4())
        
        # è·å–å·®å¼‚å†…å®¹
        orig_content = '\n\n'.join(paragraphs1[i1:i2]) if i1 < i2 else ''
        new_content = '\n\n'.join(paragraphs2[j1:j2]) if j1 < j2 else ''
        
        # ç¡®å®šå·®å¼‚ç±»å‹
        if tag == 'replace':
            diff_type = 'modified'
        elif tag == 'insert':
            diff_type = 'added'
        elif tag == 'delete':
            diff_type = 'deleted'
        else:
            diff_type = tag
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚
        char_diff = None
        if diff_type == 'modified' and orig_content and new_content:
            char_diff = compute_char_level_diff(orig_content, new_content)
        
        diff = {
            'id': diff_id,
            'type': diff_type,
            'original': orig_content,
            'new': new_content,
            'start_idx_orig': i1,
            'end_idx_orig': i2,
            'start_idx_new': j1,
            'end_idx_new': j2,
            'char_diff': char_diff,
            'line_numbers_orig': [],  # æ®µè½çº§åˆ«ä¸æä¾›è¡Œå·
            'line_numbers_new': [],
            'context_before_orig': paragraphs1[max(0, i1-1):i1] if i1 > 0 else [],
            'context_after_orig': paragraphs1[i2:i2+1] if i2 < len(paragraphs1) else [],
            'context_before_new': paragraphs2[max(0, j1-1):j1] if j1 > 0 else [],
            'context_after_new': paragraphs2[j2:j2+1] if j2 < len(paragraphs2) else []
        }
        
        differences.append(diff)

    return differences


def compute_smart_chunk_differences(text1, text2):
    """ä½¿ç”¨æ™ºèƒ½åˆ†å‰²è®¡ç®—å·®å¼‚"""
    # æ™ºèƒ½åˆ†å‰²æ–‡æœ¬
    chunks1 = smart_split_text(text1, max_chunk_size=500)
    chunks2 = smart_split_text(text2, max_chunk_size=500)
    
    matcher = difflib.SequenceMatcher(None, chunks1, chunks2)
    differences = []

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            continue
        
        # ç”Ÿæˆå”¯ä¸€ID
        diff_id = str(uuid.uuid4())
        
        # è·å–å·®å¼‚å†…å®¹
        orig_content = '\n---\n'.join(chunks1[i1:i2]) if i1 < i2 else ''
        new_content = '\n---\n'.join(chunks2[j1:j2]) if j1 < j2 else ''
        
        # ç¡®å®šå·®å¼‚ç±»å‹
        if tag == 'replace':
            diff_type = 'modified'
        elif tag == 'insert':
            diff_type = 'added'
        elif tag == 'delete':
            diff_type = 'deleted'
        else:
            diff_type = tag
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚
        char_diff = None
        if diff_type == 'modified' and orig_content and new_content:
            char_diff = compute_char_level_diff(orig_content, new_content)
        
        diff = {
            'id': diff_id,
            'type': diff_type,
            'original': orig_content,
            'new': new_content,
            'start_idx_orig': i1,
            'end_idx_orig': i2,
            'start_idx_new': j1,
            'end_idx_new': j2,
            'char_diff': char_diff,
            'line_numbers_orig': [],  # æ™ºèƒ½åˆ†å‰²ä¸æä¾›è¡Œå·
            'line_numbers_new': [],
            'context_before_orig': chunks1[max(0, i1-1):i1] if i1 > 0 else [],
            'context_after_orig': chunks1[i2:i2+1] if i2 < len(chunks1) else [],
            'context_before_new': chunks2[max(0, j1-1):j1] if j1 > 0 else [],
            'context_after_new': chunks2[j2:j2+1] if j2 < len(chunks2) else []
        }
        
        differences.append(diff)

    return differences


def compute_char_level_diff(text1, text2):
    """è®¡ç®—å­—ç¬¦çº§åˆ«çš„å·®å¼‚ï¼Œç”¨äºæ›´ç²¾ç¡®çš„é«˜äº®æ˜¾ç¤º"""
    try:
        matcher = difflib.SequenceMatcher(None, text1, text2)
        char_diffs = []
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                continue
            
            char_diff = {
                'type': tag,
                'start_orig': i1,
                'end_orig': i2,
                'start_new': j1,
                'end_new': j2,
                'content_orig': text1[i1:i2] if i1 < i2 else '',
                'content_new': text2[j1:j2] if j1 < j2 else ''
            }
            char_diffs.append(char_diff)
        
        return char_diffs
    except Exception as e:
        print(f"å­—ç¬¦çº§åˆ«å·®å¼‚è®¡ç®—å¤±è´¥: {e}")
        return None


def search_keywords_in_text(text, keywords, search_mode='exact'):
    """
    åœ¨æ–‡æœ¬ä¸­æœç´¢å…³é”®è¯
    
    Args:
        text: è¦æœç´¢çš„æ–‡æœ¬
        keywords: å…³é”®è¯å­—ç¬¦ä¸²
        search_mode: æœç´¢æ¨¡å¼ ('exact', 'fuzzy', 'regex')
    
    Returns:
        tuple: (æ˜¯å¦åŒ¹é…, åŒ¹é…çš„è¯æ±‡åˆ—è¡¨, åŒ¹é…ä½ç½®)
    """
    import re
    
    if not text or not keywords:
        return False, [], []
    
    matches = []
    positions = []
    
    if search_mode == 'exact':
        # ç²¾ç¡®åŒ¹é…
        keyword_lower = keywords.lower()
        text_lower = text.lower()
        if keyword_lower in text_lower:
            matches.append(keywords)
            # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…ä½ç½®
            start = 0
            while True:
                pos = text_lower.find(keyword_lower, start)
                if pos == -1:
                    break
                positions.append((pos, pos + len(keywords)))
                start = pos + 1
    
    elif search_mode == 'fuzzy':
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³é”®è¯çš„å•è¯ï¼‰
        keyword_words = keywords.lower().split()
        text_lower = text.lower()
        for word in keyword_words:
            if word in text_lower:
                matches.append(word)
                # æ‰¾åˆ°åŒ¹é…ä½ç½®
                start = 0
                while True:
                    pos = text_lower.find(word, start)
                    if pos == -1:
                        break
                    positions.append((pos, pos + len(word)))
                    start = pos + 1
    
    elif search_mode == 'regex':
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        try:
            pattern = re.compile(keywords, re.IGNORECASE)
            found_matches = pattern.findall(text)
            matches = list(set(found_matches))
            
            # æ‰¾åˆ°åŒ¹é…ä½ç½®
            for match in pattern.finditer(text):
                positions.append((match.start(), match.end()))
        except re.error as e:
            print(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}")
            return False, [], []
    
    return len(matches) > 0, matches, positions


def get_context_around_matches(text, positions, context_size=100):
    """
    è·å–åŒ¹é…ä½ç½®å‘¨å›´çš„ä¸Šä¸‹æ–‡
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        positions: åŒ¹é…ä½ç½®åˆ—è¡¨ [(start, end), ...]
        context_size: ä¸Šä¸‹æ–‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    
    Returns:
        list: ä¸Šä¸‹æ–‡ç‰‡æ®µåˆ—è¡¨
    """
    if not positions:
        return []
    
    contexts = []
    for start, end in positions:
        # è®¡ç®—ä¸Šä¸‹æ–‡èŒƒå›´
        context_start = max(0, start - context_size)
        context_end = min(len(text), end + context_size)
        
        # æå–ä¸Šä¸‹æ–‡
        context = text[context_start:context_end]
        
        # æ ‡è®°åŒ¹é…ä½ç½®
        relative_start = start - context_start
        relative_end = end - context_start
        
        contexts.append({
            'text': context,
            'match_start': relative_start,
            'match_end': relative_end,
            'original_start': start,
            'original_end': end
        })
    
    return contexts


def calculate_match_score(matches, text_length, match_positions):
    """
    è®¡ç®—åŒ¹é…åˆ†æ•°
    
    Args:
        matches: åŒ¹é…çš„è¯æ±‡åˆ—è¡¨
        text_length: æ–‡æœ¬é•¿åº¦
        match_positions: åŒ¹é…ä½ç½®åˆ—è¡¨
    
    Returns:
        dict: åŒ…å«å„ç§åˆ†æ•°çš„å­—å…¸
    """
    if not matches:
        return {
            'match_count': 0,
            'match_ratio': 0.0,
            'density_score': 0.0,
            'coverage_score': 0.0
        }
    
    match_count = len(matches)
    match_ratio = match_count / max(text_length, 1) * 100
    
    # è®¡ç®—å¯†åº¦åˆ†æ•°ï¼ˆåŒ¹é…ä½ç½®çš„å¹³å‡å¯†åº¦ï¼‰
    if match_positions:
        total_match_length = sum(end - start for start, end in match_positions)
        density_score = total_match_length / max(text_length, 1) * 100
    else:
        density_score = 0.0
    
    # è®¡ç®—è¦†ç›–åˆ†æ•°ï¼ˆåŒ¹é…ä½ç½®è¦†ç›–çš„æ–‡æœ¬æ¯”ä¾‹ï¼‰
    if match_positions:
        covered_positions = set()
        for start, end in match_positions:
            covered_positions.update(range(start, end))
        coverage_score = len(covered_positions) / max(text_length, 1) * 100
    else:
        coverage_score = 0.0
    
    return {
        'match_count': match_count,
        'match_ratio': round(match_ratio, 2),
        'density_score': round(density_score, 2),
        'coverage_score': round(coverage_score, 2)
    }

# å¦‚æœä¹‹å‰æ›¿æ¢äº†printå‡½æ•°ï¼Œç°åœ¨æ¢å¤å®ƒ
if '_original_print' in globals():
    print = _original_print

def calculate_similarity_and_diff(text1, text2, task_id=None):
    """è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç”Ÿæˆå·®å¼‚ï¼ˆä¼˜å…ˆä½¿ç”¨diff-match-patchï¼‰"""
    similarity = compute_similarity(text1, text2)

    if _DMP_AVAILABLE:
        try:
            # ä¼˜å…ˆä½¿ç”¨ diff-match-patch è¿›è¡Œå­—ç¬¦çº§æ¯”è¾ƒ
            dmp = dmp_module.diff_match_patch()
            diffs = dmp.diff_main(text1, text2)
            dmp.diff_cleanupSemantic(diffs)
            
            # ç”Ÿæˆç¾è§‚çš„HTMLé«˜äº®å·®å¼‚
            pretty_html = dmp.diff_prettyHtml(diffs)
            
            # ä¸ºäº†å®‰å…¨ï¼Œå°†HTMLä¸­çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸º<br>ï¼Œå¹¶å¤„ç†ç©ºæ ¼
            pretty_html = pretty_html.replace('\\n', '<br>').replace(' &para;', '')
            
            return similarity, pretty_html

        except Exception as e:
            print(f"diff-match-patch æ‰§è¡Œå¤±è´¥: {e}, å›é€€åˆ° difflib")
    
    # å¦‚æœ diff-match-patch ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹çš„ difflib
    d = difflib.Differ()
    diff = d.compare(text1.splitlines(), text2.splitlines())
    
    diff_html = []
    for line in diff:
        if line.startswith('+ '):
            diff_html.append(f'<span class="diff-added">{line[2:]}</span>')
        elif line.startswith('- '):
            diff_html.append(f'<span class="diff-removed">{line[2:]}</span>')
        elif line.startswith('? '):
            # å¿½ç•¥difflibçš„æç¤ºè¡Œ
            continue
        else:
            diff_html.append(f'<span>{line[2:]}</span>')
    
    return similarity, '<br>'.join(diff_html)

def extract_text(file_path):
    """ä»æ–‡ä»¶æå–çº¯æ–‡æœ¬"""
    # ... existing code ...

# [é‡å¤çš„å…¨å±€å˜é‡å®šä¹‰å·²åˆ é™¤ï¼Œè¯·å‚è€ƒç¬¬1305è¡Œé™„è¿‘çš„ç»Ÿä¸€å®šä¹‰]

def get_model_info():
    """è·å–å½“å‰åŠ è½½çš„æ¨¡å‹ä¿¡æ¯"""
    return {
        "startup_mode": _startup_mode,
        "model_source": _model_source,
        "load_time_seconds": round(_model_load_time, 2),
        "is_tfidf": _use_tfidf_fallback,
        "device": str(device)
    }

# å¯åŠ¨æµç¨‹å·²ç§»è‡³ startup_manager.py æ–‡ä»¶ä¸­